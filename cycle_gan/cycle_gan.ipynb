{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This code is based on Kayo Yin implementation that can be found at https://github.com/kayoyin/DirtyDocuments\n",
    "# To properly use GPU processors, export the following env:\n",
    "# export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\n",
    "# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "import scipy.misc\n",
    "\n",
    "from keras.datasets import mnist\n",
    "# from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "rcParams['figure.figsize'] = 30, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class InstanceNormalization(Layer):\n",
    "    \"\"\"Instance normalization layer.\n",
    "\n",
    "    Normalize the activations of the previous layer at each step,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "\n",
    "    # Arguments\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `InstanceNormalization`.\n",
    "            Setting `axis=None` will normalize all values in each\n",
    "            instance of the batch.\n",
    "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a Sequential model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "\n",
    "    # References\n",
    "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
    "        https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 axis=None,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "* [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593.pdf)\n",
    "* [Keras Implementation of CycleGAN](https://github.com/eriklindernoren/Keras-GAN/tree/master/cyclegan)\n",
    "* [Understanding and Implementing CycleGAN in TensorFlow](https://hardikbansal.github.io/CycleGANBlog/)\n",
    "* [Image-to-image translation with Conditional Adversial Nets](https://phillipi.github.io/pix2pix/)\n",
    "* [Conditional GAN](https://arxiv.org/pdf/1411.1784.pdf)\n",
    "\n",
    "## Data sources:\n",
    "### Dirty documents\n",
    "* [Digitized Materials from the Rare Book & Special Collections Division](https://www.loc.gov/rr/rarebook/digitalcoll.html)\n",
    "\n",
    "### Clean documents\n",
    "* Anywhere online\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, img_res=(768, 512)):\n",
    "        self.img_res = img_res\n",
    "        self.img_height = img_res[0]\n",
    "        self.img_width = img_res[1]\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False, is_random = True):\n",
    "        path = glob('../%s/*' % domain)\n",
    "        \n",
    "        path_size = len(path)\n",
    "        train_size = round(path_size * 0.7) - 1\n",
    "        test_size = round(path_size * 0.2) + 1\n",
    "        path = path[:train_size] if not is_testing else path[train_size: train_size + test_size]\n",
    "        \n",
    "        if is_random: batch_images = np.random.choice(path, size=batch_size)\n",
    "        else: batch_images = path\n",
    "        \n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = image.load_img(img_path, color_mode='grayscale', target_size=(self.img_height, self.img_width))\n",
    "            img = image.img_to_array(img).astype('float32')\n",
    "            img = img / 255.0\n",
    "            if not is_testing and is_random:\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "\n",
    "              \n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_typeA = \"is_ocr_images\"\n",
    "        data_typeB = \"crappy_images\"\n",
    "        path_A = glob('../%s/*' % data_typeA)\n",
    "        path_B = glob('../%s/*' % data_typeB)\n",
    "        \n",
    "        path_size = len(path_A)\n",
    "\n",
    "        train_size = round(path_size * 0.7) - 1\n",
    "        test_size = round(path_size * 0.2) + 1\n",
    "        \n",
    "        path_A = path_A[:train_size] if not is_testing else path_A[train_size: train_size + test_size]\n",
    "        path_B = path_B[:train_size] if not is_testing else path_B[train_size: train_size + test_size]\n",
    "\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                try:\n",
    "                    img_A = image.load_img(img_A, color_mode='grayscale', target_size=(self.img_height, self.img_width))\n",
    "                    img_A = image.img_to_array(img_A).astype('float32')\n",
    "                    img_A = img_A / 255.0\n",
    "\n",
    "                    img_B = image.load_img(img_B, color_mode='grayscale', target_size=(self.img_height, self.img_width))\n",
    "                    img_B = image.img_to_array(img_B).astype('float32')\n",
    "                    img_B = img_B / 255.0\n",
    "\n",
    "                    if not is_testing and np.random.random() > 0.5:\n",
    "                            img_A = np.fliplr(img_A)\n",
    "                            img_B = np.fliplr(img_B)\n",
    "\n",
    "                    imgs_A.append(img_A)\n",
    "                    imgs_B.append(img_B)\n",
    "                except Exception as e:\n",
    "                    print()\n",
    "\n",
    "            imgs_A = np.array(imgs_A)\n",
    "            imgs_B = np.array(imgs_B)\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img.resize(self.img_res)\n",
    "        img = img/255.0\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return cv2.imread(path).astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self, d_A=None, d_B=None, g_AB=None, g_BA=None):\n",
    "        # Input shape\n",
    "        self.img_rows = 768\n",
    "        self.img_cols = 512\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.data_loader = DataLoader(img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch1 = int(self.img_rows / 2**4)\n",
    "        patch2 = int(self.img_cols / 2**4)\n",
    "        self.disc_patch = (patch1, patch2, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss (mainly to preserve color consistency)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        if d_A:\n",
    "            self.d_A = d_A\n",
    "        else:\n",
    "            self.d_A = self.build_discriminator()\n",
    "            self.d_A.compile(\n",
    "                loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "        if d_B:\n",
    "            self.d_B = d_B\n",
    "        else:\n",
    "            self.d_B = self.build_discriminator()\n",
    "            self.d_B.compile(\n",
    "                loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator() if not g_AB else g_AB\n",
    "        self.g_BA = self.build_generator() if not g_BA else g_BA\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "\n",
    "        #img_A = img_A.reshape(1,256, 256,1)\n",
    "        #img_B = img_B.reshape(1,256, 256,1)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "        \n",
    "        model = Model(d0, output_img)\n",
    "        \n",
    "        model.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "        \n",
    "        print(model.summary())\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        model = Model(img, validity)\n",
    "        model.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        \n",
    "        unlinkFile = True\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                try:\n",
    "                    # ----------------------\n",
    "                    #  Train Discriminators\n",
    "                    # ----------------------\n",
    "\n",
    "                    # Translate images to opposite domain\n",
    "                    imgs_A = imgs_A.reshape(-1,self.img_rows, self.img_cols,1)\n",
    "                    imgs_B = imgs_B.reshape(-1,self.img_rows, self.img_cols,1)\n",
    "                    fake_B = self.g_AB.predict(imgs_A)\n",
    "                    fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                    # Train the discriminators (original images = real / translated = Fake)\n",
    "                    dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                    dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                    dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                    dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                    dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                    dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                    # Total disciminator loss\n",
    "                    d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                    # ------------------\n",
    "                    #  Train Generators\n",
    "                    # ------------------\n",
    "\n",
    "                    # Train the generators\n",
    "                    g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                            [valid, valid,\n",
    "                                                            imgs_A, imgs_B,\n",
    "                                                            imgs_A, imgs_B])\n",
    "\n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                    # Plot the progress\n",
    "                    if batch_i % sample_interval == 0:\n",
    "                        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                            % ( epoch+1, epochs,\n",
    "                                                                                batch_i+1, self.data_loader.n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                g_loss[0],\n",
    "                                                                                np.mean(g_loss[1:3]),\n",
    "                                                                                np.mean(g_loss[3:5]),\n",
    "                                                                                np.mean(g_loss[5:6]),\n",
    "                                                                                elapsed_time))\n",
    "                \n",
    "                    g_losses.append(g_loss[0])\n",
    "                    d_losses.append(d_loss[0])\n",
    "                except Exception as e:\n",
    "                    print()\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if (batch_i % 5000 == 0):\n",
    "                    self.d_A.save('backup_d_A.h5')\n",
    "                    self.d_B.save('backup_d_B.h5')\n",
    "                    self.g_AB.save('backup_g_AB.h5')\n",
    "                    self.g_BA.save('backup_g_BA.h5')\n",
    "                    if unlinkFile:\n",
    "                        if os.path.exists('./backup_losses.csv'):\n",
    "                            os.remove('./backup_losses.csv')\n",
    "                        unlinkFile = False\n",
    "                    with open('./backup_losses.csv', 'a+', newline='') as csvfile:\n",
    "                        spamwriter = csv.writer(csvfile, delimiter=';',\n",
    "                                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                        for i in range(len(g_losses)):\n",
    "                            spamwriter.writerow([d_losses[i], g_losses[i]])\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "                    g_losses = []\n",
    "                    d_losses = []\n",
    "                    \n",
    "        if len(d_losses) > 0:\n",
    "            with open('./backup_losses.csv', 'a+', newline='') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter=';',\n",
    "                                        quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                for i in range(len(g_losses)):\n",
    "                    spamwriter.writerow([d_losses[i], g_losses[i]])\n",
    "                \n",
    "        return d_losses, g_losses\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('gan_images/', exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A = self.data_loader.load_data(domain=\"is_ocr_images\", batch_size=1, is_testing=True)\n",
    "        imgs_B = self.data_loader.load_data(domain=\"crappy_images\", batch_size=1, is_testing=True)\n",
    "\n",
    "        imgs_A = imgs_A.reshape(-1,self.img_rows, self.img_cols,1)\n",
    "        imgs_B = imgs_B.reshape(-1,self.img_rows, self.img_cols,1)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "        \n",
    "        gen_imgs = np.concatenate([imgs_A[:,:,:,0], fake_B[:,:,:,0], reconstr_A[:,:,:,0], imgs_B[:,:,:,0], fake_A[:,:,:,0], reconstr_B[:,:,:,0]])\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt], cmap = 'gray')\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        \n",
    "        fig.savefig(\"gan_images/%d_%d.png\" % (epoch, batch_i), dpi = 800)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "d_A = load_model('backup_d_A.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "d_B = load_model('backup_d_B.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "g_AB = load_model('backup_g_AB.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "g_BA = load_model('backup_g_BA.h5',custom_objects={'InstanceNormalization':InstanceNormalization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/1] [Batch 1/56436] [D loss: 0.019172, acc:  99%] [G loss: 2.832920, adv: 0.884437, recon: 0.048015, id: 0.009634] time: 0:00:05.934462 \n",
      "[Epoch 1/1] [Batch 51/56436] [D loss: 0.067486, acc:  90%] [G loss: 1.972445, adv: 0.592944, recon: 0.036384, id: 0.007567] time: 0:00:42.605688 \n",
      "[Epoch 1/1] [Batch 101/56436] [D loss: 0.180833, acc:  75%] [G loss: 1.657620, adv: 0.526552, recon: 0.028580, id: 0.009571] time: 0:01:13.890628 \n",
      "[Epoch 1/1] [Batch 151/56436] [D loss: 0.219502, acc:  70%] [G loss: 1.097382, adv: 0.327657, recon: 0.021085, id: 0.002922] time: 0:01:45.911144 \n",
      "[Epoch 1/1] [Batch 201/56436] [D loss: 0.312072, acc:  50%] [G loss: 1.293526, adv: 0.075337, recon: 0.051993, id: 0.060419] time: 0:02:17.446003 \n",
      "[Epoch 1/1] [Batch 251/56436] [D loss: 0.500160, acc:  49%] [G loss: 0.954714, adv: 0.015619, recon: 0.042178, id: 0.026914] time: 0:02:48.287698 \n",
      "[Epoch 1/1] [Batch 301/56436] [D loss: 0.274819, acc:  68%] [G loss: 0.728165, adv: 0.176269, recon: 0.017022, id: 0.007722] time: 0:03:19.314336 \n",
      "[Epoch 1/1] [Batch 351/56436] [D loss: 0.329883, acc:  50%] [G loss: 1.308771, adv: 0.055464, recon: 0.055067, id: 0.039571] time: 0:03:50.223044 \n",
      "[Epoch 1/1] [Batch 401/56436] [D loss: 0.258208, acc:  66%] [G loss: 1.426271, adv: 0.164885, recon: 0.050938, id: 0.010822] time: 0:04:21.085103 \n",
      "[Epoch 1/1] [Batch 451/56436] [D loss: 0.358361, acc:  52%] [G loss: 1.178795, adv: 0.077080, recon: 0.047086, id: 0.022621] time: 0:04:51.891841 \n",
      "[Epoch 1/1] [Batch 501/56436] [D loss: 0.434585, acc:  50%] [G loss: 0.564059, adv: 0.040178, recon: 0.022431, id: 0.012303] time: 0:05:22.631968 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 551/56436] [D loss: 0.602907, acc:  28%] [G loss: 0.904346, adv: 0.100452, recon: 0.026946, id: 0.005236] time: 0:05:52.435886 \n",
      "[Epoch 1/1] [Batch 601/56436] [D loss: 0.396968, acc:  50%] [G loss: 0.956209, adv: 0.032905, recon: 0.041114, id: 0.037912] time: 0:06:23.379422 \n",
      "[Epoch 1/1] [Batch 651/56436] [D loss: 0.194198, acc:  73%] [G loss: 1.298778, adv: 0.382314, recon: 0.024094, id: 0.012814] time: 0:06:54.311870 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 701/56436] [D loss: 0.324578, acc:  51%] [G loss: 0.991284, adv: 0.081420, recon: 0.037766, id: 0.031142] time: 0:07:24.833199 \n",
      "[Epoch 1/1] [Batch 751/56436] [D loss: 0.353609, acc:  50%] [G loss: 0.791472, adv: 0.047551, recon: 0.031578, id: 0.025873] time: 0:07:55.845090 \n",
      "[Epoch 1/1] [Batch 801/56436] [D loss: 0.399659, acc:  42%] [G loss: 1.509735, adv: 0.045738, recon: 0.065288, id: 0.038458] time: 0:08:27.551028 \n",
      "[Epoch 1/1] [Batch 851/56436] [D loss: 0.476423, acc:  49%] [G loss: 1.123617, adv: 0.022566, recon: 0.049916, id: 0.036674] time: 0:08:58.948398 \n",
      "[Epoch 1/1] [Batch 901/56436] [D loss: 0.211662, acc:  70%] [G loss: 5.701989, adv: 0.637067, recon: 0.201761, id: 0.357159] time: 0:09:30.426344 \n",
      "[Epoch 1/1] [Batch 951/56436] [D loss: 0.528384, acc:  49%] [G loss: 0.978649, adv: 0.024900, recon: 0.042665, id: 0.033337] time: 0:10:02.617873 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 1001/56436] [D loss: 0.496377, acc:  48%] [G loss: 0.816998, adv: 0.023951, recon: 0.034009, id: 0.013391] time: 0:10:34.637568 \n",
      "[Epoch 1/1] [Batch 1051/56436] [D loss: 0.488024, acc:  49%] [G loss: 0.887138, adv: 0.019500, recon: 0.039527, id: 0.012787] time: 0:11:08.448060 \n",
      "[Epoch 1/1] [Batch 1101/56436] [D loss: 0.543986, acc:  49%] [G loss: 0.714252, adv: 0.013480, recon: 0.031291, id: 0.018937] time: 0:11:42.053933 \n",
      "[Epoch 1/1] [Batch 1151/56436] [D loss: 0.486382, acc:  48%] [G loss: 0.404663, adv: 0.013190, recon: 0.017145, id: 0.010431] time: 0:12:15.278158 \n",
      "[Epoch 1/1] [Batch 1201/56436] [D loss: 0.470107, acc:  49%] [G loss: 0.653958, adv: 0.011504, recon: 0.029028, id: 0.015875] time: 0:12:48.185917 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 1251/56436] [D loss: 0.547235, acc:  51%] [G loss: 0.484231, adv: 0.032656, recon: 0.019208, id: 0.021549] time: 0:13:21.468106 \n",
      "[Epoch 1/1] [Batch 1301/56436] [D loss: 0.488301, acc:  50%] [G loss: 1.046397, adv: 0.020164, recon: 0.046438, id: 0.021034] time: 0:13:53.506792 \n",
      "[Epoch 1/1] [Batch 1351/56436] [D loss: 0.664184, acc:  48%] [G loss: 0.637236, adv: 0.054349, recon: 0.023856, id: 0.008740] time: 0:14:25.831211 \n",
      "[Epoch 1/1] [Batch 1401/56436] [D loss: 0.183420, acc:  55%] [G loss: 1.304722, adv: 0.209760, recon: 0.041123, id: 0.026389] time: 0:14:57.566389 \n",
      "[Epoch 1/1] [Batch 1451/56436] [D loss: 0.415151, acc:  50%] [G loss: 1.020758, adv: 0.059264, recon: 0.040665, id: 0.050092] time: 0:15:29.793128 \n",
      "[Epoch 1/1] [Batch 1501/56436] [D loss: 0.448937, acc:  49%] [G loss: 1.132567, adv: 0.053822, recon: 0.047144, id: 0.013053] time: 0:16:02.534753 \n",
      "[Epoch 1/1] [Batch 1551/56436] [D loss: 0.526280, acc:  48%] [G loss: 0.653738, adv: 0.040115, recon: 0.026107, id: 0.026213] time: 0:16:34.508942 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 1601/56436] [D loss: 0.524227, acc:  48%] [G loss: 0.820026, adv: 0.012046, recon: 0.037217, id: 0.011836] time: 0:17:07.225278 \n",
      "[Epoch 1/1] [Batch 1651/56436] [D loss: 0.413534, acc:  51%] [G loss: 0.909030, adv: 0.046884, recon: 0.032501, id: 0.041583] time: 0:17:41.605151 \n",
      "[Epoch 1/1] [Batch 1701/56436] [D loss: 0.442804, acc:  48%] [G loss: 0.493171, adv: 0.031795, recon: 0.018755, id: 0.011408] time: 0:18:15.875256 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 1751/56436] [D loss: 0.289915, acc:  51%] [G loss: 0.670523, adv: 0.111813, recon: 0.020191, id: 0.010697] time: 0:18:49.453860 \n",
      "[Epoch 1/1] [Batch 1801/56436] [D loss: 0.425061, acc:  49%] [G loss: 0.517610, adv: 0.024004, recon: 0.021259, id: 0.014868] time: 0:19:23.605730 \n",
      "[Epoch 1/1] [Batch 1851/56436] [D loss: 0.456527, acc:  50%] [G loss: 0.643436, adv: 0.045053, recon: 0.023865, id: 0.022533] time: 0:19:58.025450 \n",
      "[Epoch 1/1] [Batch 1901/56436] [D loss: 0.478516, acc:  49%] [G loss: 0.543785, adv: 0.029259, recon: 0.022426, id: 0.022842] time: 0:20:32.435565 \n",
      "[Epoch 1/1] [Batch 1951/56436] [D loss: 0.495205, acc:  49%] [G loss: 0.709338, adv: 0.026699, recon: 0.029597, id: 0.032313] time: 0:21:05.528151 \n",
      "[Epoch 1/1] [Batch 2001/56436] [D loss: 0.518558, acc:  48%] [G loss: 0.425615, adv: 0.012913, recon: 0.018497, id: 0.012088] time: 0:21:39.242289 \n",
      "[Epoch 1/1] [Batch 2051/56436] [D loss: 0.489934, acc:  50%] [G loss: 0.795693, adv: 0.028897, recon: 0.033838, id: 0.026050] time: 0:22:11.341122 \n",
      "[Epoch 1/1] [Batch 2101/56436] [D loss: 0.579580, acc:  49%] [G loss: 0.482910, adv: 0.019033, recon: 0.019693, id: 0.031808] time: 0:22:42.294362 \n",
      "[Epoch 1/1] [Batch 2151/56436] [D loss: 0.591678, acc:  46%] [G loss: 0.919259, adv: 0.012711, recon: 0.041751, id: 0.024460] time: 0:23:13.386002 \n",
      "[Epoch 1/1] [Batch 2201/56436] [D loss: 0.534865, acc:  51%] [G loss: 0.829181, adv: 0.042210, recon: 0.033840, id: 0.037435] time: 0:23:44.431435 \n",
      "[Epoch 1/1] [Batch 2251/56436] [D loss: 0.525907, acc:  50%] [G loss: 0.898558, adv: 0.013997, recon: 0.038933, id: 0.036137] time: 0:24:15.447452 \n",
      "[Epoch 1/1] [Batch 2301/56436] [D loss: 0.506944, acc:  50%] [G loss: 1.095474, adv: 0.033264, recon: 0.046279, id: 0.051664] time: 0:24:46.457901 \n",
      "[Epoch 1/1] [Batch 2351/56436] [D loss: 0.427147, acc:  49%] [G loss: 0.525498, adv: 0.030786, recon: 0.020782, id: 0.019257] time: 0:25:17.506065 \n",
      "[Epoch 1/1] [Batch 2401/56436] [D loss: 0.483900, acc:  49%] [G loss: 0.735489, adv: 0.016746, recon: 0.031583, id: 0.038503] time: 0:25:48.440651 \n",
      "[Epoch 1/1] [Batch 2451/56436] [D loss: 0.383107, acc:  51%] [G loss: 0.805948, adv: 0.051122, recon: 0.031151, id: 0.016960] time: 0:26:19.437336 \n",
      "[Epoch 1/1] [Batch 2501/56436] [D loss: 0.650634, acc:  25%] [G loss: 0.543005, adv: 0.011308, recon: 0.023793, id: 0.010294] time: 0:26:50.404848 \n",
      "[Epoch 1/1] [Batch 2551/56436] [D loss: 0.470361, acc:  48%] [G loss: 0.418500, adv: 0.009655, recon: 0.017914, id: 0.011336] time: 0:27:21.856203 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 2601/56436] [D loss: 0.465004, acc:  47%] [G loss: 0.481921, adv: 0.025043, recon: 0.019724, id: 0.008122] time: 0:27:52.937611 \n",
      "[Epoch 1/1] [Batch 2651/56436] [D loss: 0.508301, acc:  50%] [G loss: 0.739665, adv: 0.041381, recon: 0.029117, id: 0.010753] time: 0:28:24.074972 \n",
      "[Epoch 1/1] [Batch 2701/56436] [D loss: 0.561361, acc:  49%] [G loss: 0.717768, adv: 0.013462, recon: 0.031069, id: 0.020989] time: 0:28:55.191246 \n",
      "[Epoch 1/1] [Batch 2751/56436] [D loss: 0.355563, acc:  52%] [G loss: 1.039687, adv: 0.070470, recon: 0.041220, id: 0.025222] time: 0:29:26.276309 \n",
      "[Epoch 1/1] [Batch 2801/56436] [D loss: 0.359320, acc:  55%] [G loss: 1.524261, adv: 0.103685, recon: 0.062969, id: 0.011901] time: 0:29:57.382864 \n",
      "[Epoch 1/1] [Batch 2851/56436] [D loss: 0.514213, acc:  49%] [G loss: 0.699301, adv: 0.023233, recon: 0.029307, id: 0.035110] time: 0:30:28.582268 \n",
      "[Epoch 1/1] [Batch 2901/56436] [D loss: 0.517878, acc:  49%] [G loss: 0.430297, adv: 0.012476, recon: 0.018499, id: 0.010342] time: 0:30:59.718978 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 2951/56436] [D loss: 0.582259, acc:  25%] [G loss: 0.786897, adv: 0.023470, recon: 0.034049, id: 0.006423] time: 0:31:30.332425 \n",
      "[Epoch 1/1] [Batch 3001/56436] [D loss: 0.357249, acc:  49%] [G loss: 0.816379, adv: 0.069490, recon: 0.030639, id: 0.034976] time: 0:32:01.484961 \n",
      "[Epoch 1/1] [Batch 3051/56436] [D loss: 0.474996, acc:  50%] [G loss: 0.938534, adv: 0.048536, recon: 0.037298, id: 0.060572] time: 0:32:32.617746 \n",
      "[Epoch 1/1] [Batch 3101/56436] [D loss: 0.482990, acc:  50%] [G loss: 1.404005, adv: 0.024119, recon: 0.061714, id: 0.066241] time: 0:33:03.796065 \n",
      "[Epoch 1/1] [Batch 3151/56436] [D loss: 0.417023, acc:  47%] [G loss: 1.571629, adv: 0.064646, recon: 0.068469, id: 0.029359] time: 0:33:34.949538 \n",
      "[Epoch 1/1] [Batch 3201/56436] [D loss: 0.338277, acc:  51%] [G loss: 1.130818, adv: 0.062705, recon: 0.046912, id: 0.011390] time: 0:34:06.098364 \n",
      "[Epoch 1/1] [Batch 3251/56436] [D loss: 0.465796, acc:  49%] [G loss: 0.980717, adv: 0.038167, recon: 0.031527, id: 0.035404] time: 0:34:37.204946 \n",
      "[Epoch 1/1] [Batch 3301/56436] [D loss: 0.612902, acc:  25%] [G loss: 0.417711, adv: 0.028819, recon: 0.016505, id: 0.004722] time: 0:35:08.333506 \n",
      "[Epoch 1/1] [Batch 3351/56436] [D loss: 0.573530, acc:  48%] [G loss: 0.523445, adv: 0.010480, recon: 0.022887, id: 0.013537] time: 0:35:39.293584 \n",
      "[Epoch 1/1] [Batch 3401/56436] [D loss: 0.330821, acc:  50%] [G loss: 0.715094, adv: 0.106593, recon: 0.023479, id: 0.009035] time: 0:36:10.226487 \n",
      "[Epoch 1/1] [Batch 3451/56436] [D loss: 0.486687, acc:  51%] [G loss: 0.542932, adv: 0.028866, recon: 0.021325, id: 0.017286] time: 0:36:41.737568 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 3501/56436] [D loss: 0.582707, acc:  48%] [G loss: 0.585778, adv: 0.013166, recon: 0.025403, id: 0.017397] time: 0:37:12.374197 \n",
      "[Epoch 1/1] [Batch 3551/56436] [D loss: 0.635270, acc:  49%] [G loss: 0.439305, adv: 0.027353, recon: 0.017360, id: 0.018999] time: 0:37:43.630572 \n",
      "[Epoch 1/1] [Batch 3601/56436] [D loss: 0.474130, acc:  48%] [G loss: 0.442677, adv: 0.015623, recon: 0.018892, id: 0.010397] time: 0:38:15.202503 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 3651/56436] [D loss: 0.554511, acc:  48%] [G loss: 0.479907, adv: 0.011240, recon: 0.020804, id: 0.019170] time: 0:38:46.040911 \n",
      "[Epoch 1/1] [Batch 3701/56436] [D loss: 0.450053, acc:  45%] [G loss: 0.453358, adv: 0.026720, recon: 0.017946, id: 0.030726] time: 0:39:17.347148 \n",
      "[Epoch 1/1] [Batch 3751/56436] [D loss: 0.404849, acc:  51%] [G loss: 0.756240, adv: 0.067822, recon: 0.027953, id: 0.031781] time: 0:39:48.618733 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 3801/56436] [D loss: 0.485644, acc:  47%] [G loss: 1.416128, adv: 0.022252, recon: 0.062512, id: 0.038604] time: 0:40:19.777380 \n",
      "[Epoch 1/1] [Batch 3851/56436] [D loss: 0.372594, acc:  51%] [G loss: 1.169551, adv: 0.039604, recon: 0.044023, id: 0.038447] time: 0:40:51.288513 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 3901/56436] [D loss: 0.629276, acc:  49%] [G loss: 0.815081, adv: 0.029186, recon: 0.034761, id: 0.027375] time: 0:41:22.217745 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 3951/56436] [D loss: 0.545332, acc:  49%] [G loss: 0.491336, adv: 0.019635, recon: 0.020522, id: 0.014214] time: 0:41:52.759383 \n",
      "[Epoch 1/1] [Batch 4001/56436] [D loss: 0.503763, acc:  49%] [G loss: 0.729911, adv: 0.017527, recon: 0.031337, id: 0.012104] time: 0:42:23.995592 \n",
      "[Epoch 1/1] [Batch 4051/56436] [D loss: 0.452654, acc:  50%] [G loss: 0.774907, adv: 0.027014, recon: 0.033603, id: 0.017846] time: 0:42:55.528721 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 4101/56436] [D loss: 0.325426, acc:  53%] [G loss: 0.695051, adv: 0.088889, recon: 0.023238, id: 0.027480] time: 0:43:26.527874 \n",
      "[Epoch 1/1] [Batch 4151/56436] [D loss: 0.471537, acc:  49%] [G loss: 0.303540, adv: 0.011543, recon: 0.012435, id: 0.016299] time: 0:43:58.481991 \n",
      "[Epoch 1/1] [Batch 4201/56436] [D loss: 0.475382, acc:  51%] [G loss: 0.745897, adv: 0.047303, recon: 0.029079, id: 0.039013] time: 0:44:29.485549 \n",
      "[Epoch 1/1] [Batch 4251/56436] [D loss: 0.497255, acc:  49%] [G loss: 0.563412, adv: 0.008795, recon: 0.024609, id: 0.022843] time: 0:45:00.571173 \n",
      "[Epoch 1/1] [Batch 4301/56436] [D loss: 0.452263, acc:  50%] [G loss: 0.825726, adv: 0.029999, recon: 0.036009, id: 0.030050] time: 0:45:31.616712 \n",
      "[Epoch 1/1] [Batch 4351/56436] [D loss: 0.491108, acc:  47%] [G loss: 1.098506, adv: 0.009613, recon: 0.050631, id: 0.010453] time: 0:46:02.660844 \n",
      "[Epoch 1/1] [Batch 4401/56436] [D loss: 0.532318, acc:  49%] [G loss: 0.585860, adv: 0.009974, recon: 0.025573, id: 0.020122] time: 0:46:33.742055 \n",
      "[Epoch 1/1] [Batch 4451/56436] [D loss: 0.486330, acc:  48%] [G loss: 0.728885, adv: 0.019058, recon: 0.031607, id: 0.022209] time: 0:47:04.782837 \n",
      "[Epoch 1/1] [Batch 4501/56436] [D loss: 0.534366, acc:  48%] [G loss: 0.820404, adv: 0.055773, recon: 0.032308, id: 0.017098] time: 0:47:36.237342 \n",
      "[Epoch 1/1] [Batch 4551/56436] [D loss: 0.388603, acc:  51%] [G loss: 0.947857, adv: 0.043158, recon: 0.039786, id: 0.034610] time: 0:48:07.954466 \n",
      "[Epoch 1/1] [Batch 4601/56436] [D loss: 0.511346, acc:  49%] [G loss: 0.386597, adv: 0.019618, recon: 0.015909, id: 0.012868] time: 0:48:39.209469 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 4651/56436] [D loss: 0.533833, acc:  50%] [G loss: 0.879612, adv: 0.026932, recon: 0.038045, id: 0.030136] time: 0:49:09.748640 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 4701/56436] [D loss: 0.540025, acc:  50%] [G loss: 0.508147, adv: 0.007772, recon: 0.022457, id: 0.018861] time: 0:49:40.712105 \n",
      "[Epoch 1/1] [Batch 4751/56436] [D loss: 0.422943, acc:  48%] [G loss: 0.380620, adv: 0.031870, recon: 0.014447, id: 0.010126] time: 0:50:12.243383 \n",
      "[Epoch 1/1] [Batch 4801/56436] [D loss: 0.318582, acc:  49%] [G loss: 0.758256, adv: 0.063347, recon: 0.029024, id: 0.017541] time: 0:50:43.491089 \n",
      "[Epoch 1/1] [Batch 4851/56436] [D loss: 0.652526, acc:  25%] [G loss: 0.615731, adv: 0.022686, recon: 0.026185, id: 0.006960] time: 0:51:15.059994 \n",
      "[Epoch 1/1] [Batch 4901/56436] [D loss: 0.478578, acc:  50%] [G loss: 0.803889, adv: 0.019295, recon: 0.034474, id: 0.033829] time: 0:51:46.663513 \n",
      "[Epoch 1/1] [Batch 4951/56436] [D loss: 0.347024, acc:  51%] [G loss: 0.872932, adv: 0.108635, recon: 0.030026, id: 0.043362] time: 0:52:17.897388 \n",
      "[Epoch 1/1] [Batch 5001/56436] [D loss: 0.596470, acc:  49%] [G loss: 0.464237, adv: 0.013972, recon: 0.019731, id: 0.025601] time: 0:52:49.048327 \n",
      "[Epoch 1/1] [Batch 5051/56436] [D loss: 0.333855, acc:  51%] [G loss: 0.693490, adv: 0.072723, recon: 0.025596, id: 0.011573] time: 0:53:25.662895 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 5101/56436] [D loss: 0.429750, acc:  50%] [G loss: 0.381966, adv: 0.028235, recon: 0.014721, id: 0.012886] time: 0:53:56.149763 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 5151/56436] [D loss: 0.370378, acc:  51%] [G loss: 0.526805, adv: 0.107604, recon: 0.013969, id: 0.008329] time: 0:54:26.857767 \n",
      "[Epoch 1/1] [Batch 5201/56436] [D loss: 0.491679, acc:  49%] [G loss: 1.030268, adv: 0.019031, recon: 0.045090, id: 0.036205] time: 0:54:57.985965 \n",
      "[Epoch 1/1] [Batch 5251/56436] [D loss: 0.518501, acc:  50%] [G loss: 0.475828, adv: 0.022953, recon: 0.019882, id: 0.016303] time: 0:55:29.562479 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 5301/56436] [D loss: 0.518471, acc:  49%] [G loss: 0.526086, adv: 0.008491, recon: 0.023259, id: 0.012097] time: 0:56:00.887169 \n",
      "[Epoch 1/1] [Batch 5351/56436] [D loss: 0.566582, acc:  50%] [G loss: 1.419102, adv: 0.030897, recon: 0.050330, id: 0.076810] time: 0:56:32.274888 \n",
      "[Epoch 1/1] [Batch 5401/56436] [D loss: 0.491417, acc:  49%] [G loss: 0.477926, adv: 0.007513, recon: 0.020817, id: 0.014419] time: 0:57:03.615862 \n",
      "[Epoch 1/1] [Batch 5451/56436] [D loss: 0.433716, acc:  50%] [G loss: 0.666171, adv: 0.019024, recon: 0.028218, id: 0.036822] time: 0:57:34.708476 \n",
      "[Epoch 1/1] [Batch 5501/56436] [D loss: 0.491423, acc:  50%] [G loss: 0.636688, adv: 0.020128, recon: 0.027066, id: 0.021695] time: 0:58:05.867685 \n",
      "[Epoch 1/1] [Batch 5551/56436] [D loss: 0.516652, acc:  47%] [G loss: 0.609951, adv: 0.009222, recon: 0.026862, id: 0.018040] time: 0:58:36.889040 \n",
      "[Epoch 1/1] [Batch 5601/56436] [D loss: 0.563470, acc:  49%] [G loss: 0.714022, adv: 0.020257, recon: 0.019719, id: 0.025537] time: 0:59:08.150569 \n",
      "[Epoch 1/1] [Batch 5651/56436] [D loss: 0.492097, acc:  49%] [G loss: 0.905949, adv: 0.022037, recon: 0.038865, id: 0.024824] time: 0:59:39.388419 \n",
      "[Epoch 1/1] [Batch 5701/56436] [D loss: 0.530863, acc:  27%] [G loss: 1.169114, adv: 0.058167, recon: 0.048138, id: 0.044160] time: 1:00:10.521464 \n",
      "[Epoch 1/1] [Batch 5751/56436] [D loss: 0.559604, acc:  49%] [G loss: 0.551091, adv: 0.016120, recon: 0.023296, id: 0.039357] time: 1:00:41.735810 \n",
      "[Epoch 1/1] [Batch 5801/56436] [D loss: 0.438531, acc:  49%] [G loss: 0.387316, adv: 0.014129, recon: 0.015855, id: 0.013186] time: 1:01:13.047880 \n",
      "[Epoch 1/1] [Batch 5851/56436] [D loss: 0.523538, acc:  50%] [G loss: 1.034671, adv: 0.027348, recon: 0.044736, id: 0.026321] time: 1:01:44.306574 \n",
      "[Epoch 1/1] [Batch 5901/56436] [D loss: 0.559477, acc:  26%] [G loss: 0.575558, adv: 0.035386, recon: 0.023075, id: 0.006527] time: 1:02:15.614927 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 5951/56436] [D loss: 0.426149, acc:  49%] [G loss: 0.656112, adv: 0.033639, recon: 0.026763, id: 0.016204] time: 1:02:46.426236 \n",
      "[Epoch 1/1] [Batch 6001/56436] [D loss: 0.522501, acc:  49%] [G loss: 0.640874, adv: 0.020032, recon: 0.027354, id: 0.015379] time: 1:03:17.950845 \n",
      "[Epoch 1/1] [Batch 6051/56436] [D loss: 0.523528, acc:  49%] [G loss: 0.479102, adv: 0.010542, recon: 0.020196, id: 0.017348] time: 1:03:49.104171 \n",
      "[Epoch 1/1] [Batch 6101/56436] [D loss: 0.363808, acc:  50%] [G loss: 0.864570, adv: 0.057608, recon: 0.025319, id: 0.013645] time: 1:04:20.338465 \n",
      "[Epoch 1/1] [Batch 6151/56436] [D loss: 0.459816, acc:  49%] [G loss: 0.798096, adv: 0.023376, recon: 0.033632, id: 0.037063] time: 1:04:51.508365 \n",
      "[Epoch 1/1] [Batch 6201/56436] [D loss: 0.338380, acc:  47%] [G loss: 1.086623, adv: 0.141577, recon: 0.036559, id: 0.063057] time: 1:05:22.654216 \n",
      "[Epoch 1/1] [Batch 6251/56436] [D loss: 0.427365, acc:  51%] [G loss: 0.547125, adv: 0.074278, recon: 0.017620, id: 0.035736] time: 1:05:53.837028 \n",
      "[Epoch 1/1] [Batch 6301/56436] [D loss: 0.562646, acc:  49%] [G loss: 0.545268, adv: 0.015945, recon: 0.022516, id: 0.036692] time: 1:06:25.086944 \n",
      "[Epoch 1/1] [Batch 6351/56436] [D loss: 0.509172, acc:  49%] [G loss: 0.800307, adv: 0.011030, recon: 0.032532, id: 0.013908] time: 1:06:56.241106 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 6401/56436] [D loss: 0.717035, acc:  49%] [G loss: 0.885022, adv: 0.041555, recon: 0.035908, id: 0.055633] time: 1:07:26.807220 \n",
      "[Epoch 1/1] [Batch 6451/56436] [D loss: 0.524063, acc:  49%] [G loss: 0.519141, adv: 0.025164, recon: 0.021029, id: 0.023161] time: 1:07:57.789856 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 6501/56436] [D loss: 0.421222, acc:  51%] [G loss: 1.196088, adv: 0.103523, recon: 0.044846, id: 0.061963] time: 1:08:27.687358 \n",
      "[Epoch 1/1] [Batch 6551/56436] [D loss: 0.498049, acc:  46%] [G loss: 0.697502, adv: 0.029432, recon: 0.029609, id: 0.026955] time: 1:08:58.776734 \n",
      "[Epoch 1/1] [Batch 6601/56436] [D loss: 0.540531, acc:  48%] [G loss: 0.563320, adv: 0.014015, recon: 0.024653, id: 0.013923] time: 1:09:30.066011 \n",
      "[Epoch 1/1] [Batch 6651/56436] [D loss: 0.379841, acc:  48%] [G loss: 1.022492, adv: 0.063141, recon: 0.040970, id: 0.025084] time: 1:10:02.869563 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 6701/56436] [D loss: 0.320565, acc:  52%] [G loss: 0.774495, adv: 0.095662, recon: 0.025893, id: 0.038351] time: 1:10:35.731622 \n",
      "[Epoch 1/1] [Batch 6751/56436] [D loss: 0.659507, acc:  50%] [G loss: 1.029690, adv: 0.052263, recon: 0.042080, id: 0.046320] time: 1:11:09.193994 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 6801/56436] [D loss: 0.568653, acc:  48%] [G loss: 0.632364, adv: 0.028475, recon: 0.025926, id: 0.024299] time: 1:11:41.850989 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 6851/56436] [D loss: 0.475760, acc:  49%] [G loss: 0.501376, adv: 0.014010, recon: 0.021288, id: 0.027997] time: 1:12:15.181983 \n",
      "[Epoch 1/1] [Batch 6901/56436] [D loss: 0.428942, acc:  50%] [G loss: 0.544127, adv: 0.087661, recon: 0.016986, id: 0.021088] time: 1:12:48.641780 \n",
      "[Epoch 1/1] [Batch 6951/56436] [D loss: 0.550546, acc:  49%] [G loss: 0.516602, adv: 0.010132, recon: 0.022453, id: 0.014688] time: 1:13:21.927294 \n",
      "[Epoch 1/1] [Batch 7001/56436] [D loss: 0.534945, acc:  44%] [G loss: 0.720897, adv: 0.013570, recon: 0.031317, id: 0.030539] time: 1:13:55.250662 \n",
      "[Epoch 1/1] [Batch 7051/56436] [D loss: 0.590822, acc:  25%] [G loss: 0.578578, adv: 0.020554, recon: 0.024607, id: 0.004050] time: 1:14:26.845105 \n",
      "[Epoch 1/1] [Batch 7101/56436] [D loss: 0.444723, acc:  49%] [G loss: 0.677336, adv: 0.017436, recon: 0.030378, id: 0.014668] time: 1:14:57.957406 \n",
      "[Epoch 1/1] [Batch 7151/56436] [D loss: 0.387625, acc:  51%] [G loss: 0.575785, adv: 0.047950, recon: 0.022206, id: 0.017144] time: 1:15:29.044777 \n",
      "[Epoch 1/1] [Batch 7201/56436] [D loss: 0.467699, acc:  49%] [G loss: 0.652403, adv: 0.023352, recon: 0.027326, id: 0.011060] time: 1:16:01.041026 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 7251/56436] [D loss: 0.476571, acc:  48%] [G loss: 0.976074, adv: 0.043449, recon: 0.039473, id: 0.061099] time: 1:16:33.892986 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 7301/56436] [D loss: 0.511501, acc:  49%] [G loss: 0.654196, adv: 0.015065, recon: 0.028438, id: 0.016133] time: 1:17:06.461621 \n",
      "[Epoch 1/1] [Batch 7351/56436] [D loss: 0.423526, acc:  50%] [G loss: 0.442346, adv: 0.030688, recon: 0.017131, id: 0.016716] time: 1:17:39.755275 \n",
      "[Epoch 1/1] [Batch 7401/56436] [D loss: 0.520043, acc:  49%] [G loss: 0.500088, adv: 0.008691, recon: 0.021735, id: 0.012438] time: 1:18:11.435587 \n",
      "[Epoch 1/1] [Batch 7451/56436] [D loss: 0.450293, acc:  49%] [G loss: 0.508657, adv: 0.017740, recon: 0.020993, id: 0.014986] time: 1:18:43.863906 \n",
      "[Epoch 1/1] [Batch 7501/56436] [D loss: 0.516995, acc:  51%] [G loss: 0.668163, adv: 0.031337, recon: 0.028346, id: 0.015670] time: 1:19:16.692985 \n",
      "[Epoch 1/1] [Batch 7551/56436] [D loss: 0.446467, acc:  49%] [G loss: 0.450722, adv: 0.016464, recon: 0.019176, id: 0.014073] time: 1:19:49.339739 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 7601/56436] [D loss: 0.447102, acc:  49%] [G loss: 0.621084, adv: 0.027948, recon: 0.025987, id: 0.009548] time: 1:20:20.794401 \n",
      "[Epoch 1/1] [Batch 7651/56436] [D loss: 0.320392, acc:  50%] [G loss: 0.673314, adv: 0.072753, recon: 0.023866, id: 0.015385] time: 1:20:51.904070 \n",
      "[Epoch 1/1] [Batch 7701/56436] [D loss: 0.469852, acc:  48%] [G loss: 0.449623, adv: 0.007148, recon: 0.020000, id: 0.013377] time: 1:21:23.049662 \n",
      "[Epoch 1/1] [Batch 7751/56436] [D loss: 0.399896, acc:  50%] [G loss: 0.650734, adv: 0.046261, recon: 0.025626, id: 0.006626] time: 1:21:54.159113 \n",
      "[Epoch 1/1] [Batch 7801/56436] [D loss: 0.535203, acc:  48%] [G loss: 0.951510, adv: 0.030235, recon: 0.041319, id: 0.024461] time: 1:22:25.233728 \n",
      "[Epoch 1/1] [Batch 7851/56436] [D loss: 0.355618, acc:  47%] [G loss: 0.819363, adv: 0.048045, recon: 0.032355, id: 0.025991] time: 1:22:56.777811 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 7901/56436] [D loss: 0.297465, acc:  50%] [G loss: 0.585211, adv: 0.078789, recon: 0.019574, id: 0.006645] time: 1:23:27.500607 \n",
      "[Epoch 1/1] [Batch 7951/56436] [D loss: 0.651002, acc:  49%] [G loss: 0.542137, adv: 0.027358, recon: 0.022014, id: 0.014264] time: 1:23:58.748260 \n",
      "[Epoch 1/1] [Batch 8001/56436] [D loss: 0.376791, acc:  50%] [G loss: 0.503307, adv: 0.037945, recon: 0.019189, id: 0.023178] time: 1:24:29.888550 \n",
      "[Epoch 1/1] [Batch 8051/56436] [D loss: 0.477821, acc:  49%] [G loss: 0.739751, adv: 0.019555, recon: 0.029492, id: 0.023683] time: 1:25:01.032434 \n",
      "[Epoch 1/1] [Batch 8101/56436] [D loss: 0.441440, acc:  50%] [G loss: 0.726722, adv: 0.025904, recon: 0.030463, id: 0.027926] time: 1:25:32.367595 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 8151/56436] [D loss: 0.533836, acc:  47%] [G loss: 0.551154, adv: 0.008221, recon: 0.024497, id: 0.007262] time: 1:26:03.452356 \n",
      "[Epoch 1/1] [Batch 8201/56436] [D loss: 0.558985, acc:  49%] [G loss: 0.626305, adv: 0.023301, recon: 0.025435, id: 0.022628] time: 1:26:35.088873 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 8251/56436] [D loss: 0.497947, acc:  49%] [G loss: 0.621431, adv: 0.019093, recon: 0.026190, id: 0.011423] time: 1:27:06.895092 \n",
      "[Epoch 1/1] [Batch 8301/56436] [D loss: 0.388450, acc:  50%] [G loss: 0.809887, adv: 0.054945, recon: 0.031527, id: 0.018714] time: 1:27:39.976993 \n",
      "[Epoch 1/1] [Batch 8351/56436] [D loss: 0.500083, acc:  49%] [G loss: 0.785913, adv: 0.019846, recon: 0.028192, id: 0.015032] time: 1:28:13.604783 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 8401/56436] [D loss: 0.514254, acc:  50%] [G loss: 0.618212, adv: 0.010700, recon: 0.022960, id: 0.021974] time: 1:28:46.652074 \n",
      "[Epoch 1/1] [Batch 8451/56436] [D loss: 0.507560, acc:  48%] [G loss: 0.458510, adv: 0.007213, recon: 0.015526, id: 0.010450] time: 1:29:20.393670 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 8501/56436] [D loss: 0.429711, acc:  50%] [G loss: 0.575393, adv: 0.026400, recon: 0.023031, id: 0.018449] time: 1:29:52.086616 \n",
      "[Epoch 1/1] [Batch 8551/56436] [D loss: 0.600808, acc:  47%] [G loss: 0.601627, adv: 0.018157, recon: 0.024331, id: 0.012422] time: 1:30:25.734171 \n",
      "[Epoch 1/1] [Batch 8601/56436] [D loss: 0.511243, acc:  48%] [G loss: 0.344523, adv: 0.015683, recon: 0.012905, id: 0.006615] time: 1:30:59.221607 \n",
      "[Epoch 1/1] [Batch 8651/56436] [D loss: 0.332918, acc:  51%] [G loss: 0.436271, adv: 0.072628, recon: 0.012192, id: 0.014643] time: 1:31:32.250362 \n",
      "[Epoch 1/1] [Batch 8701/56436] [D loss: 0.474239, acc:  49%] [G loss: 0.631252, adv: 0.009279, recon: 0.027789, id: 0.011047] time: 1:32:04.782146 \n",
      "[Epoch 1/1] [Batch 8751/56436] [D loss: 0.482045, acc:  50%] [G loss: 0.803111, adv: 0.023428, recon: 0.034189, id: 0.052690] time: 1:32:36.025500 \n",
      "[Epoch 1/1] [Batch 8801/56436] [D loss: 0.552972, acc:  25%] [G loss: 0.656239, adv: 0.034457, recon: 0.025989, id: 0.016237] time: 1:33:07.828515 \n",
      "[Epoch 1/1] [Batch 8851/56436] [D loss: 0.488895, acc:  49%] [G loss: 0.439712, adv: 0.011112, recon: 0.018752, id: 0.013996] time: 1:33:39.287502 \n",
      "[Epoch 1/1] [Batch 8901/56436] [D loss: 0.515771, acc:  47%] [G loss: 0.750401, adv: 0.021741, recon: 0.032596, id: 0.008736] time: 1:34:10.641900 \n",
      "[Epoch 1/1] [Batch 8951/56436] [D loss: 0.493828, acc:  49%] [G loss: 0.405407, adv: 0.011605, recon: 0.017380, id: 0.024544] time: 1:34:43.230442 \n",
      "[Epoch 1/1] [Batch 9001/56436] [D loss: 0.466794, acc:  49%] [G loss: 0.515291, adv: 0.013743, recon: 0.022157, id: 0.017717] time: 1:35:16.189147 \n",
      "[Epoch 1/1] [Batch 9051/56436] [D loss: 0.409585, acc:  50%] [G loss: 0.806727, adv: 0.034602, recon: 0.032640, id: 0.015336] time: 1:35:49.095646 \n",
      "[Epoch 1/1] [Batch 9101/56436] [D loss: 0.612351, acc:  49%] [G loss: 0.286305, adv: 0.018454, recon: 0.011271, id: 0.008964] time: 1:36:22.546201 \n",
      "[Epoch 1/1] [Batch 9151/56436] [D loss: 0.551026, acc:  49%] [G loss: 0.480603, adv: 0.015925, recon: 0.019052, id: 0.026760] time: 1:36:54.362081 \n",
      "[Epoch 1/1] [Batch 9201/56436] [D loss: 0.586840, acc:  49%] [G loss: 0.471910, adv: 0.018568, recon: 0.019221, id: 0.021129] time: 1:37:25.613128 \n",
      "[Epoch 1/1] [Batch 9251/56436] [D loss: 0.496267, acc:  50%] [G loss: 0.939287, adv: 0.025037, recon: 0.040005, id: 0.031659] time: 1:37:56.743254 \n",
      "[Epoch 1/1] [Batch 9301/56436] [D loss: 0.519830, acc:  48%] [G loss: 0.555904, adv: 0.012457, recon: 0.023784, id: 0.017837] time: 1:38:28.124206 \n",
      "[Epoch 1/1] [Batch 9351/56436] [D loss: 0.503548, acc:  50%] [G loss: 0.522246, adv: 0.015589, recon: 0.022274, id: 0.026255] time: 1:38:59.486018 \n",
      "[Epoch 1/1] [Batch 9401/56436] [D loss: 0.386069, acc:  57%] [G loss: 0.949125, adv: 0.133459, recon: 0.031287, id: 0.018141] time: 1:39:30.899795 \n",
      "[Epoch 1/1] [Batch 9451/56436] [D loss: 0.595601, acc:  49%] [G loss: 0.761371, adv: 0.017977, recon: 0.032496, id: 0.048522] time: 1:40:02.898197 \n",
      "[Epoch 1/1] [Batch 9501/56436] [D loss: 0.587275, acc:  44%] [G loss: 0.965312, adv: 0.042513, recon: 0.040441, id: 0.042853] time: 1:40:34.432987 \n",
      "[Epoch 1/1] [Batch 9551/56436] [D loss: 0.531021, acc:  49%] [G loss: 0.689740, adv: 0.008664, recon: 0.030563, id: 0.021657] time: 1:41:05.725537 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 9601/56436] [D loss: 0.496837, acc:  50%] [G loss: 0.574642, adv: 0.014408, recon: 0.024510, id: 0.017696] time: 1:41:37.327388 \n",
      "[Epoch 1/1] [Batch 9651/56436] [D loss: 0.195863, acc:  75%] [G loss: 2.155529, adv: 0.263066, recon: 0.068598, id: 0.239390] time: 1:42:09.039753 \n",
      "[Epoch 1/1] [Batch 9701/56436] [D loss: 0.488217, acc:  50%] [G loss: 0.317221, adv: 0.012352, recon: 0.013461, id: 0.014853] time: 1:42:40.304635 \n",
      "[Epoch 1/1] [Batch 9751/56436] [D loss: 0.550280, acc:  25%] [G loss: 0.764920, adv: 0.019248, recon: 0.034292, id: 0.014929] time: 1:43:12.224535 \n",
      "[Epoch 1/1] [Batch 9801/56436] [D loss: 0.556234, acc:  48%] [G loss: 0.171377, adv: 0.010243, recon: 0.006555, id: 0.009947] time: 1:43:44.794627 \n",
      "[Epoch 1/1] [Batch 9851/56436] [D loss: 0.405359, acc:  50%] [G loss: 0.704364, adv: 0.040463, recon: 0.028130, id: 0.028039] time: 1:44:16.760711 \n",
      "[Epoch 1/1] [Batch 9901/56436] [D loss: 0.462967, acc:  48%] [G loss: 0.474604, adv: 0.019623, recon: 0.019041, id: 0.027136] time: 1:44:48.686022 \n",
      "[Epoch 1/1] [Batch 9951/56436] [D loss: 0.413083, acc:  50%] [G loss: 0.276351, adv: 0.059054, recon: 0.006553, id: 0.017979] time: 1:45:21.159166 \n",
      "[Epoch 1/1] [Batch 10001/56436] [D loss: 0.509443, acc:  49%] [G loss: 0.376318, adv: 0.007239, recon: 0.015942, id: 0.022589] time: 1:45:52.964662 \n",
      "[Epoch 1/1] [Batch 10051/56436] [D loss: 0.514020, acc:  49%] [G loss: 0.297475, adv: 0.011031, recon: 0.012604, id: 0.013920] time: 1:46:31.855770 \n",
      "[Epoch 1/1] [Batch 10101/56436] [D loss: 0.461264, acc:  49%] [G loss: 0.476464, adv: 0.009872, recon: 0.020666, id: 0.012661] time: 1:47:04.504725 \n",
      "[Epoch 1/1] [Batch 10151/56436] [D loss: 0.552686, acc:  49%] [G loss: 0.396971, adv: 0.009201, recon: 0.017179, id: 0.008694] time: 1:47:36.440524 \n",
      "[Epoch 1/1] [Batch 10201/56436] [D loss: 0.427270, acc:  50%] [G loss: 0.568066, adv: 0.021351, recon: 0.023521, id: 0.032181] time: 1:48:08.147508 \n",
      "[Epoch 1/1] [Batch 10251/56436] [D loss: 0.563077, acc:  49%] [G loss: 0.684537, adv: 0.018478, recon: 0.028474, id: 0.018743] time: 1:48:40.037724 \n",
      "[Epoch 1/1] [Batch 10301/56436] [D loss: 0.505590, acc:  48%] [G loss: 0.345458, adv: 0.015377, recon: 0.014270, id: 0.011118] time: 1:49:12.752084 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 10351/56436] [D loss: 0.609779, acc:  49%] [G loss: 0.450719, adv: 0.023375, recon: 0.014052, id: 0.012065] time: 1:49:44.141581 \n",
      "[Epoch 1/1] [Batch 10401/56436] [D loss: 0.513268, acc:  48%] [G loss: 0.443399, adv: 0.008039, recon: 0.019424, id: 0.007593] time: 1:50:16.104659 \n",
      "[Epoch 1/1] [Batch 10451/56436] [D loss: 0.474847, acc:  49%] [G loss: 0.611452, adv: 0.019030, recon: 0.026321, id: 0.011984] time: 1:50:48.067907 \n",
      "[Epoch 1/1] [Batch 10501/56436] [D loss: 0.486578, acc:  51%] [G loss: 0.618272, adv: 0.042571, recon: 0.024503, id: 0.027060] time: 1:51:20.410551 \n",
      "[Epoch 1/1] [Batch 10551/56436] [D loss: 0.537377, acc:  50%] [G loss: 0.822109, adv: 0.021651, recon: 0.035497, id: 0.022267] time: 1:51:52.074660 \n",
      "[Epoch 1/1] [Batch 10601/56436] [D loss: 0.578977, acc:  49%] [G loss: 0.773287, adv: 0.026596, recon: 0.033277, id: 0.020289] time: 1:52:24.023347 \n",
      "[Epoch 1/1] [Batch 10651/56436] [D loss: 0.621192, acc:  25%] [G loss: 0.702796, adv: 0.025350, recon: 0.029702, id: 0.027633] time: 1:52:55.149683 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 10701/56436] [D loss: 0.404577, acc:  50%] [G loss: 0.512744, adv: 0.033210, recon: 0.019838, id: 0.016687] time: 1:53:26.704299 \n",
      "[Epoch 1/1] [Batch 10751/56436] [D loss: 0.533697, acc:  50%] [G loss: 0.586332, adv: 0.010033, recon: 0.025613, id: 0.044941] time: 1:53:58.909566 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 10801/56436] [D loss: 0.453987, acc:  48%] [G loss: 0.788124, adv: 0.022544, recon: 0.034136, id: 0.030455] time: 1:54:30.200679 \n",
      "[Epoch 1/1] [Batch 10851/56436] [D loss: 0.400147, acc:  50%] [G loss: 0.742738, adv: 0.039675, recon: 0.027653, id: 0.013861] time: 1:55:02.362291 \n",
      "[Epoch 1/1] [Batch 10901/56436] [D loss: 0.529848, acc:  50%] [G loss: 0.759234, adv: 0.014213, recon: 0.030025, id: 0.024797] time: 1:55:34.447649 \n",
      "[Epoch 1/1] [Batch 10951/56436] [D loss: 0.626990, acc:  49%] [G loss: 0.603768, adv: 0.021121, recon: 0.024825, id: 0.019844] time: 1:56:06.570836 \n",
      "[Epoch 1/1] [Batch 11001/56436] [D loss: 0.360205, acc:  29%] [G loss: 0.938194, adv: 0.120742, recon: 0.030572, id: 0.044478] time: 1:56:38.420116 \n",
      "[Epoch 1/1] [Batch 11051/56436] [D loss: 0.473420, acc:  49%] [G loss: 0.390074, adv: 0.009587, recon: 0.016355, id: 0.010007] time: 1:57:09.970134 \n",
      "[Epoch 1/1] [Batch 11101/56436] [D loss: 0.698998, acc:  49%] [G loss: 0.534586, adv: 0.046490, recon: 0.019653, id: 0.011215] time: 1:57:42.446544 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 11151/56436] [D loss: 0.480443, acc:  49%] [G loss: 0.466901, adv: 0.012097, recon: 0.020210, id: 0.011773] time: 1:58:14.194054 \n",
      "[Epoch 1/1] [Batch 11201/56436] [D loss: 0.211604, acc:  75%] [G loss: 3.287635, adv: 0.740018, recon: 0.079699, id: 0.170917] time: 1:58:45.879566 \n",
      "[Epoch 1/1] [Batch 11251/56436] [D loss: 0.530388, acc:  45%] [G loss: 0.821714, adv: 0.013071, recon: 0.036127, id: 0.045866] time: 1:59:17.763817 \n",
      "[Epoch 1/1] [Batch 11301/56436] [D loss: 0.502598, acc:  49%] [G loss: 0.688304, adv: 0.015305, recon: 0.030072, id: 0.020530] time: 1:59:49.979080 \n",
      "[Epoch 1/1] [Batch 11351/56436] [D loss: 0.455601, acc:  49%] [G loss: 2.226207, adv: 0.269225, recon: 0.077675, id: 0.111719] time: 2:00:21.666727 \n",
      "[Epoch 1/1] [Batch 11401/56436] [D loss: 0.378924, acc:  49%] [G loss: 0.924749, adv: 0.034401, recon: 0.039528, id: 0.019498] time: 2:00:53.184563 \n",
      "[Epoch 1/1] [Batch 11451/56436] [D loss: 0.578990, acc:  49%] [G loss: 0.554273, adv: 0.015076, recon: 0.023742, id: 0.011984] time: 2:01:24.781837 \n",
      "[Epoch 1/1] [Batch 11501/56436] [D loss: 0.637017, acc:  25%] [G loss: 0.502248, adv: 0.008455, recon: 0.021456, id: 0.026211] time: 2:01:56.951150 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 11551/56436] [D loss: 0.398378, acc:  50%] [G loss: 0.575572, adv: 0.039205, recon: 0.022640, id: 0.011591] time: 2:02:28.194641 \n",
      "[Epoch 1/1] [Batch 11601/56436] [D loss: 0.556907, acc:  50%] [G loss: 0.753475, adv: 0.019399, recon: 0.032650, id: 0.020264] time: 2:02:59.630263 \n",
      "[Epoch 1/1] [Batch 11651/56436] [D loss: 0.478621, acc:  49%] [G loss: 0.721128, adv: 0.032171, recon: 0.029950, id: 0.008094] time: 2:03:31.147307 \n",
      "[Epoch 1/1] [Batch 11701/56436] [D loss: 0.413902, acc:  49%] [G loss: 0.424188, adv: 0.032916, recon: 0.016256, id: 0.002611] time: 2:04:02.236447 \n",
      "[Epoch 1/1] [Batch 11751/56436] [D loss: 0.461857, acc:  51%] [G loss: 0.552279, adv: 0.023635, recon: 0.023359, id: 0.014658] time: 2:04:33.384214 \n",
      "[Epoch 1/1] [Batch 11801/56436] [D loss: 0.470474, acc:  51%] [G loss: 0.602218, adv: 0.043555, recon: 0.023728, id: 0.024862] time: 2:05:04.623090 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 11851/56436] [D loss: 0.526795, acc:  48%] [G loss: 0.365645, adv: 0.016354, recon: 0.015063, id: 0.013393] time: 2:05:35.526946 \n",
      "[Epoch 1/1] [Batch 11901/56436] [D loss: 0.468039, acc:  49%] [G loss: 0.690651, adv: 0.014732, recon: 0.030258, id: 0.013046] time: 2:06:07.727124 \n",
      "[Epoch 1/1] [Batch 11951/56436] [D loss: 0.502621, acc:  49%] [G loss: 0.425128, adv: 0.013001, recon: 0.018087, id: 0.019296] time: 2:06:40.077127 \n",
      "[Epoch 1/1] [Batch 12001/56436] [D loss: 0.432836, acc:  51%] [G loss: 0.609886, adv: 0.049484, recon: 0.023185, id: 0.029349] time: 2:07:12.040279 \n",
      "[Epoch 1/1] [Batch 12051/56436] [D loss: 0.555378, acc:  49%] [G loss: 0.397162, adv: 0.008165, recon: 0.016554, id: 0.027131] time: 2:07:44.680057 \n",
      "[Epoch 1/1] [Batch 12101/56436] [D loss: 0.507771, acc:  49%] [G loss: 0.555910, adv: 0.012730, recon: 0.024078, id: 0.018130] time: 2:08:16.473330 \n",
      "[Epoch 1/1] [Batch 12151/56436] [D loss: 0.458002, acc:  50%] [G loss: 0.311509, adv: 0.014558, recon: 0.012889, id: 0.009421] time: 2:08:47.796473 \n",
      "[Epoch 1/1] [Batch 12201/56436] [D loss: 0.482560, acc:  50%] [G loss: 0.616197, adv: 0.026243, recon: 0.025658, id: 0.024941] time: 2:09:19.513849 \n",
      "[Epoch 1/1] [Batch 12251/56436] [D loss: 0.498356, acc:  50%] [G loss: 0.570796, adv: 0.014782, recon: 0.024495, id: 0.027864] time: 2:09:51.008875 \n",
      "[Epoch 1/1] [Batch 12301/56436] [D loss: 0.554081, acc:  50%] [G loss: 0.664392, adv: 0.020078, recon: 0.028469, id: 0.012565] time: 2:10:23.008039 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 12351/56436] [D loss: 0.504844, acc:  48%] [G loss: 0.443468, adv: 0.009018, recon: 0.019257, id: 0.006207] time: 2:10:54.857268 \n",
      "[Epoch 1/1] [Batch 12401/56436] [D loss: 0.509341, acc:  49%] [G loss: 0.710971, adv: 0.008998, recon: 0.030956, id: 0.028372] time: 2:11:26.551037 \n",
      "[Epoch 1/1] [Batch 12451/56436] [D loss: 0.553453, acc:  49%] [G loss: 0.242696, adv: 0.011867, recon: 0.009888, id: 0.005731] time: 2:11:59.398202 \n",
      "[Epoch 1/1] [Batch 12501/56436] [D loss: 0.448790, acc:  50%] [G loss: 0.710369, adv: 0.017285, recon: 0.030935, id: 0.009108] time: 2:12:31.593057 \n",
      "[Epoch 1/1] [Batch 12551/56436] [D loss: 0.464867, acc:  49%] [G loss: 0.648673, adv: 0.059917, recon: 0.024514, id: 0.016573] time: 2:13:03.815838 \n",
      "[Epoch 1/1] [Batch 12601/56436] [D loss: 0.488945, acc:  50%] [G loss: 0.457587, adv: 0.014979, recon: 0.019523, id: 0.022219] time: 2:13:36.577988 \n",
      "[Epoch 1/1] [Batch 12651/56436] [D loss: 0.521556, acc:  48%] [G loss: 0.483855, adv: 0.007505, recon: 0.021300, id: 0.003636] time: 2:14:07.925653 \n",
      "[Epoch 1/1] [Batch 12701/56436] [D loss: 0.437194, acc:  49%] [G loss: 0.544718, adv: 0.012444, recon: 0.023690, id: 0.012247] time: 2:14:39.037135 \n",
      "[Epoch 1/1] [Batch 12751/56436] [D loss: 0.618868, acc:  25%] [G loss: 0.556672, adv: 0.034405, recon: 0.022142, id: 0.012223] time: 2:15:10.243949 \n",
      "[Epoch 1/1] [Batch 12801/56436] [D loss: 0.559858, acc:  25%] [G loss: 0.891359, adv: 0.033032, recon: 0.035459, id: 0.089621] time: 2:15:41.442126 \n",
      "[Epoch 1/1] [Batch 12851/56436] [D loss: 0.458171, acc:  49%] [G loss: 0.663855, adv: 0.015413, recon: 0.029182, id: 0.024116] time: 2:16:12.664959 \n",
      "[Epoch 1/1] [Batch 12901/56436] [D loss: 0.611955, acc:  26%] [G loss: 0.828985, adv: 0.079987, recon: 0.029485, id: 0.037461] time: 2:16:43.827515 \n",
      "[Epoch 1/1] [Batch 12951/56436] [D loss: 0.460358, acc:  50%] [G loss: 0.282593, adv: 0.025425, recon: 0.009841, id: 0.018204] time: 2:17:14.994522 \n",
      "[Epoch 1/1] [Batch 13001/56436] [D loss: 0.461021, acc:  50%] [G loss: 0.610357, adv: 0.017963, recon: 0.025525, id: 0.014822] time: 2:17:46.076884 \n",
      "[Epoch 1/1] [Batch 13051/56436] [D loss: 0.425421, acc:  49%] [G loss: 1.023568, adv: 0.022874, recon: 0.044916, id: 0.023313] time: 2:18:17.397739 \n",
      "[Epoch 1/1] [Batch 13101/56436] [D loss: 0.451267, acc:  49%] [G loss: 0.482472, adv: 0.010743, recon: 0.021082, id: 0.018825] time: 2:18:48.891082 \n",
      "[Epoch 1/1] [Batch 13151/56436] [D loss: 0.582134, acc:  25%] [G loss: 0.642272, adv: 0.019055, recon: 0.027836, id: 0.027792] time: 2:19:20.181469 \n",
      "[Epoch 1/1] [Batch 13201/56436] [D loss: 0.545529, acc:  49%] [G loss: 0.443877, adv: 0.008138, recon: 0.019592, id: 0.007721] time: 2:19:51.379247 \n",
      "[Epoch 1/1] [Batch 13251/56436] [D loss: 0.559624, acc:  49%] [G loss: 0.344183, adv: 0.013992, recon: 0.013508, id: 0.012100] time: 2:20:22.745744 \n",
      "[Epoch 1/1] [Batch 13301/56436] [D loss: 0.481057, acc:  48%] [G loss: 0.380262, adv: 0.024785, recon: 0.014715, id: 0.002354] time: 2:20:54.085366 \n",
      "[Epoch 1/1] [Batch 13351/56436] [D loss: 0.450788, acc:  48%] [G loss: 0.281504, adv: 0.007905, recon: 0.011943, id: 0.004200] time: 2:21:25.548049 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 13401/56436] [D loss: 0.543887, acc:  48%] [G loss: 0.522970, adv: 0.008652, recon: 0.022956, id: 0.025658] time: 2:21:57.051512 \n",
      "[Epoch 1/1] [Batch 13451/56436] [D loss: 0.398604, acc:  49%] [G loss: 0.473376, adv: 0.025678, recon: 0.019491, id: 0.006015] time: 2:22:28.323451 \n",
      "[Epoch 1/1] [Batch 13501/56436] [D loss: 0.549624, acc:  47%] [G loss: 0.733324, adv: 0.014498, recon: 0.031532, id: 0.044714] time: 2:22:59.659843 \n",
      "[Epoch 1/1] [Batch 13551/56436] [D loss: 0.490587, acc:  46%] [G loss: 0.517400, adv: 0.022276, recon: 0.021494, id: 0.016875] time: 2:23:31.222793 \n",
      "[Epoch 1/1] [Batch 13601/56436] [D loss: 0.567951, acc:  48%] [G loss: 0.396310, adv: 0.009563, recon: 0.017127, id: 0.008248] time: 2:24:02.543733 \n",
      "[Epoch 1/1] [Batch 13651/56436] [D loss: 0.577214, acc:  49%] [G loss: 0.479899, adv: 0.018176, recon: 0.020228, id: 0.016782] time: 2:24:33.952713 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 13701/56436] [D loss: 0.603718, acc:  48%] [G loss: 0.215691, adv: 0.019770, recon: 0.007851, id: 0.004520] time: 2:25:04.722654 \n",
      "[Epoch 1/1] [Batch 13751/56436] [D loss: 0.484996, acc:  50%] [G loss: 0.637820, adv: 0.011557, recon: 0.027833, id: 0.026500] time: 2:25:36.080075 \n",
      "[Epoch 1/1] [Batch 13801/56436] [D loss: 0.491423, acc:  49%] [G loss: 0.343786, adv: 0.013144, recon: 0.014227, id: 0.008295] time: 2:26:07.145508 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 13851/56436] [D loss: 0.493941, acc:  49%] [G loss: 0.519308, adv: 0.012708, recon: 0.022353, id: 0.005609] time: 2:26:37.725340 \n",
      "[Epoch 1/1] [Batch 13901/56436] [D loss: 0.457904, acc:  49%] [G loss: 0.472907, adv: 0.019213, recon: 0.019839, id: 0.004863] time: 2:27:08.925163 \n",
      "[Epoch 1/1] [Batch 13951/56436] [D loss: 0.574240, acc:  49%] [G loss: 0.516220, adv: 0.019399, recon: 0.021782, id: 0.007454] time: 2:27:39.948712 \n",
      "[Epoch 1/1] [Batch 14001/56436] [D loss: 0.632439, acc:  25%] [G loss: 0.149125, adv: 0.011984, recon: 0.005827, id: 0.000919] time: 2:28:11.025444 \n",
      "[Epoch 1/1] [Batch 14051/56436] [D loss: 0.497477, acc:  47%] [G loss: 0.554133, adv: 0.012847, recon: 0.024278, id: 0.008974] time: 2:28:42.100545 \n",
      "[Epoch 1/1] [Batch 14101/56436] [D loss: 0.461887, acc:  48%] [G loss: 0.354740, adv: 0.006788, recon: 0.015406, id: 0.006194] time: 2:29:13.186634 \n",
      "[Epoch 1/1] [Batch 14151/56436] [D loss: 0.435360, acc:  50%] [G loss: 0.744115, adv: 0.023137, recon: 0.031347, id: 0.032812] time: 2:29:44.221371 \n",
      "[Epoch 1/1] [Batch 14201/56436] [D loss: 0.680258, acc:  38%] [G loss: 0.468100, adv: 0.043466, recon: 0.017293, id: 0.018086] time: 2:30:15.247879 \n",
      "[Epoch 1/1] [Batch 14251/56436] [D loss: 0.486516, acc:  47%] [G loss: 0.522579, adv: 0.013846, recon: 0.022143, id: 0.003873] time: 2:30:46.352859 \n",
      "[Epoch 1/1] [Batch 14301/56436] [D loss: 0.572734, acc:  50%] [G loss: 0.375835, adv: 0.013600, recon: 0.015746, id: 0.018508] time: 2:31:17.915772 \n",
      "[Epoch 1/1] [Batch 14351/56436] [D loss: 0.455401, acc:  48%] [G loss: 0.495898, adv: 0.030277, recon: 0.019777, id: 0.023497] time: 2:31:49.087987 \n",
      "[Epoch 1/1] [Batch 14401/56436] [D loss: 0.435258, acc:  48%] [G loss: 0.532894, adv: 0.013224, recon: 0.023020, id: 0.017237] time: 2:32:20.185726 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 14451/56436] [D loss: 0.524670, acc:  49%] [G loss: 0.678039, adv: 0.010797, recon: 0.030064, id: 0.011339] time: 2:32:50.674317 \n",
      "[Epoch 1/1] [Batch 14501/56436] [D loss: 0.516234, acc:  50%] [G loss: 0.514857, adv: 0.027340, recon: 0.021082, id: 0.022661] time: 2:33:21.802044 \n",
      "[Epoch 1/1] [Batch 14551/56436] [D loss: 0.583952, acc:  48%] [G loss: 0.331704, adv: 0.010576, recon: 0.013807, id: 0.004977] time: 2:33:52.821237 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 14601/56436] [D loss: 0.520540, acc:  48%] [G loss: 0.643711, adv: 0.014526, recon: 0.028551, id: 0.022629] time: 2:34:22.748780 \n",
      "[Epoch 1/1] [Batch 14651/56436] [D loss: 0.438107, acc:  50%] [G loss: 0.548577, adv: 0.018601, recon: 0.023038, id: 0.023042] time: 2:34:53.800952 \n",
      "[Epoch 1/1] [Batch 14701/56436] [D loss: 0.451305, acc:  49%] [G loss: 0.762268, adv: 0.018632, recon: 0.032976, id: 0.032045] time: 2:35:24.824432 \n",
      "[Epoch 1/1] [Batch 14751/56436] [D loss: 0.402791, acc:  50%] [G loss: 0.491351, adv: 0.024444, recon: 0.019658, id: 0.020485] time: 2:35:55.958052 \n",
      "[Epoch 1/1] [Batch 14801/56436] [D loss: 0.632109, acc:  49%] [G loss: 0.347324, adv: 0.028224, recon: 0.013114, id: 0.015159] time: 2:36:27.005192 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 14851/56436] [D loss: 0.468816, acc:  28%] [G loss: 0.964995, adv: 0.052200, recon: 0.038959, id: 0.051906] time: 2:36:57.493645 \n",
      "[Epoch 1/1] [Batch 14901/56436] [D loss: 0.389460, acc:  50%] [G loss: 0.394901, adv: 0.031106, recon: 0.015292, id: 0.012359] time: 2:37:28.904389 \n",
      "[Epoch 1/1] [Batch 14951/56436] [D loss: 0.597095, acc:  46%] [G loss: 0.477623, adv: 0.009380, recon: 0.020761, id: 0.014731] time: 2:38:00.016343 \n",
      "[Epoch 1/1] [Batch 15001/56436] [D loss: 0.420675, acc:  49%] [G loss: 0.307207, adv: 0.023210, recon: 0.011868, id: 0.012494] time: 2:38:31.066901 \n",
      "[Epoch 1/1] [Batch 15051/56436] [D loss: 0.498548, acc:  50%] [G loss: 0.385654, adv: 0.010122, recon: 0.016846, id: 0.011437] time: 2:39:08.025408 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 15101/56436] [D loss: 0.489020, acc:  49%] [G loss: 0.529357, adv: 0.011989, recon: 0.023094, id: 0.026229] time: 2:39:38.463751 \n",
      "[Epoch 1/1] [Batch 15151/56436] [D loss: 0.430453, acc:  49%] [G loss: 0.512944, adv: 0.036683, recon: 0.020162, id: 0.018343] time: 2:40:09.585680 \n",
      "[Epoch 1/1] [Batch 15201/56436] [D loss: 0.399459, acc:  50%] [G loss: 0.405217, adv: 0.032598, recon: 0.015196, id: 0.011791] time: 2:40:40.447632 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 15251/56436] [D loss: 0.558441, acc:  48%] [G loss: 0.464415, adv: 0.016294, recon: 0.019549, id: 0.005196] time: 2:41:10.846989 \n",
      "[Epoch 1/1] [Batch 15301/56436] [D loss: 0.429776, acc:  48%] [G loss: 0.635759, adv: 0.023657, recon: 0.027072, id: 0.005304] time: 2:41:42.043469 \n",
      "[Epoch 1/1] [Batch 15351/56436] [D loss: 0.477767, acc:  49%] [G loss: 0.565835, adv: 0.012061, recon: 0.024521, id: 0.008887] time: 2:42:13.175982 \n",
      "[Epoch 1/1] [Batch 15401/56436] [D loss: 0.617239, acc:  49%] [G loss: 0.526221, adv: 0.017636, recon: 0.022687, id: 0.005894] time: 2:42:44.189184 \n",
      "[Epoch 1/1] [Batch 15451/56436] [D loss: 0.612887, acc:  49%] [G loss: 0.405620, adv: 0.019443, recon: 0.016454, id: 0.013452] time: 2:43:15.210139 \n",
      "[Epoch 1/1] [Batch 15501/56436] [D loss: 0.483403, acc:  49%] [G loss: 0.577474, adv: 0.010527, recon: 0.025273, id: 0.009979] time: 2:43:46.285856 \n",
      "[Epoch 1/1] [Batch 15551/56436] [D loss: 0.508326, acc:  49%] [G loss: 0.779773, adv: 0.010971, recon: 0.034270, id: 0.026274] time: 2:44:17.448257 \n",
      "[Epoch 1/1] [Batch 15601/56436] [D loss: 0.485929, acc:  50%] [G loss: 0.596136, adv: 0.013546, recon: 0.025814, id: 0.023737] time: 2:44:48.782418 \n",
      "[Epoch 1/1] [Batch 15651/56436] [D loss: 0.491756, acc:  49%] [G loss: 0.520002, adv: 0.014252, recon: 0.022711, id: 0.010737] time: 2:45:19.904717 \n",
      "[Epoch 1/1] [Batch 15701/56436] [D loss: 0.496266, acc:  48%] [G loss: 0.340282, adv: 0.010573, recon: 0.014510, id: 0.004783] time: 2:45:51.000310 \n",
      "[Epoch 1/1] [Batch 15751/56436] [D loss: 0.524774, acc:  48%] [G loss: 0.645772, adv: 0.009272, recon: 0.028631, id: 0.023950] time: 2:46:22.106941 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 15801/56436] [D loss: 0.567573, acc:  48%] [G loss: 0.421509, adv: 0.008450, recon: 0.018209, id: 0.005310] time: 2:46:52.660563 \n",
      "[Epoch 1/1] [Batch 15851/56436] [D loss: 0.595767, acc:  50%] [G loss: 0.475123, adv: 0.017437, recon: 0.020358, id: 0.021587] time: 2:47:23.934243 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 15951/56436] [D loss: 0.621509, acc:  49%] [G loss: 0.259193, adv: 0.016776, recon: 0.009844, id: 0.009221] time: 2:48:24.998419 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 16001/56436] [D loss: 0.639256, acc:  50%] [G loss: 0.441510, adv: 0.024216, recon: 0.017883, id: 0.015028] time: 2:48:55.470847 \n",
      "[Epoch 1/1] [Batch 16051/56436] [D loss: 0.498526, acc:  49%] [G loss: 0.429200, adv: 0.006325, recon: 0.018882, id: 0.016422] time: 2:49:26.568378 \n",
      "[Epoch 1/1] [Batch 16101/56436] [D loss: 0.490069, acc:  49%] [G loss: 0.537080, adv: 0.013493, recon: 0.023343, id: 0.011194] time: 2:49:57.618697 \n",
      "[Epoch 1/1] [Batch 16151/56436] [D loss: 0.606499, acc:  46%] [G loss: 0.337886, adv: 0.013806, recon: 0.013586, id: 0.008431] time: 2:50:28.844240 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 16251/56436] [D loss: 0.649136, acc:  28%] [G loss: 0.793880, adv: 0.014531, recon: 0.034872, id: 0.026795] time: 2:51:30.275185 \n",
      "[Epoch 1/1] [Batch 16301/56436] [D loss: 0.448795, acc:  50%] [G loss: 0.540548, adv: 0.012059, recon: 0.023502, id: 0.020135] time: 2:52:01.245276 \n",
      "[Epoch 1/1] [Batch 16351/56436] [D loss: 0.496357, acc:  50%] [G loss: 0.430127, adv: 0.020105, recon: 0.018047, id: 0.010788] time: 2:52:32.286423 \n",
      "[Epoch 1/1] [Batch 16401/56436] [D loss: 0.490376, acc:  49%] [G loss: 0.385838, adv: 0.011692, recon: 0.016547, id: 0.006231] time: 2:53:03.684467 \n",
      "[Epoch 1/1] [Batch 16451/56436] [D loss: 0.537280, acc:  49%] [G loss: 0.358228, adv: 0.008751, recon: 0.015506, id: 0.008412] time: 2:53:34.744162 \n",
      "[Epoch 1/1] [Batch 16501/56436] [D loss: 0.557268, acc:  49%] [G loss: 0.763805, adv: 0.019637, recon: 0.032904, id: 0.020949] time: 2:54:05.725043 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 16551/56436] [D loss: 0.574874, acc:  48%] [G loss: 0.621277, adv: 0.012694, recon: 0.026965, id: 0.003221] time: 2:54:36.279329 \n",
      "[Epoch 1/1] [Batch 16601/56436] [D loss: 0.598780, acc:  50%] [G loss: 0.434488, adv: 0.014939, recon: 0.018537, id: 0.020990] time: 2:55:07.495992 \n",
      "[Epoch 1/1] [Batch 16651/56436] [D loss: 0.552034, acc:  48%] [G loss: 0.364842, adv: 0.010697, recon: 0.015907, id: 0.005518] time: 2:55:38.597202 \n",
      "[Epoch 1/1] [Batch 16701/56436] [D loss: 0.455581, acc:  49%] [G loss: 0.316423, adv: 0.012291, recon: 0.013512, id: 0.010490] time: 2:56:09.618924 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 16751/56436] [D loss: 0.483907, acc:  49%] [G loss: 0.653210, adv: 0.016558, recon: 0.028982, id: 0.007462] time: 2:56:40.088702 \n",
      "[Epoch 1/1] [Batch 16801/56436] [D loss: 0.487133, acc:  49%] [G loss: 0.642102, adv: 0.008888, recon: 0.028486, id: 0.020680] time: 2:57:11.128653 \n",
      "[Epoch 1/1] [Batch 16851/56436] [D loss: 0.524473, acc:  49%] [G loss: 0.305170, adv: 0.006173, recon: 0.013372, id: 0.006994] time: 2:57:42.152956 \n",
      "[Epoch 1/1] [Batch 16901/56436] [D loss: 0.476036, acc:  49%] [G loss: 0.523019, adv: 0.015490, recon: 0.022619, id: 0.019111] time: 2:58:13.206359 \n",
      "[Epoch 1/1] [Batch 16951/56436] [D loss: 0.567911, acc:  48%] [G loss: 0.559733, adv: 0.014579, recon: 0.024299, id: 0.019068] time: 2:58:44.422596 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17001/56436] [D loss: 0.481843, acc:  48%] [G loss: 0.608695, adv: 0.027394, recon: 0.025115, id: 0.014954] time: 2:59:14.971880 \n",
      "[Epoch 1/1] [Batch 17051/56436] [D loss: 0.495663, acc:  48%] [G loss: 0.491915, adv: 0.008791, recon: 0.021769, id: 0.003654] time: 2:59:46.126571 \n",
      "[Epoch 1/1] [Batch 17101/56436] [D loss: 0.527054, acc:  40%] [G loss: 0.562595, adv: 0.015310, recon: 0.024220, id: 0.021460] time: 3:00:17.303618 \n",
      "[Epoch 1/1] [Batch 17151/56436] [D loss: 0.590601, acc:  25%] [G loss: 0.632862, adv: 0.017192, recon: 0.024351, id: 0.079303] time: 3:00:48.361702 \n",
      "[Epoch 1/1] [Batch 17201/56436] [D loss: 0.451273, acc:  48%] [G loss: 0.488575, adv: 0.023302, recon: 0.019746, id: 0.005225] time: 3:01:19.480573 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17251/56436] [D loss: 0.562766, acc:  49%] [G loss: 0.618863, adv: 0.014544, recon: 0.026804, id: 0.015394] time: 3:01:49.834242 \n",
      "[Epoch 1/1] [Batch 17301/56436] [D loss: 0.558592, acc:  28%] [G loss: 0.535796, adv: 0.025264, recon: 0.022123, id: 0.023454] time: 3:02:21.058119 \n",
      "[Epoch 1/1] [Batch 17351/56436] [D loss: 0.536745, acc:  50%] [G loss: 0.607895, adv: 0.010679, recon: 0.026496, id: 0.016135] time: 3:02:52.227004 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17401/56436] [D loss: 0.574181, acc:  42%] [G loss: 0.489937, adv: 0.011113, recon: 0.021261, id: 0.019753] time: 3:03:22.827597 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17451/56436] [D loss: 0.476431, acc:  48%] [G loss: 0.556904, adv: 0.010666, recon: 0.024339, id: 0.005041] time: 3:03:53.235853 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17501/56436] [D loss: 0.561116, acc:  49%] [G loss: 0.424013, adv: 0.015000, recon: 0.018021, id: 0.006410] time: 3:04:23.751111 \n",
      "[Epoch 1/1] [Batch 17551/56436] [D loss: 0.479966, acc:  49%] [G loss: 0.613750, adv: 0.033119, recon: 0.023414, id: 0.014695] time: 3:04:54.793597 \n",
      "[Epoch 1/1] [Batch 17601/56436] [D loss: 0.534476, acc:  47%] [G loss: 0.153422, adv: 0.007382, recon: 0.006289, id: 0.002457] time: 3:05:25.821473 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 17651/56436] [D loss: 0.527154, acc:  49%] [G loss: 0.372244, adv: 0.006339, recon: 0.016139, id: 0.007801] time: 3:05:56.661311 \n",
      "[Epoch 1/1] [Batch 17701/56436] [D loss: 0.587203, acc:  48%] [G loss: 0.582373, adv: 0.011624, recon: 0.025472, id: 0.006345] time: 3:06:27.941547 \n",
      "[Epoch 1/1] [Batch 17751/56436] [D loss: 0.400067, acc:  50%] [G loss: 0.483911, adv: 0.024332, recon: 0.019715, id: 0.006694] time: 3:06:59.145060 \n",
      "[Epoch 1/1] [Batch 17801/56436] [D loss: 0.597713, acc:  26%] [G loss: 0.539658, adv: 0.031819, recon: 0.021498, id: 0.011243] time: 3:07:30.252179 \n",
      "[Epoch 1/1] [Batch 17851/56436] [D loss: 0.439257, acc:  49%] [G loss: 0.531392, adv: 0.015014, recon: 0.022948, id: 0.011350] time: 3:08:01.327435 \n",
      "[Epoch 1/1] [Batch 17901/56436] [D loss: 0.390918, acc:  50%] [G loss: 0.597465, adv: 0.025659, recon: 0.025289, id: 0.010363] time: 3:08:32.464655 \n",
      "[Epoch 1/1] [Batch 17951/56436] [D loss: 0.517603, acc:  49%] [G loss: 0.218514, adv: 0.006480, recon: 0.009495, id: 0.007646] time: 3:09:03.642992 \n",
      "[Epoch 1/1] [Batch 18001/56436] [D loss: 0.478094, acc:  50%] [G loss: 0.463140, adv: 0.010923, recon: 0.020104, id: 0.013060] time: 3:09:34.733752 \n",
      "[Epoch 1/1] [Batch 18051/56436] [D loss: 0.510557, acc:  49%] [G loss: 0.325198, adv: 0.008702, recon: 0.014062, id: 0.010816] time: 3:10:05.794964 \n",
      "[Epoch 1/1] [Batch 18101/56436] [D loss: 0.626892, acc:  45%] [G loss: 0.513786, adv: 0.012756, recon: 0.021610, id: 0.029796] time: 3:10:36.851409 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18151/56436] [D loss: 0.476112, acc:  49%] [G loss: 0.400409, adv: 0.012544, recon: 0.016995, id: 0.015172] time: 3:11:07.310992 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18201/56436] [D loss: 0.624908, acc:  47%] [G loss: 0.695459, adv: 0.017242, recon: 0.028773, id: 0.041599] time: 3:11:37.831183 \n",
      "[Epoch 1/1] [Batch 18251/56436] [D loss: 0.389620, acc:  49%] [G loss: 0.811434, adv: 0.063796, recon: 0.026702, id: 0.013808] time: 3:12:08.756402 \n",
      "[Epoch 1/1] [Batch 18301/56436] [D loss: 0.421885, acc:  49%] [G loss: 0.936827, adv: 0.016826, recon: 0.042220, id: 0.010998] time: 3:12:39.947154 \n",
      "[Epoch 1/1] [Batch 18351/56436] [D loss: 0.459177, acc:  42%] [G loss: 0.446640, adv: 0.031707, recon: 0.017430, id: 0.017061] time: 3:13:11.205155 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18401/56436] [D loss: 0.496944, acc:  49%] [G loss: 0.471118, adv: 0.009054, recon: 0.020431, id: 0.010343] time: 3:13:41.707443 \n",
      "[Epoch 1/1] [Batch 18451/56436] [D loss: 0.481890, acc:  48%] [G loss: 0.379053, adv: 0.014890, recon: 0.016033, id: 0.004567] time: 3:14:12.838247 \n",
      "[Epoch 1/1] [Batch 18501/56436] [D loss: 0.805222, acc:  25%] [G loss: 0.996722, adv: 0.035455, recon: 0.041935, id: 0.035876] time: 3:14:43.978148 \n",
      "[Epoch 1/1] [Batch 18551/56436] [D loss: 0.578269, acc:  49%] [G loss: 0.520326, adv: 0.014935, recon: 0.022195, id: 0.010482] time: 3:15:15.116516 \n",
      "[Epoch 1/1] [Batch 18601/56436] [D loss: 0.484671, acc:  49%] [G loss: 0.523519, adv: 0.007717, recon: 0.022672, id: 0.009432] time: 3:15:46.322558 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18651/56436] [D loss: 0.567606, acc:  45%] [G loss: 0.782025, adv: 0.014147, recon: 0.034143, id: 0.029858] time: 3:16:16.846432 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18701/56436] [D loss: 0.707816, acc:  25%] [G loss: 0.515130, adv: 0.012370, recon: 0.021482, id: 0.034918] time: 3:16:47.203670 \n",
      "[Epoch 1/1] [Batch 18751/56436] [D loss: 0.527276, acc:  48%] [G loss: 0.455365, adv: 0.008667, recon: 0.019870, id: 0.007573] time: 3:17:18.164871 \n",
      "[Epoch 1/1] [Batch 18801/56436] [D loss: 0.459824, acc:  48%] [G loss: 0.454247, adv: 0.015516, recon: 0.019352, id: 0.005389] time: 3:17:49.516163 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 18851/56436] [D loss: 0.563383, acc:  49%] [G loss: 0.346589, adv: 0.012826, recon: 0.014522, id: 0.013897] time: 3:18:20.100947 \n",
      "[Epoch 1/1] [Batch 18901/56436] [D loss: 0.535304, acc:  50%] [G loss: 0.645327, adv: 0.013202, recon: 0.028295, id: 0.031008] time: 3:18:51.140830 \n",
      "[Epoch 1/1] [Batch 18951/56436] [D loss: 0.541681, acc:  49%] [G loss: 0.369005, adv: 0.010513, recon: 0.016024, id: 0.008107] time: 3:19:22.129836 \n",
      "[Epoch 1/1] [Batch 19001/56436] [D loss: 0.498065, acc:  49%] [G loss: 0.472296, adv: 0.009161, recon: 0.020263, id: 0.008329] time: 3:19:53.581071 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 19051/56436] [D loss: 0.436527, acc:  50%] [G loss: 0.556225, adv: 0.024063, recon: 0.023018, id: 0.016681] time: 3:20:24.156428 \n",
      "[Epoch 1/1] [Batch 19101/56436] [D loss: 0.454077, acc:  48%] [G loss: 0.942794, adv: 0.015925, recon: 0.041481, id: 0.027415] time: 3:20:55.356649 \n",
      "[Epoch 1/1] [Batch 19151/56436] [D loss: 0.490764, acc:  49%] [G loss: 0.337186, adv: 0.007635, recon: 0.014621, id: 0.007361] time: 3:21:26.460499 \n",
      "[Epoch 1/1] [Batch 19201/56436] [D loss: 0.523562, acc:  49%] [G loss: 0.222144, adv: 0.022238, recon: 0.006753, id: 0.004868] time: 3:21:57.569363 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 19251/56436] [D loss: 0.464349, acc:  48%] [G loss: 0.476983, adv: 0.016454, recon: 0.020594, id: 0.003333] time: 3:22:28.062321 \n",
      "[Epoch 1/1] [Batch 19301/56436] [D loss: 0.501322, acc:  49%] [G loss: 0.436478, adv: 0.008667, recon: 0.019080, id: 0.012915] time: 3:22:59.209658 \n",
      "[Epoch 1/1] [Batch 19351/56436] [D loss: 0.565195, acc:  48%] [G loss: 0.225175, adv: 0.009353, recon: 0.009293, id: 0.007076] time: 3:23:30.346933 \n",
      "[Epoch 1/1] [Batch 19401/56436] [D loss: 0.486136, acc:  49%] [G loss: 0.416085, adv: 0.012474, recon: 0.017868, id: 0.007426] time: 3:24:01.255183 \n",
      "[Epoch 1/1] [Batch 19451/56436] [D loss: 0.559255, acc:  36%] [G loss: 0.593315, adv: 0.014001, recon: 0.025511, id: 0.032049] time: 3:24:32.300719 \n",
      "[Epoch 1/1] [Batch 19501/56436] [D loss: 0.562240, acc:  49%] [G loss: 0.326256, adv: 0.010483, recon: 0.013984, id: 0.012266] time: 3:25:03.302571 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 19551/56436] [D loss: 0.487406, acc:  49%] [G loss: 0.428431, adv: 0.010964, recon: 0.018333, id: 0.017336] time: 3:25:34.012450 \n",
      "[Epoch 1/1] [Batch 19601/56436] [D loss: 0.500288, acc:  50%] [G loss: 0.519487, adv: 0.009886, recon: 0.022271, id: 0.017839] time: 3:26:05.292724 \n",
      "[Epoch 1/1] [Batch 19651/56436] [D loss: 0.484812, acc:  48%] [G loss: 0.379373, adv: 0.008715, recon: 0.016607, id: 0.003046] time: 3:26:36.354773 \n",
      "[Epoch 1/1] [Batch 19701/56436] [D loss: 0.639334, acc:  25%] [G loss: 0.644763, adv: 0.012295, recon: 0.028342, id: 0.017328] time: 3:27:07.374782 \n",
      "[Epoch 1/1] [Batch 19751/56436] [D loss: 0.677611, acc:  25%] [G loss: 0.337145, adv: 0.016751, recon: 0.013944, id: 0.014462] time: 3:27:38.377040 \n",
      "[Epoch 1/1] [Batch 19801/56436] [D loss: 0.385765, acc:  42%] [G loss: 0.388011, adv: 0.079583, recon: 0.008318, id: 0.052927] time: 3:28:09.478868 \n",
      "[Epoch 1/1] [Batch 19851/56436] [D loss: 0.514860, acc:  49%] [G loss: 0.212002, adv: 0.012757, recon: 0.008189, id: 0.008653] time: 3:28:40.466337 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 19901/56436] [D loss: 0.474988, acc:  35%] [G loss: 0.620778, adv: 0.023984, recon: 0.026054, id: 0.019508] time: 3:29:10.887935 \n",
      "[Epoch 1/1] [Batch 19951/56436] [D loss: 0.498389, acc:  49%] [G loss: 0.234842, adv: 0.006236, recon: 0.010219, id: 0.012310] time: 3:29:41.857128 \n",
      "[Epoch 1/1] [Batch 20001/56436] [D loss: 0.451559, acc:  49%] [G loss: 0.398013, adv: 0.011115, recon: 0.017264, id: 0.006537] time: 3:30:12.910239 \n",
      "[Epoch 1/1] [Batch 20051/56436] [D loss: 0.483921, acc:  49%] [G loss: 0.299455, adv: 0.010954, recon: 0.012972, id: 0.004990] time: 3:30:49.668953 \n",
      "[Epoch 1/1] [Batch 20101/56436] [D loss: 0.629439, acc:  48%] [G loss: 0.444773, adv: 0.018526, recon: 0.018679, id: 0.002271] time: 3:31:20.974196 \n",
      "[Epoch 1/1] [Batch 20151/56436] [D loss: 0.514064, acc:  48%] [G loss: 0.163852, adv: 0.016341, recon: 0.006044, id: 0.002104] time: 3:31:52.394495 \n",
      "[Epoch 1/1] [Batch 20201/56436] [D loss: 0.421589, acc:  50%] [G loss: 0.663406, adv: 0.023494, recon: 0.028190, id: 0.009979] time: 3:32:23.607851 \n",
      "[Epoch 1/1] [Batch 20251/56436] [D loss: 0.496645, acc:  49%] [G loss: 0.335983, adv: 0.008476, recon: 0.014556, id: 0.006155] time: 3:32:54.801545 \n",
      "[Epoch 1/1] [Batch 20301/56436] [D loss: 0.595273, acc:  48%] [G loss: 0.511171, adv: 0.011584, recon: 0.022193, id: 0.002464] time: 3:33:26.042759 \n",
      "[Epoch 1/1] [Batch 20351/56436] [D loss: 0.498585, acc:  49%] [G loss: 0.507421, adv: 0.009144, recon: 0.022377, id: 0.009218] time: 3:33:57.177846 \n",
      "[Epoch 1/1] [Batch 20401/56436] [D loss: 0.530252, acc:  48%] [G loss: 0.466755, adv: 0.009767, recon: 0.019511, id: 0.002514] time: 3:34:28.456237 \n",
      "[Epoch 1/1] [Batch 20451/56436] [D loss: 0.567140, acc:  25%] [G loss: 0.335774, adv: 0.033760, recon: 0.012102, id: 0.008772] time: 3:34:59.506890 \n",
      "[Epoch 1/1] [Batch 20501/56436] [D loss: 0.582108, acc:  48%] [G loss: 0.304224, adv: 0.010933, recon: 0.010550, id: 0.008968] time: 3:35:30.709510 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 20551/56436] [D loss: 0.567920, acc:  48%] [G loss: 0.573018, adv: 0.022280, recon: 0.024024, id: 0.005871] time: 3:36:01.172202 \n",
      "[Epoch 1/1] [Batch 20601/56436] [D loss: 0.537711, acc:  48%] [G loss: 0.554547, adv: 0.009640, recon: 0.024135, id: 0.004986] time: 3:36:32.370039 \n",
      "[Epoch 1/1] [Batch 20651/56436] [D loss: 0.489412, acc:  49%] [G loss: 0.365274, adv: 0.008790, recon: 0.015703, id: 0.009060] time: 3:37:03.416298 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 20701/56436] [D loss: 0.425253, acc:  49%] [G loss: 0.487005, adv: 0.041051, recon: 0.018210, id: 0.004049] time: 3:37:33.994669 \n",
      "[Epoch 1/1] [Batch 20751/56436] [D loss: 0.494988, acc:  48%] [G loss: 0.472348, adv: 0.010125, recon: 0.020270, id: 0.005402] time: 3:38:05.030939 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 20801/56436] [D loss: 0.494899, acc:  48%] [G loss: 0.498542, adv: 0.018965, recon: 0.021247, id: 0.002636] time: 3:38:35.474015 \n",
      "[Epoch 1/1] [Batch 20851/56436] [D loss: 0.586426, acc:  49%] [G loss: 0.660508, adv: 0.013740, recon: 0.029619, id: 0.009930] time: 3:39:06.569103 \n",
      "[Epoch 1/1] [Batch 20901/56436] [D loss: 0.667450, acc:  25%] [G loss: 0.480583, adv: 0.016707, recon: 0.020659, id: 0.018175] time: 3:39:38.006949 \n",
      "[Epoch 1/1] [Batch 20951/56436] [D loss: 0.503346, acc:  49%] [G loss: 0.480941, adv: 0.013136, recon: 0.020718, id: 0.021204] time: 3:40:09.119975 \n",
      "[Epoch 1/1] [Batch 21001/56436] [D loss: 0.439908, acc:  49%] [G loss: 0.209355, adv: 0.014444, recon: 0.008152, id: 0.008540] time: 3:40:40.192462 \n",
      "[Epoch 1/1] [Batch 21051/56436] [D loss: 0.433270, acc:  49%] [G loss: 0.390335, adv: 0.017577, recon: 0.016208, id: 0.005414] time: 3:41:11.338612 \n",
      "[Epoch 1/1] [Batch 21101/56436] [D loss: 0.536624, acc:  49%] [G loss: 0.580794, adv: 0.010331, recon: 0.025209, id: 0.015965] time: 3:41:42.537480 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 21151/56436] [D loss: 0.535625, acc:  49%] [G loss: 0.273536, adv: 0.008804, recon: 0.009625, id: 0.009670] time: 3:42:13.083355 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 21201/56436] [D loss: 0.536977, acc:  49%] [G loss: 0.465700, adv: 0.010249, recon: 0.020362, id: 0.020059] time: 3:42:43.573415 \n",
      "[Epoch 1/1] [Batch 21251/56436] [D loss: 0.530586, acc:  49%] [G loss: 0.297109, adv: 0.008187, recon: 0.012705, id: 0.008478] time: 3:43:14.705944 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 21301/56436] [D loss: 0.494668, acc:  49%] [G loss: 0.616796, adv: 0.011479, recon: 0.026498, id: 0.016395] time: 3:43:45.189093 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 21351/56436] [D loss: 0.458446, acc:  49%] [G loss: 0.471049, adv: 0.026175, recon: 0.018698, id: 0.019338] time: 3:44:15.480853 \n",
      "[Epoch 1/1] [Batch 21401/56436] [D loss: 0.498269, acc:  49%] [G loss: 0.607406, adv: 0.009834, recon: 0.026556, id: 0.016445] time: 3:44:46.444591 \n",
      "[Epoch 1/1] [Batch 21451/56436] [D loss: 0.464395, acc:  49%] [G loss: 0.436105, adv: 0.010997, recon: 0.018964, id: 0.016904] time: 3:45:17.615043 \n",
      "[Epoch 1/1] [Batch 21501/56436] [D loss: 0.552226, acc:  43%] [G loss: 0.188964, adv: 0.007633, recon: 0.007824, id: 0.005379] time: 3:45:48.646424 \n",
      "[Epoch 1/1] [Batch 21551/56436] [D loss: 0.557499, acc:  49%] [G loss: 0.365940, adv: 0.011290, recon: 0.016020, id: 0.006893] time: 3:46:19.625760 \n",
      "[Epoch 1/1] [Batch 21601/56436] [D loss: 0.540596, acc:  48%] [G loss: 0.394257, adv: 0.007956, recon: 0.016311, id: 0.024750] time: 3:46:51.193074 \n",
      "[Epoch 1/1] [Batch 21651/56436] [D loss: 0.513014, acc:  48%] [G loss: 0.283259, adv: 0.014529, recon: 0.011373, id: 0.008364] time: 3:47:22.581763 \n",
      "[Epoch 1/1] [Batch 21701/56436] [D loss: 0.534486, acc:  48%] [G loss: 0.309540, adv: 0.014394, recon: 0.012826, id: 0.014915] time: 3:47:53.820194 \n",
      "[Epoch 1/1] [Batch 21751/56436] [D loss: 0.462946, acc:  49%] [G loss: 0.543825, adv: 0.010456, recon: 0.024324, id: 0.009302] time: 3:48:25.121903 \n",
      "[Epoch 1/1] [Batch 21801/56436] [D loss: 0.483507, acc:  48%] [G loss: 0.253986, adv: 0.009773, recon: 0.010763, id: 0.004292] time: 3:48:56.236157 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 21851/56436] [D loss: 0.571466, acc:  49%] [G loss: 0.576847, adv: 0.010283, recon: 0.025326, id: 0.024332] time: 3:49:26.751463 \n",
      "[Epoch 1/1] [Batch 21901/56436] [D loss: 0.546840, acc:  48%] [G loss: 0.205208, adv: 0.012729, recon: 0.008309, id: 0.003033] time: 3:49:57.815284 \n",
      "[Epoch 1/1] [Batch 21951/56436] [D loss: 0.574035, acc:  49%] [G loss: 0.462425, adv: 0.010644, recon: 0.020392, id: 0.006332] time: 3:50:28.873275 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 22001/56436] [D loss: 0.492860, acc:  49%] [G loss: 0.358981, adv: 0.007022, recon: 0.015684, id: 0.017968] time: 3:50:59.360818 \n",
      "[Epoch 1/1] [Batch 22051/56436] [D loss: 0.593697, acc:  50%] [G loss: 0.364422, adv: 0.016706, recon: 0.014909, id: 0.017937] time: 3:51:30.523763 \n",
      "[Epoch 1/1] [Batch 22101/56436] [D loss: 0.564629, acc:  50%] [G loss: 0.557591, adv: 0.021411, recon: 0.023722, id: 0.019952] time: 3:52:01.574713 \n",
      "[Epoch 1/1] [Batch 22151/56436] [D loss: 0.407526, acc:  49%] [G loss: 0.273747, adv: 0.021114, recon: 0.010570, id: 0.015908] time: 3:52:32.786387 \n",
      "[Epoch 1/1] [Batch 22201/56436] [D loss: 0.555395, acc:  48%] [G loss: 0.182518, adv: 0.009190, recon: 0.007426, id: 0.004671] time: 3:53:03.890419 \n",
      "[Epoch 1/1] [Batch 22251/56436] [D loss: 0.633814, acc:  25%] [G loss: 0.365197, adv: 0.013027, recon: 0.015139, id: 0.004543] time: 3:53:35.377657 \n",
      "[Epoch 1/1] [Batch 22301/56436] [D loss: 0.566483, acc:  25%] [G loss: 0.276897, adv: 0.007105, recon: 0.011836, id: 0.003421] time: 3:54:06.527124 \n",
      "[Epoch 1/1] [Batch 22351/56436] [D loss: 0.543489, acc:  49%] [G loss: 0.435681, adv: 0.011287, recon: 0.018902, id: 0.006511] time: 3:54:37.809488 \n",
      "[Epoch 1/1] [Batch 22401/56436] [D loss: 0.544759, acc:  48%] [G loss: 0.207520, adv: 0.008123, recon: 0.008256, id: 0.004063] time: 3:55:09.049446 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 22451/56436] [D loss: 0.425192, acc:  49%] [G loss: 0.675010, adv: 0.077839, recon: 0.018081, id: 0.011952] time: 3:55:39.609667 \n",
      "[Epoch 1/1] [Batch 22501/56436] [D loss: 0.435418, acc:  49%] [G loss: 0.305342, adv: 0.017045, recon: 0.012417, id: 0.013338] time: 3:56:10.773350 \n",
      "[Epoch 1/1] [Batch 22551/56436] [D loss: 0.429709, acc:  42%] [G loss: 1.793029, adv: 0.083652, recon: 0.074444, id: 0.067598] time: 3:56:42.059004 \n",
      "[Epoch 1/1] [Batch 22601/56436] [D loss: 0.558436, acc:  49%] [G loss: 0.356024, adv: 0.009794, recon: 0.014468, id: 0.013420] time: 3:57:13.139800 \n",
      "[Epoch 1/1] [Batch 22651/56436] [D loss: 0.362306, acc:  50%] [G loss: 2.517135, adv: 0.651992, recon: 0.054306, id: 0.087557] time: 3:57:44.281909 \n",
      "[Epoch 1/1] [Batch 22701/56436] [D loss: 0.529032, acc:  49%] [G loss: 0.596405, adv: 0.011292, recon: 0.024780, id: 0.014820] time: 3:58:15.477597 \n",
      "[Epoch 1/1] [Batch 22751/56436] [D loss: 0.412750, acc:  50%] [G loss: 0.369510, adv: 0.048330, recon: 0.011398, id: 0.008091] time: 3:58:46.645544 \n",
      "[Epoch 1/1] [Batch 22801/56436] [D loss: 0.546789, acc:  42%] [G loss: 0.374882, adv: 0.011633, recon: 0.014802, id: 0.000911] time: 3:59:17.774114 \n",
      "[Epoch 1/1] [Batch 22851/56436] [D loss: 0.405479, acc:  49%] [G loss: 0.475597, adv: 0.021325, recon: 0.019279, id: 0.016138] time: 3:59:49.183719 \n",
      "[Epoch 1/1] [Batch 22901/56436] [D loss: 0.573070, acc:  49%] [G loss: 0.531437, adv: 0.012591, recon: 0.022652, id: 0.007368] time: 4:00:20.456517 \n",
      "[Epoch 1/1] [Batch 22951/56436] [D loss: 0.632787, acc:  48%] [G loss: 0.409112, adv: 0.018107, recon: 0.016197, id: 0.004941] time: 4:00:51.700764 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 23001/56436] [D loss: 0.494527, acc:  49%] [G loss: 0.280177, adv: 0.010309, recon: 0.011215, id: 0.012516] time: 4:01:22.320258 \n",
      "[Epoch 1/1] [Batch 23051/56436] [D loss: 0.586833, acc:  41%] [G loss: 0.867715, adv: 0.010065, recon: 0.038706, id: 0.052795] time: 4:01:53.420473 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 23101/56436] [D loss: 0.704028, acc:  25%] [G loss: 0.241265, adv: 0.005915, recon: 0.010252, id: 0.003979] time: 4:02:23.965736 \n",
      "[Epoch 1/1] [Batch 23151/56436] [D loss: 0.529475, acc:  50%] [G loss: 0.871424, adv: 0.009375, recon: 0.039161, id: 0.029911] time: 4:02:55.041417 \n",
      "[Epoch 1/1] [Batch 23201/56436] [D loss: 0.523514, acc:  49%] [G loss: 0.617090, adv: 0.008094, recon: 0.027556, id: 0.019415] time: 4:03:26.204565 \n",
      "[Epoch 1/1] [Batch 23251/56436] [D loss: 0.527352, acc:  50%] [G loss: 0.530101, adv: 0.011805, recon: 0.022324, id: 0.016734] time: 4:03:57.431647 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 23301/56436] [D loss: 0.507442, acc:  49%] [G loss: 0.517174, adv: 0.008242, recon: 0.022670, id: 0.007744] time: 4:04:28.006270 \n",
      "[Epoch 1/1] [Batch 23351/56436] [D loss: 0.514329, acc:  49%] [G loss: 0.301399, adv: 0.006720, recon: 0.013129, id: 0.006191] time: 4:04:59.045781 \n",
      "[Epoch 1/1] [Batch 23401/56436] [D loss: 0.536003, acc:  49%] [G loss: 0.469382, adv: 0.010584, recon: 0.020537, id: 0.010880] time: 4:05:30.187760 \n",
      "[Epoch 1/1] [Batch 23451/56436] [D loss: 0.523384, acc:  48%] [G loss: 0.222482, adv: 0.009661, recon: 0.009258, id: 0.004585] time: 4:06:01.166743 \n",
      "[Epoch 1/1] [Batch 23501/56436] [D loss: 0.415208, acc:  49%] [G loss: 0.584569, adv: 0.015697, recon: 0.024532, id: 0.013491] time: 4:06:32.165279 \n",
      "[Epoch 1/1] [Batch 23551/56436] [D loss: 0.510384, acc:  49%] [G loss: 0.347674, adv: 0.018408, recon: 0.014153, id: 0.015345] time: 4:07:03.535046 \n",
      "[Epoch 1/1] [Batch 23601/56436] [D loss: 0.568995, acc:  49%] [G loss: 0.619825, adv: 0.030950, recon: 0.025130, id: 0.014783] time: 4:07:34.862129 \n",
      "[Epoch 1/1] [Batch 23651/56436] [D loss: 0.521458, acc:  49%] [G loss: 0.386822, adv: 0.009798, recon: 0.016759, id: 0.014582] time: 4:08:06.175273 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 23701/56436] [D loss: 0.594976, acc:  26%] [G loss: 0.723285, adv: 0.019176, recon: 0.031007, id: 0.014864] time: 4:08:36.230462 \n",
      "[Epoch 1/1] [Batch 23751/56436] [D loss: 0.619270, acc:  49%] [G loss: 0.486079, adv: 0.020970, recon: 0.020369, id: 0.014310] time: 4:09:07.319519 \n",
      "[Epoch 1/1] [Batch 23801/56436] [D loss: 0.428774, acc:  48%] [G loss: 0.644645, adv: 0.021174, recon: 0.027150, id: 0.022890] time: 4:09:38.483551 \n",
      "[Epoch 1/1] [Batch 23851/56436] [D loss: 0.608084, acc:  47%] [G loss: 0.573981, adv: 0.023499, recon: 0.024051, id: 0.019510] time: 4:10:09.671362 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 23901/56436] [D loss: 0.480527, acc:  49%] [G loss: 0.453876, adv: 0.014059, recon: 0.019548, id: 0.006728] time: 4:10:39.700137 \n",
      "[Epoch 1/1] [Batch 23951/56436] [D loss: 0.477793, acc:  49%] [G loss: 0.427719, adv: 0.009620, recon: 0.018714, id: 0.016120] time: 4:11:10.849427 \n",
      "[Epoch 1/1] [Batch 24001/56436] [D loss: 0.830994, acc:  25%] [G loss: 0.538502, adv: 0.016214, recon: 0.023204, id: 0.012158] time: 4:11:42.038138 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 24051/56436] [D loss: 0.471039, acc:  47%] [G loss: 0.425966, adv: 0.015828, recon: 0.017060, id: 0.015474] time: 4:12:12.514836 \n",
      "[Epoch 1/1] [Batch 24101/56436] [D loss: 0.416922, acc:  49%] [G loss: 0.157390, adv: 0.021896, recon: 0.005136, id: 0.003806] time: 4:12:43.640247 \n",
      "[Epoch 1/1] [Batch 24151/56436] [D loss: 0.413732, acc:  49%] [G loss: 0.441612, adv: 0.021158, recon: 0.018346, id: 0.013596] time: 4:13:14.555374 \n",
      "[Epoch 1/1] [Batch 24201/56436] [D loss: 0.485101, acc:  50%] [G loss: 0.405358, adv: 0.016145, recon: 0.016875, id: 0.015356] time: 4:13:45.379408 \n",
      "[Epoch 1/1] [Batch 24251/56436] [D loss: 0.478284, acc:  49%] [G loss: 0.351775, adv: 0.011683, recon: 0.014165, id: 0.010652] time: 4:14:16.543182 \n",
      "[Epoch 1/1] [Batch 24301/56436] [D loss: 0.404782, acc:  49%] [G loss: 0.450668, adv: 0.020590, recon: 0.018719, id: 0.010046] time: 4:14:47.897476 \n",
      "[Epoch 1/1] [Batch 24351/56436] [D loss: 0.431149, acc:  49%] [G loss: 0.388930, adv: 0.017796, recon: 0.016043, id: 0.007648] time: 4:15:19.200751 \n",
      "[Epoch 1/1] [Batch 24401/56436] [D loss: 0.643806, acc:  25%] [G loss: 0.437143, adv: 0.013862, recon: 0.018847, id: 0.010582] time: 4:15:50.634140 \n",
      "[Epoch 1/1] [Batch 24451/56436] [D loss: 0.424580, acc:  28%] [G loss: 0.982342, adv: 0.088140, recon: 0.036600, id: 0.026226] time: 4:16:21.823391 \n",
      "[Epoch 1/1] [Batch 24501/56436] [D loss: 0.472438, acc:  48%] [G loss: 0.182960, adv: 0.007120, recon: 0.007716, id: 0.003640] time: 4:16:52.986074 \n",
      "[Epoch 1/1] [Batch 24551/56436] [D loss: 0.488797, acc:  49%] [G loss: 0.481572, adv: 0.007644, recon: 0.021065, id: 0.003701] time: 4:17:24.169476 \n",
      "[Epoch 1/1] [Batch 24601/56436] [D loss: 0.500211, acc:  49%] [G loss: 0.452795, adv: 0.007749, recon: 0.019552, id: 0.004019] time: 4:17:55.278913 \n",
      "[Epoch 1/1] [Batch 24651/56436] [D loss: 0.426981, acc:  49%] [G loss: 0.289773, adv: 0.013998, recon: 0.011935, id: 0.011768] time: 4:18:26.413471 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 24701/56436] [D loss: 0.523351, acc:  48%] [G loss: 0.460907, adv: 0.006618, recon: 0.020229, id: 0.004733] time: 4:18:57.035116 \n",
      "[Epoch 1/1] [Batch 24751/56436] [D loss: 0.509686, acc:  48%] [G loss: 0.469931, adv: 0.007015, recon: 0.020655, id: 0.004294] time: 4:19:28.126144 \n",
      "[Epoch 1/1] [Batch 24801/56436] [D loss: 0.724359, acc:  25%] [G loss: 0.508079, adv: 0.013065, recon: 0.022229, id: 0.016167] time: 4:19:59.183697 \n",
      "[Epoch 1/1] [Batch 24851/56436] [D loss: 0.471586, acc:  47%] [G loss: 0.352838, adv: 0.014291, recon: 0.014360, id: 0.020057] time: 4:20:30.312398 \n",
      "[Epoch 1/1] [Batch 24901/56436] [D loss: 0.517998, acc:  49%] [G loss: 0.833609, adv: 0.018812, recon: 0.037446, id: 0.022231] time: 4:21:01.340966 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 24951/56436] [D loss: 0.469704, acc:  49%] [G loss: 0.484226, adv: 0.016290, recon: 0.020483, id: 0.006698] time: 4:21:31.958324 \n",
      "[Epoch 1/1] [Batch 25001/56436] [D loss: 0.550967, acc:  48%] [G loss: 0.448962, adv: 0.012783, recon: 0.019305, id: 0.003112] time: 4:22:03.568365 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 25051/56436] [D loss: 0.605280, acc:  25%] [G loss: 0.461908, adv: 0.016371, recon: 0.019469, id: 0.000123] time: 4:22:39.892329 \n",
      "[Epoch 1/1] [Batch 25101/56436] [D loss: 0.475601, acc:  50%] [G loss: 0.486400, adv: 0.012989, recon: 0.020914, id: 0.013474] time: 4:23:11.015833 \n",
      "[Epoch 1/1] [Batch 25151/56436] [D loss: 0.460620, acc:  50%] [G loss: 0.563841, adv: 0.026515, recon: 0.023706, id: 0.013485] time: 4:23:42.128924 \n",
      "[Epoch 1/1] [Batch 25201/56436] [D loss: 0.504716, acc:  48%] [G loss: 0.254888, adv: 0.013621, recon: 0.010373, id: 0.003200] time: 4:24:13.270461 \n",
      "[Epoch 1/1] [Batch 25251/56436] [D loss: 0.575134, acc:  48%] [G loss: 0.683195, adv: 0.012114, recon: 0.029726, id: 0.019109] time: 4:24:44.468095 \n",
      "[Epoch 1/1] [Batch 25301/56436] [D loss: 0.479759, acc:  46%] [G loss: 0.539813, adv: 0.014604, recon: 0.023149, id: 0.009720] time: 4:25:15.547938 \n",
      "[Epoch 1/1] [Batch 25351/56436] [D loss: 0.554249, acc:  48%] [G loss: 0.311042, adv: 0.006266, recon: 0.014013, id: 0.001583] time: 4:25:46.586361 \n",
      "[Epoch 1/1] [Batch 25401/56436] [D loss: 0.471850, acc:  47%] [G loss: 0.431053, adv: 0.011651, recon: 0.018454, id: 0.003483] time: 4:26:17.604385 \n",
      "[Epoch 1/1] [Batch 25451/56436] [D loss: 0.462885, acc:  49%] [G loss: 0.692426, adv: 0.014425, recon: 0.030214, id: 0.014998] time: 4:26:48.737384 \n",
      "[Epoch 1/1] [Batch 25501/56436] [D loss: 0.525863, acc:  39%] [G loss: 0.336538, adv: 0.009264, recon: 0.014502, id: 0.008657] time: 4:27:19.761520 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 25551/56436] [D loss: 0.470521, acc:  49%] [G loss: 0.551859, adv: 0.011118, recon: 0.024080, id: 0.021366] time: 4:27:50.384790 \n",
      "[Epoch 1/1] [Batch 25601/56436] [D loss: 0.599569, acc:  49%] [G loss: 0.437623, adv: 0.014079, recon: 0.018217, id: 0.008506] time: 4:28:21.412095 \n",
      "[Epoch 1/1] [Batch 25651/56436] [D loss: 0.673828, acc:  49%] [G loss: 0.367042, adv: 0.038414, recon: 0.013181, id: 0.013571] time: 4:28:52.447781 \n",
      "[Epoch 1/1] [Batch 25701/56436] [D loss: 0.539370, acc:  49%] [G loss: 0.229995, adv: 0.008107, recon: 0.010095, id: 0.006667] time: 4:29:23.509764 \n",
      "[Epoch 1/1] [Batch 25751/56436] [D loss: 0.598198, acc:  49%] [G loss: 0.601413, adv: 0.014901, recon: 0.026191, id: 0.007078] time: 4:29:54.582928 \n",
      "[Epoch 1/1] [Batch 25801/56436] [D loss: 0.477022, acc:  48%] [G loss: 0.537750, adv: 0.008472, recon: 0.023746, id: 0.003439] time: 4:30:25.596039 \n",
      "[Epoch 1/1] [Batch 25851/56436] [D loss: 0.514538, acc:  48%] [G loss: 0.187825, adv: 0.006634, recon: 0.008077, id: 0.005342] time: 4:30:57.065034 \n",
      "[Epoch 1/1] [Batch 25901/56436] [D loss: 0.417392, acc:  49%] [G loss: 0.438413, adv: 0.017054, recon: 0.018404, id: 0.017342] time: 4:31:28.562086 \n",
      "[Epoch 1/1] [Batch 25951/56436] [D loss: 0.517748, acc:  49%] [G loss: 0.354273, adv: 0.007369, recon: 0.015286, id: 0.016123] time: 4:31:59.750334 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 26001/56436] [D loss: 0.502177, acc:  49%] [G loss: 1.144225, adv: 0.012850, recon: 0.050398, id: 0.008696] time: 4:32:30.351573 \n",
      "[Epoch 1/1] [Batch 26051/56436] [D loss: 0.511724, acc:  49%] [G loss: 0.534466, adv: 0.006959, recon: 0.024213, id: 0.011238] time: 4:33:01.524364 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 26101/56436] [D loss: 0.445066, acc:  49%] [G loss: 0.607461, adv: 0.013376, recon: 0.026726, id: 0.007404] time: 4:33:32.140383 \n",
      "[Epoch 1/1] [Batch 26151/56436] [D loss: 0.498440, acc:  48%] [G loss: 0.586672, adv: 0.007418, recon: 0.025964, id: 0.024011] time: 4:34:03.214831 \n",
      "[Epoch 1/1] [Batch 26201/56436] [D loss: 0.522466, acc:  46%] [G loss: 0.843269, adv: 0.019495, recon: 0.036406, id: 0.048855] time: 4:34:34.285554 \n",
      "[Epoch 1/1] [Batch 26251/56436] [D loss: 0.393901, acc:  49%] [G loss: 0.481160, adv: 0.022926, recon: 0.017662, id: 0.007282] time: 4:35:05.386129 \n",
      "[Epoch 1/1] [Batch 26301/56436] [D loss: 0.456026, acc:  48%] [G loss: 0.544767, adv: 0.013292, recon: 0.023535, id: 0.009316] time: 4:35:36.595354 \n",
      "[Epoch 1/1] [Batch 26351/56436] [D loss: 0.524014, acc:  49%] [G loss: 0.279464, adv: 0.008367, recon: 0.011940, id: 0.005002] time: 4:36:07.793125 \n",
      "[Epoch 1/1] [Batch 26401/56436] [D loss: 0.507854, acc:  48%] [G loss: 0.186035, adv: 0.014843, recon: 0.007009, id: 0.002506] time: 4:36:38.928737 \n",
      "[Epoch 1/1] [Batch 26451/56436] [D loss: 0.568465, acc:  50%] [G loss: 0.490376, adv: 0.007932, recon: 0.020784, id: 0.016025] time: 4:37:10.094645 \n",
      "[Epoch 1/1] [Batch 26501/56436] [D loss: 0.464740, acc:  49%] [G loss: 0.231318, adv: 0.011072, recon: 0.008879, id: 0.004530] time: 4:37:41.495861 \n",
      "[Epoch 1/1] [Batch 26551/56436] [D loss: 0.462354, acc:  49%] [G loss: 0.351270, adv: 0.014902, recon: 0.014730, id: 0.005255] time: 4:38:12.732815 \n",
      "[Epoch 1/1] [Batch 26601/56436] [D loss: 0.514681, acc:  49%] [G loss: 0.326021, adv: 0.015213, recon: 0.013418, id: 0.008568] time: 4:38:44.055338 \n",
      "[Epoch 1/1] [Batch 26651/56436] [D loss: 0.440552, acc:  49%] [G loss: 0.495360, adv: 0.018097, recon: 0.020810, id: 0.017703] time: 4:39:15.346967 \n",
      "[Epoch 1/1] [Batch 26701/56436] [D loss: 0.561907, acc:  49%] [G loss: 0.418310, adv: 0.011277, recon: 0.013462, id: 0.115123] time: 4:39:46.547120 \n",
      "[Epoch 1/1] [Batch 26751/56436] [D loss: 0.544197, acc:  50%] [G loss: 0.453050, adv: 0.011472, recon: 0.019588, id: 0.018576] time: 4:40:17.808478 \n",
      "[Epoch 1/1] [Batch 26801/56436] [D loss: 0.430405, acc:  49%] [G loss: 0.447870, adv: 0.013965, recon: 0.016383, id: 0.008710] time: 4:40:48.890731 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 26851/56436] [D loss: 0.490728, acc:  49%] [G loss: 0.503371, adv: 0.013669, recon: 0.021730, id: 0.009306] time: 4:41:19.443536 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 26901/56436] [D loss: 0.499680, acc:  49%] [G loss: 0.338811, adv: 0.007756, recon: 0.014816, id: 0.010485] time: 4:41:49.936238 \n",
      "[Epoch 1/1] [Batch 26951/56436] [D loss: 0.485128, acc:  49%] [G loss: 0.599339, adv: 0.007611, recon: 0.025980, id: 0.011883] time: 4:42:20.987302 \n",
      "[Epoch 1/1] [Batch 27001/56436] [D loss: 0.594920, acc:  25%] [G loss: 0.399086, adv: 0.006692, recon: 0.017258, id: 0.000977] time: 4:42:52.018876 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 27051/56436] [D loss: 0.522300, acc:  50%] [G loss: 0.418312, adv: 0.013444, recon: 0.017719, id: 0.018262] time: 4:43:22.628780 \n",
      "[Epoch 1/1] [Batch 27101/56436] [D loss: 0.491372, acc:  49%] [G loss: 0.179017, adv: 0.006492, recon: 0.007469, id: 0.008284] time: 4:43:53.596806 \n",
      "[Epoch 1/1] [Batch 27151/56436] [D loss: 0.562766, acc:  49%] [G loss: 0.480067, adv: 0.010725, recon: 0.020737, id: 0.018267] time: 4:44:24.607659 \n",
      "[Epoch 1/1] [Batch 27201/56436] [D loss: 0.450512, acc:  50%] [G loss: 0.508723, adv: 0.013535, recon: 0.022040, id: 0.014333] time: 4:44:56.027415 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 27251/56436] [D loss: 0.500295, acc:  47%] [G loss: 0.511119, adv: 0.012114, recon: 0.022066, id: 0.009031] time: 4:45:26.777075 \n",
      "[Epoch 1/1] [Batch 27301/56436] [D loss: 0.504795, acc:  48%] [G loss: 0.346979, adv: 0.007594, recon: 0.015361, id: 0.006033] time: 4:45:57.908142 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 27351/56436] [D loss: 0.474359, acc:  48%] [G loss: 0.444031, adv: 0.006976, recon: 0.019844, id: 0.003197] time: 4:46:28.528732 \n",
      "[Epoch 1/1] [Batch 27401/56436] [D loss: 0.509368, acc:  48%] [G loss: 0.396510, adv: 0.006841, recon: 0.017160, id: 0.003451] time: 4:46:59.630018 \n",
      "[Epoch 1/1] [Batch 27451/56436] [D loss: 0.797984, acc:  25%] [G loss: 0.463682, adv: 0.022293, recon: 0.019121, id: 0.006540] time: 4:47:30.604548 \n",
      "[Epoch 1/1] [Batch 27501/56436] [D loss: 0.476068, acc:  48%] [G loss: 0.172473, adv: 0.014561, recon: 0.006578, id: 0.004748] time: 4:48:01.556064 \n",
      "[Epoch 1/1] [Batch 27551/56436] [D loss: 0.588123, acc:  43%] [G loss: 0.741835, adv: 0.009307, recon: 0.032848, id: 0.047568] time: 4:48:32.557093 \n",
      "[Epoch 1/1] [Batch 27601/56436] [D loss: 0.659303, acc:  42%] [G loss: 0.263747, adv: 0.015140, recon: 0.010590, id: 0.001106] time: 4:49:03.890415 \n",
      "[Epoch 1/1] [Batch 27651/56436] [D loss: 0.476603, acc:  49%] [G loss: 0.583234, adv: 0.008141, recon: 0.026116, id: 0.019425] time: 4:49:34.959596 \n",
      "[Epoch 1/1] [Batch 27701/56436] [D loss: 0.568661, acc:  48%] [G loss: 0.607476, adv: 0.011295, recon: 0.026669, id: 0.023885] time: 4:50:05.985099 \n",
      "[Epoch 1/1] [Batch 27751/56436] [D loss: 0.517999, acc:  48%] [G loss: 0.449225, adv: 0.008622, recon: 0.019517, id: 0.009463] time: 4:50:37.030679 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 27801/56436] [D loss: 0.516611, acc:  49%] [G loss: 0.318402, adv: 0.007828, recon: 0.013763, id: 0.006125] time: 4:51:07.564856 \n",
      "[Epoch 1/1] [Batch 27851/56436] [D loss: 0.464229, acc:  49%] [G loss: 0.537738, adv: 0.012835, recon: 0.023325, id: 0.023101] time: 4:51:38.600643 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 27901/56436] [D loss: 0.506640, acc:  49%] [G loss: 0.379982, adv: 0.008951, recon: 0.016299, id: 0.005571] time: 4:52:09.022756 \n",
      "[Epoch 1/1] [Batch 27951/56436] [D loss: 0.458451, acc:  49%] [G loss: 0.475932, adv: 0.011609, recon: 0.020885, id: 0.009947] time: 4:52:40.112630 \n",
      "[Epoch 1/1] [Batch 28001/56436] [D loss: 0.563388, acc:  43%] [G loss: 0.512488, adv: 0.007607, recon: 0.023343, id: 0.013283] time: 4:53:11.156104 \n",
      "[Epoch 1/1] [Batch 28051/56436] [D loss: 0.481776, acc:  49%] [G loss: 0.594193, adv: 0.015077, recon: 0.025829, id: 0.010197] time: 4:53:42.524903 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 28101/56436] [D loss: 0.585498, acc:  25%] [G loss: 0.229128, adv: 0.006491, recon: 0.009889, id: 0.002124] time: 4:54:13.177590 \n",
      "[Epoch 1/1] [Batch 28151/56436] [D loss: 0.463907, acc:  46%] [G loss: 0.392721, adv: 0.019811, recon: 0.016043, id: 0.004403] time: 4:54:44.359517 \n",
      "[Epoch 1/1] [Batch 28201/56436] [D loss: 0.505108, acc:  49%] [G loss: 0.404543, adv: 0.005849, recon: 0.017959, id: 0.005849] time: 4:55:15.634072 \n",
      "[Epoch 1/1] [Batch 28251/56436] [D loss: 0.524395, acc:  50%] [G loss: 0.565694, adv: 0.009454, recon: 0.024680, id: 0.014794] time: 4:55:46.734780 \n",
      "[Epoch 1/1] [Batch 28301/56436] [D loss: 0.509959, acc:  49%] [G loss: 0.300514, adv: 0.008274, recon: 0.012872, id: 0.008609] time: 4:56:17.837106 \n",
      "[Epoch 1/1] [Batch 28351/56436] [D loss: 0.426555, acc:  49%] [G loss: 0.249931, adv: 0.015643, recon: 0.009927, id: 0.004931] time: 4:56:48.979264 \n",
      "[Epoch 1/1] [Batch 28401/56436] [D loss: 0.755859, acc:  25%] [G loss: 0.689754, adv: 0.020756, recon: 0.028782, id: 0.020621] time: 4:57:20.134020 \n",
      "[Epoch 1/1] [Batch 28451/56436] [D loss: 0.529414, acc:  48%] [G loss: 0.358360, adv: 0.009605, recon: 0.015382, id: 0.004805] time: 4:57:51.071187 \n",
      "[Epoch 1/1] [Batch 28501/56436] [D loss: 0.545530, acc:  48%] [G loss: 0.182382, adv: 0.008235, recon: 0.007674, id: 0.003536] time: 4:58:22.117104 \n",
      "[Epoch 1/1] [Batch 28551/56436] [D loss: 0.483402, acc:  49%] [G loss: 0.517844, adv: 0.009108, recon: 0.022338, id: 0.019106] time: 4:58:53.384568 \n",
      "[Epoch 1/1] [Batch 28601/56436] [D loss: 0.580854, acc:  49%] [G loss: 0.556323, adv: 0.015093, recon: 0.023641, id: 0.014308] time: 4:59:24.569267 \n",
      "[Epoch 1/1] [Batch 28651/56436] [D loss: 0.455526, acc:  49%] [G loss: 0.412715, adv: 0.017898, recon: 0.016906, id: 0.006520] time: 4:59:55.694075 \n",
      "[Epoch 1/1] [Batch 28701/56436] [D loss: 0.407452, acc:  49%] [G loss: 0.402814, adv: 0.020715, recon: 0.016419, id: 0.007153] time: 5:00:27.121745 \n",
      "[Epoch 1/1] [Batch 28751/56436] [D loss: 0.503393, acc:  48%] [G loss: 0.234872, adv: 0.009713, recon: 0.009768, id: 0.002610] time: 5:00:58.304814 \n",
      "[Epoch 1/1] [Batch 28801/56436] [D loss: 0.563115, acc:  49%] [G loss: 0.449439, adv: 0.010383, recon: 0.019408, id: 0.014382] time: 5:01:29.477085 \n",
      "[Epoch 1/1] [Batch 28851/56436] [D loss: 0.447189, acc:  49%] [G loss: 0.512288, adv: 0.012287, recon: 0.022365, id: 0.006224] time: 5:02:00.577076 \n",
      "[Epoch 1/1] [Batch 28901/56436] [D loss: 0.555678, acc:  49%] [G loss: 0.458135, adv: 0.009861, recon: 0.020153, id: 0.006371] time: 5:02:31.774250 \n",
      "[Epoch 1/1] [Batch 28951/56436] [D loss: 0.507224, acc:  48%] [G loss: 0.345254, adv: 0.010085, recon: 0.014889, id: 0.008309] time: 5:03:02.916918 \n",
      "[Epoch 1/1] [Batch 29001/56436] [D loss: 0.542943, acc:  48%] [G loss: 0.446738, adv: 0.012349, recon: 0.019513, id: 0.009103] time: 5:03:34.096708 \n",
      "[Epoch 1/1] [Batch 29051/56436] [D loss: 0.497358, acc:  49%] [G loss: 0.615524, adv: 0.007762, recon: 0.026803, id: 0.026735] time: 5:04:05.259190 \n",
      "[Epoch 1/1] [Batch 29101/56436] [D loss: 0.505947, acc:  49%] [G loss: 0.612914, adv: 0.006925, recon: 0.027070, id: 0.007979] time: 5:04:36.206398 \n",
      "[Epoch 1/1] [Batch 29151/56436] [D loss: 0.448724, acc:  49%] [G loss: 0.352100, adv: 0.015787, recon: 0.014541, id: 0.012289] time: 5:05:07.329668 \n",
      "[Epoch 1/1] [Batch 29201/56436] [D loss: 0.498493, acc:  48%] [G loss: 0.414164, adv: 0.009711, recon: 0.018105, id: 0.018350] time: 5:05:38.334649 \n",
      "[Epoch 1/1] [Batch 29251/56436] [D loss: 0.511814, acc:  49%] [G loss: 0.234672, adv: 0.005709, recon: 0.010087, id: 0.010396] time: 5:06:09.553555 \n",
      "[Epoch 1/1] [Batch 29301/56436] [D loss: 0.545375, acc:  48%] [G loss: 0.391884, adv: 0.007150, recon: 0.017066, id: 0.003089] time: 5:06:40.854277 \n",
      "[Epoch 1/1] [Batch 29351/56436] [D loss: 0.500618, acc:  49%] [G loss: 0.285786, adv: 0.006417, recon: 0.012501, id: 0.002970] time: 5:07:12.095465 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 29401/56436] [D loss: 0.533755, acc:  49%] [G loss: 0.215380, adv: 0.010583, recon: 0.008849, id: 0.009044] time: 5:07:42.784722 \n",
      "[Epoch 1/1] [Batch 29451/56436] [D loss: 0.531063, acc:  50%] [G loss: 0.436919, adv: 0.009336, recon: 0.019112, id: 0.015279] time: 5:08:13.901936 \n",
      "[Epoch 1/1] [Batch 29501/56436] [D loss: 0.559526, acc:  48%] [G loss: 0.399469, adv: 0.006357, recon: 0.017619, id: 0.024212] time: 5:08:45.069247 \n",
      "[Epoch 1/1] [Batch 29551/56436] [D loss: 0.383807, acc:  49%] [G loss: 0.597533, adv: 0.026676, recon: 0.022457, id: 0.069021] time: 5:09:16.176108 \n",
      "[Epoch 1/1] [Batch 29601/56436] [D loss: 0.504331, acc:  48%] [G loss: 0.478049, adv: 0.007001, recon: 0.020706, id: 0.004541] time: 5:09:47.393199 \n",
      "[Epoch 1/1] [Batch 29651/56436] [D loss: 0.543528, acc:  50%] [G loss: 0.392711, adv: 0.025370, recon: 0.015588, id: 0.014731] time: 5:10:18.725772 \n",
      "[Epoch 1/1] [Batch 29701/56436] [D loss: 0.470819, acc:  49%] [G loss: 0.941639, adv: 0.009741, recon: 0.040992, id: 0.010665] time: 5:10:49.907247 \n",
      "[Epoch 1/1] [Batch 29751/56436] [D loss: 0.494686, acc:  48%] [G loss: 0.168558, adv: 0.007317, recon: 0.006761, id: 0.004410] time: 5:11:21.054230 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 29801/56436] [D loss: 0.491497, acc:  48%] [G loss: 0.138457, adv: 0.005602, recon: 0.005886, id: 0.003089] time: 5:11:51.660932 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 29851/56436] [D loss: 0.474982, acc:  49%] [G loss: 0.517179, adv: 0.008725, recon: 0.022092, id: 0.018176] time: 5:12:22.238270 \n",
      "[Epoch 1/1] [Batch 29901/56436] [D loss: 0.502674, acc:  49%] [G loss: 0.355596, adv: 0.007073, recon: 0.015650, id: 0.009307] time: 5:12:53.254503 \n",
      "[Epoch 1/1] [Batch 29951/56436] [D loss: 0.561865, acc:  45%] [G loss: 0.368504, adv: 0.009496, recon: 0.016139, id: 0.002904] time: 5:13:24.552368 \n",
      "[Epoch 1/1] [Batch 30001/56436] [D loss: 0.471915, acc:  49%] [G loss: 0.547755, adv: 0.009589, recon: 0.024098, id: 0.027920] time: 5:13:55.845583 \n",
      "[Epoch 1/1] [Batch 30051/56436] [D loss: 0.616292, acc:  43%] [G loss: 0.492957, adv: 0.025314, recon: 0.020318, id: 0.020599] time: 5:14:33.079140 \n",
      "[Epoch 1/1] [Batch 30101/56436] [D loss: 0.545145, acc:  49%] [G loss: 0.281917, adv: 0.010716, recon: 0.008594, id: 0.006357] time: 5:15:04.262782 \n",
      "[Epoch 1/1] [Batch 30151/56436] [D loss: 0.471121, acc:  49%] [G loss: 0.497979, adv: 0.010682, recon: 0.021770, id: 0.008335] time: 5:15:35.465572 \n",
      "[Epoch 1/1] [Batch 30201/56436] [D loss: 0.508574, acc:  49%] [G loss: 0.487686, adv: 0.013906, recon: 0.020696, id: 0.008046] time: 5:16:06.666306 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 30251/56436] [D loss: 0.479103, acc:  49%] [G loss: 0.259858, adv: 0.009099, recon: 0.011188, id: 0.008487] time: 5:16:37.220015 \n",
      "[Epoch 1/1] [Batch 30301/56436] [D loss: 0.440945, acc:  48%] [G loss: 0.325343, adv: 0.015912, recon: 0.012496, id: 0.011725] time: 5:17:08.357202 \n",
      "[Epoch 1/1] [Batch 30351/56436] [D loss: 0.439780, acc:  49%] [G loss: 0.472619, adv: 0.012048, recon: 0.020495, id: 0.015089] time: 5:17:39.577336 \n",
      "[Epoch 1/1] [Batch 30401/56436] [D loss: 0.552204, acc:  34%] [G loss: 0.345652, adv: 0.007976, recon: 0.015077, id: 0.004928] time: 5:18:10.713077 \n",
      "[Epoch 1/1] [Batch 30451/56436] [D loss: 0.531411, acc:  48%] [G loss: 0.287811, adv: 0.006055, recon: 0.012832, id: 0.006916] time: 5:18:41.764818 \n",
      "[Epoch 1/1] [Batch 30501/56436] [D loss: 0.580742, acc:  49%] [G loss: 0.666709, adv: 0.014507, recon: 0.029164, id: 0.015765] time: 5:19:12.855820 \n",
      "[Epoch 1/1] [Batch 30551/56436] [D loss: 0.460079, acc:  49%] [G loss: 0.361838, adv: 0.010548, recon: 0.015836, id: 0.004868] time: 5:19:43.959199 \n",
      "[Epoch 1/1] [Batch 30601/56436] [D loss: 0.524791, acc:  49%] [G loss: 0.452841, adv: 0.009190, recon: 0.019594, id: 0.005810] time: 5:20:15.003282 \n",
      "[Epoch 1/1] [Batch 30651/56436] [D loss: 0.440175, acc:  49%] [G loss: 0.649410, adv: 0.016859, recon: 0.028074, id: 0.016792] time: 5:20:45.980089 \n",
      "[Epoch 1/1] [Batch 30701/56436] [D loss: 0.510771, acc:  49%] [G loss: 0.479479, adv: 0.008308, recon: 0.020928, id: 0.018597] time: 5:21:17.305263 \n",
      "[Epoch 1/1] [Batch 30751/56436] [D loss: 0.494504, acc:  49%] [G loss: 0.502167, adv: 0.004514, recon: 0.021975, id: 0.006113] time: 5:21:48.399059 \n",
      "[Epoch 1/1] [Batch 30801/56436] [D loss: 0.383817, acc:  48%] [G loss: 0.529788, adv: 0.028885, recon: 0.021045, id: 0.003580] time: 5:22:19.536233 \n",
      "[Epoch 1/1] [Batch 30851/56436] [D loss: 0.564035, acc:  49%] [G loss: 0.570156, adv: 0.011589, recon: 0.024744, id: 0.005133] time: 5:22:51.082302 \n",
      "[Epoch 1/1] [Batch 30901/56436] [D loss: 0.585186, acc:  50%] [G loss: 0.580510, adv: 0.013036, recon: 0.025428, id: 0.019329] time: 5:23:22.446936 \n",
      "[Epoch 1/1] [Batch 30951/56436] [D loss: 0.588051, acc:  31%] [G loss: 0.440175, adv: 0.007522, recon: 0.018843, id: 0.007368] time: 5:23:53.648663 \n",
      "[Epoch 1/1] [Batch 31001/56436] [D loss: 0.504996, acc:  49%] [G loss: 0.560899, adv: 0.008202, recon: 0.024747, id: 0.017020] time: 5:24:24.883267 \n",
      "[Epoch 1/1] [Batch 31051/56436] [D loss: 0.638704, acc:  25%] [G loss: 0.258774, adv: 0.004846, recon: 0.011505, id: 0.004103] time: 5:24:56.007106 \n",
      "[Epoch 1/1] [Batch 31101/56436] [D loss: 0.492878, acc:  48%] [G loss: 0.353529, adv: 0.006871, recon: 0.015546, id: 0.003014] time: 5:25:27.108597 \n",
      "[Epoch 1/1] [Batch 31151/56436] [D loss: 0.486868, acc:  49%] [G loss: 0.497552, adv: 0.008061, recon: 0.021840, id: 0.023722] time: 5:25:58.332767 \n",
      "[Epoch 1/1] [Batch 31201/56436] [D loss: 0.555949, acc:  48%] [G loss: 0.228296, adv: 0.008717, recon: 0.009382, id: 0.003497] time: 5:26:29.523556 \n",
      "[Epoch 1/1] [Batch 31251/56436] [D loss: 0.378575, acc:  50%] [G loss: 0.807603, adv: 0.069491, recon: 0.030502, id: 0.012183] time: 5:27:00.584909 \n",
      "[Epoch 1/1] [Batch 31301/56436] [D loss: 0.442134, acc:  48%] [G loss: 0.601501, adv: 0.020238, recon: 0.025869, id: 0.032677] time: 5:27:31.774779 \n",
      "[Epoch 1/1] [Batch 31351/56436] [D loss: 0.558000, acc:  48%] [G loss: 0.780559, adv: 0.007739, recon: 0.035531, id: 0.003760] time: 5:28:02.946209 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 31401/56436] [D loss: 0.420052, acc:  50%] [G loss: 0.919300, adv: 0.026346, recon: 0.040479, id: 0.031335] time: 5:28:33.022037 \n",
      "[Epoch 1/1] [Batch 31451/56436] [D loss: 0.544839, acc:  49%] [G loss: 0.439079, adv: 0.008449, recon: 0.019241, id: 0.011614] time: 5:29:04.059357 \n",
      "[Epoch 1/1] [Batch 31501/56436] [D loss: 0.546964, acc:  47%] [G loss: 0.402256, adv: 0.009956, recon: 0.017305, id: 0.016354] time: 5:29:35.317572 \n",
      "[Epoch 1/1] [Batch 31551/56436] [D loss: 0.579318, acc:  48%] [G loss: 0.404094, adv: 0.017636, recon: 0.016665, id: 0.005676] time: 5:30:06.473843 \n",
      "[Epoch 1/1] [Batch 31601/56436] [D loss: 0.548244, acc:  50%] [G loss: 0.407246, adv: 0.014511, recon: 0.017207, id: 0.027692] time: 5:30:37.542396 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 31651/56436] [D loss: 0.527597, acc:  48%] [G loss: 0.154871, adv: 0.006955, recon: 0.006313, id: 0.003099] time: 5:31:08.217983 \n",
      "[Epoch 1/1] [Batch 31701/56436] [D loss: 0.480745, acc:  49%] [G loss: 0.431167, adv: 0.008188, recon: 0.018626, id: 0.013219] time: 5:31:39.322713 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 31751/56436] [D loss: 0.511028, acc:  48%] [G loss: 0.297716, adv: 0.009269, recon: 0.012796, id: 0.002946] time: 5:32:10.089585 \n",
      "[Epoch 1/1] [Batch 31801/56436] [D loss: 0.490650, acc:  49%] [G loss: 0.350401, adv: 0.006664, recon: 0.015553, id: 0.007207] time: 5:32:41.463039 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 31851/56436] [D loss: 0.509569, acc:  49%] [G loss: 0.557622, adv: 0.011762, recon: 0.024379, id: 0.020054] time: 5:33:11.691948 \n",
      "[Epoch 1/1] [Batch 31901/56436] [D loss: 0.534605, acc:  49%] [G loss: 0.430549, adv: 0.008361, recon: 0.018921, id: 0.012084] time: 5:33:42.787059 \n",
      "[Epoch 1/1] [Batch 31951/56436] [D loss: 0.471206, acc:  48%] [G loss: 0.516310, adv: 0.012047, recon: 0.022342, id: 0.007911] time: 5:34:13.976704 \n",
      "[Epoch 1/1] [Batch 32001/56436] [D loss: 0.563352, acc:  49%] [G loss: 0.585343, adv: 0.009436, recon: 0.025419, id: 0.010004] time: 5:34:45.178425 \n",
      "[Epoch 1/1] [Batch 32051/56436] [D loss: 0.466921, acc:  49%] [G loss: 0.441118, adv: 0.009328, recon: 0.019205, id: 0.014326] time: 5:35:16.349899 \n",
      "[Epoch 1/1] [Batch 32101/56436] [D loss: 0.465072, acc:  49%] [G loss: 0.416795, adv: 0.006587, recon: 0.016747, id: 0.006512] time: 5:35:47.443757 \n",
      "[Epoch 1/1] [Batch 32151/56436] [D loss: 0.450037, acc:  49%] [G loss: 0.348615, adv: 0.020093, recon: 0.014180, id: 0.008451] time: 5:36:18.619940 \n",
      "[Epoch 1/1] [Batch 32201/56436] [D loss: 0.496623, acc:  49%] [G loss: 0.472062, adv: 0.009325, recon: 0.020302, id: 0.006017] time: 5:36:49.812752 \n",
      "[Epoch 1/1] [Batch 32251/56436] [D loss: 0.517516, acc:  49%] [G loss: 0.405151, adv: 0.008675, recon: 0.017711, id: 0.018490] time: 5:37:20.874112 \n",
      "[Epoch 1/1] [Batch 32301/56436] [D loss: 0.460744, acc:  49%] [G loss: 0.311945, adv: 0.010700, recon: 0.013690, id: 0.008325] time: 5:37:52.021953 \n",
      "[Epoch 1/1] [Batch 32351/56436] [D loss: 0.471536, acc:  49%] [G loss: 0.223471, adv: 0.010280, recon: 0.009280, id: 0.008055] time: 5:38:23.323026 \n",
      "[Epoch 1/1] [Batch 32401/56436] [D loss: 0.533699, acc:  49%] [G loss: 0.429975, adv: 0.008852, recon: 0.018906, id: 0.010137] time: 5:38:54.331357 \n",
      "[Epoch 1/1] [Batch 32451/56436] [D loss: 0.493194, acc:  49%] [G loss: 0.384483, adv: 0.008223, recon: 0.016674, id: 0.007359] time: 5:39:25.576955 \n",
      "[Epoch 1/1] [Batch 32501/56436] [D loss: 0.567950, acc:  37%] [G loss: 0.691335, adv: 0.011519, recon: 0.030543, id: 0.033732] time: 5:39:58.168760 \n",
      "[Epoch 1/1] [Batch 32551/56436] [D loss: 0.519514, acc:  48%] [G loss: 0.355693, adv: 0.012550, recon: 0.015122, id: 0.007725] time: 5:40:30.616903 \n",
      "[Epoch 1/1] [Batch 32601/56436] [D loss: 0.492852, acc:  49%] [G loss: 0.210434, adv: 0.006806, recon: 0.007816, id: 0.005325] time: 5:41:02.335883 \n",
      "[Epoch 1/1] [Batch 32651/56436] [D loss: 0.481775, acc:  48%] [G loss: 0.815136, adv: 0.017376, recon: 0.036196, id: 0.040565] time: 5:41:34.032808 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 32701/56436] [D loss: 0.512347, acc:  49%] [G loss: 0.388440, adv: 0.006857, recon: 0.016976, id: 0.008027] time: 5:42:05.692877 \n",
      "[Epoch 1/1] [Batch 32751/56436] [D loss: 0.447500, acc:  49%] [G loss: 0.332329, adv: 0.013984, recon: 0.013096, id: 0.019571] time: 5:42:37.053240 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 32801/56436] [D loss: 0.492364, acc:  49%] [G loss: 0.298001, adv: 0.008697, recon: 0.012771, id: 0.010334] time: 5:43:08.152635 \n",
      "[Epoch 1/1] [Batch 32851/56436] [D loss: 0.340291, acc:  71%] [G loss: 2.225439, adv: 0.642520, recon: 0.041596, id: 0.073917] time: 5:43:39.526862 \n",
      "[Epoch 1/1] [Batch 32901/56436] [D loss: 0.452330, acc:  49%] [G loss: 0.472829, adv: 0.010992, recon: 0.020398, id: 0.020047] time: 5:44:10.774023 \n",
      "[Epoch 1/1] [Batch 32951/56436] [D loss: 0.507391, acc:  49%] [G loss: 0.347376, adv: 0.005490, recon: 0.015329, id: 0.006760] time: 5:44:42.317040 \n",
      "[Epoch 1/1] [Batch 33001/56436] [D loss: 0.623309, acc:  50%] [G loss: 0.429458, adv: 0.023123, recon: 0.016987, id: 0.014135] time: 5:45:14.060655 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 33051/56436] [D loss: 0.508607, acc:  49%] [G loss: 0.364326, adv: 0.007492, recon: 0.015735, id: 0.012082] time: 5:45:44.762013 \n",
      "[Epoch 1/1] [Batch 33101/56436] [D loss: 0.458578, acc:  49%] [G loss: 0.280011, adv: 0.011477, recon: 0.011725, id: 0.010105] time: 5:46:15.944130 \n",
      "[Epoch 1/1] [Batch 33151/56436] [D loss: 0.654857, acc:  25%] [G loss: 0.345634, adv: 0.011275, recon: 0.014755, id: 0.011567] time: 5:46:47.580179 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 33201/56436] [D loss: 0.535861, acc:  47%] [G loss: 0.395881, adv: 0.016041, recon: 0.016633, id: 0.022311] time: 5:47:18.418429 \n",
      "[Epoch 1/1] [Batch 33251/56436] [D loss: 0.479468, acc:  50%] [G loss: 0.453938, adv: 0.017552, recon: 0.019092, id: 0.006539] time: 5:47:49.733217 \n",
      "[Epoch 1/1] [Batch 33301/56436] [D loss: 0.595365, acc:  49%] [G loss: 0.293190, adv: 0.014010, recon: 0.011982, id: 0.015853] time: 5:48:20.954187 \n",
      "[Epoch 1/1] [Batch 33351/56436] [D loss: 0.519184, acc:  48%] [G loss: 0.359186, adv: 0.008457, recon: 0.015622, id: 0.003592] time: 5:48:52.178031 \n",
      "[Epoch 1/1] [Batch 33401/56436] [D loss: 0.451122, acc:  49%] [G loss: 0.408906, adv: 0.012310, recon: 0.017509, id: 0.008847] time: 5:49:23.383194 \n",
      "[Epoch 1/1] [Batch 33451/56436] [D loss: 0.478084, acc:  49%] [G loss: 0.330944, adv: 0.007750, recon: 0.014023, id: 0.003999] time: 5:49:54.402523 \n",
      "[Epoch 1/1] [Batch 33501/56436] [D loss: 0.547205, acc:  48%] [G loss: 0.397766, adv: 0.009350, recon: 0.017197, id: 0.004319] time: 5:50:25.557717 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 33551/56436] [D loss: 0.584355, acc:  48%] [G loss: 0.526385, adv: 0.015521, recon: 0.022777, id: 0.004620] time: 5:50:56.443982 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 33601/56436] [D loss: 0.446729, acc:  48%] [G loss: 0.464429, adv: 0.015706, recon: 0.019802, id: 0.016333] time: 5:51:27.066932 \n",
      "[Epoch 1/1] [Batch 33651/56436] [D loss: 0.535852, acc:  49%] [G loss: 0.596557, adv: 0.006336, recon: 0.026095, id: 0.012040] time: 5:51:58.395280 \n",
      "[Epoch 1/1] [Batch 33701/56436] [D loss: 0.527829, acc:  49%] [G loss: 0.459687, adv: 0.008430, recon: 0.020328, id: 0.014828] time: 5:52:29.547251 \n",
      "[Epoch 1/1] [Batch 33751/56436] [D loss: 0.550720, acc:  49%] [G loss: 0.520379, adv: 0.009557, recon: 0.022457, id: 0.006018] time: 5:53:01.068477 \n",
      "[Epoch 1/1] [Batch 33801/56436] [D loss: 0.476374, acc:  49%] [G loss: 0.421193, adv: 0.010702, recon: 0.018154, id: 0.006915] time: 5:53:32.458022 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 33851/56436] [D loss: 0.523840, acc:  49%] [G loss: 0.810676, adv: 0.009870, recon: 0.036378, id: 0.024420] time: 5:54:03.168476 \n",
      "[Epoch 1/1] [Batch 33901/56436] [D loss: 0.488275, acc:  49%] [G loss: 0.628005, adv: 0.009118, recon: 0.027830, id: 0.026742] time: 5:54:34.371908 \n",
      "[Epoch 1/1] [Batch 33951/56436] [D loss: 0.470960, acc:  49%] [G loss: 0.796502, adv: 0.009626, recon: 0.035696, id: 0.004720] time: 5:55:05.509984 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 34001/56436] [D loss: 0.565768, acc:  49%] [G loss: 0.325588, adv: 0.022326, recon: 0.012588, id: 0.010733] time: 5:55:36.109708 \n",
      "[Epoch 1/1] [Batch 34051/56436] [D loss: 0.516344, acc:  50%] [G loss: 0.406527, adv: 0.007220, recon: 0.018119, id: 0.018835] time: 5:56:07.234966 \n",
      "[Epoch 1/1] [Batch 34101/56436] [D loss: 0.527931, acc:  38%] [G loss: 0.796483, adv: 0.022621, recon: 0.034159, id: 0.034589] time: 5:56:38.445927 \n",
      "[Epoch 1/1] [Batch 34151/56436] [D loss: 0.363884, acc:  50%] [G loss: 2.566069, adv: 0.722588, recon: 0.050950, id: 0.070615] time: 5:57:09.645063 \n",
      "[Epoch 1/1] [Batch 34201/56436] [D loss: 0.432451, acc:  49%] [G loss: 0.562628, adv: 0.013563, recon: 0.024390, id: 0.015617] time: 5:57:40.985300 \n",
      "[Epoch 1/1] [Batch 34251/56436] [D loss: 0.497549, acc:  49%] [G loss: 0.462085, adv: 0.006991, recon: 0.020399, id: 0.006147] time: 5:58:12.169817 \n",
      "[Epoch 1/1] [Batch 34301/56436] [D loss: 0.540415, acc:  50%] [G loss: 0.673482, adv: 0.008570, recon: 0.029806, id: 0.017879] time: 5:58:43.311648 \n",
      "[Epoch 1/1] [Batch 34351/56436] [D loss: 0.532504, acc:  49%] [G loss: 0.496131, adv: 0.007892, recon: 0.021925, id: 0.007960] time: 5:59:14.805889 \n",
      "[Epoch 1/1] [Batch 34401/56436] [D loss: 0.466058, acc:  48%] [G loss: 0.486958, adv: 0.009584, recon: 0.020923, id: 0.010520] time: 5:59:46.147798 \n",
      "[Epoch 1/1] [Batch 34451/56436] [D loss: 0.449870, acc:  48%] [G loss: 0.560840, adv: 0.047166, recon: 0.021384, id: 0.011464] time: 6:00:17.450617 \n",
      "[Epoch 1/1] [Batch 34501/56436] [D loss: 0.524287, acc:  48%] [G loss: 0.274494, adv: 0.004684, recon: 0.011377, id: 0.005905] time: 6:00:48.695424 \n",
      "[Epoch 1/1] [Batch 34551/56436] [D loss: 0.556478, acc:  49%] [G loss: 0.494260, adv: 0.008718, recon: 0.021786, id: 0.005111] time: 6:01:19.856527 \n",
      "[Epoch 1/1] [Batch 34601/56436] [D loss: 0.487830, acc:  48%] [G loss: 0.333428, adv: 0.008251, recon: 0.014264, id: 0.003352] time: 6:01:51.065389 \n",
      "[Epoch 1/1] [Batch 34651/56436] [D loss: 0.638590, acc:  47%] [G loss: 0.590833, adv: 0.011223, recon: 0.025840, id: 0.038347] time: 6:02:22.266779 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 34701/56436] [D loss: 0.600973, acc:  49%] [G loss: 0.349623, adv: 0.022388, recon: 0.013700, id: 0.020671] time: 6:02:52.746736 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 34751/56436] [D loss: 0.658509, acc:  29%] [G loss: 0.339420, adv: 0.013238, recon: 0.013918, id: 0.006353] time: 6:03:23.404909 \n",
      "[Epoch 1/1] [Batch 34801/56436] [D loss: 0.542690, acc:  50%] [G loss: 0.614811, adv: 0.027207, recon: 0.025504, id: 0.030837] time: 6:03:54.566816 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 34851/56436] [D loss: 0.408816, acc:  50%] [G loss: 0.374253, adv: 0.018518, recon: 0.015235, id: 0.013972] time: 6:04:25.424262 \n",
      "[Epoch 1/1] [Batch 34901/56436] [D loss: 0.517329, acc:  25%] [G loss: 0.292896, adv: 0.022074, recon: 0.011666, id: 0.003759] time: 6:04:56.662983 \n",
      "[Epoch 1/1] [Batch 34951/56436] [D loss: 0.625739, acc:  27%] [G loss: 0.489829, adv: 0.019568, recon: 0.020529, id: 0.018340] time: 6:05:28.396524 \n",
      "[Epoch 1/1] [Batch 35001/56436] [D loss: 0.546815, acc:  50%] [G loss: 0.465457, adv: 0.010208, recon: 0.019704, id: 0.004860] time: 6:05:59.837373 \n",
      "[Epoch 1/1] [Batch 35051/56436] [D loss: 0.445593, acc:  49%] [G loss: 0.247764, adv: 0.013217, recon: 0.010001, id: 0.006115] time: 6:06:36.767710 \n",
      "[Epoch 1/1] [Batch 35101/56436] [D loss: 0.513850, acc:  47%] [G loss: 0.424097, adv: 0.009223, recon: 0.018467, id: 0.001188] time: 6:07:07.993785 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 35151/56436] [D loss: 0.529535, acc:  48%] [G loss: 0.417368, adv: 0.006833, recon: 0.018374, id: 0.002229] time: 6:07:38.715164 \n",
      "[Epoch 1/1] [Batch 35201/56436] [D loss: 0.477800, acc:  49%] [G loss: 0.660317, adv: 0.014234, recon: 0.028859, id: 0.004754] time: 6:08:09.847770 \n",
      "[Epoch 1/1] [Batch 35251/56436] [D loss: 0.569375, acc:  49%] [G loss: 0.580907, adv: 0.012650, recon: 0.025110, id: 0.026039] time: 6:08:41.070249 \n",
      "[Epoch 1/1] [Batch 35301/56436] [D loss: 0.430706, acc:  48%] [G loss: 0.565438, adv: 0.015890, recon: 0.024012, id: 0.013687] time: 6:09:12.312211 \n",
      "[Epoch 1/1] [Batch 35351/56436] [D loss: 0.500653, acc:  49%] [G loss: 0.382549, adv: 0.006722, recon: 0.016660, id: 0.009465] time: 6:09:43.482033 \n",
      "[Epoch 1/1] [Batch 35401/56436] [D loss: 0.511285, acc:  44%] [G loss: 0.408431, adv: 0.009031, recon: 0.017762, id: 0.015693] time: 6:10:14.600630 \n",
      "[Epoch 1/1] [Batch 35451/56436] [D loss: 0.505716, acc:  49%] [G loss: 0.164409, adv: 0.005115, recon: 0.007059, id: 0.006830] time: 6:10:45.768807 \n",
      "[Epoch 1/1] [Batch 35501/56436] [D loss: 0.501199, acc:  49%] [G loss: 0.683986, adv: 0.005958, recon: 0.031399, id: 0.010686] time: 6:11:16.917402 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 35551/56436] [D loss: 0.605534, acc:  48%] [G loss: 0.272565, adv: 0.013987, recon: 0.009916, id: 0.003962] time: 6:11:47.933523 \n",
      "[Epoch 1/1] [Batch 35601/56436] [D loss: 0.546653, acc:  48%] [G loss: 0.435543, adv: 0.007561, recon: 0.018359, id: 0.010687] time: 6:12:19.288625 \n",
      "[Epoch 1/1] [Batch 35651/56436] [D loss: 0.592764, acc:  50%] [G loss: 0.571450, adv: 0.014387, recon: 0.024583, id: 0.012721] time: 6:12:50.772048 \n",
      "[Epoch 1/1] [Batch 35701/56436] [D loss: 0.477951, acc:  49%] [G loss: 0.312049, adv: 0.010868, recon: 0.013212, id: 0.006746] time: 6:13:22.222193 \n",
      "[Epoch 1/1] [Batch 35751/56436] [D loss: 0.492701, acc:  49%] [G loss: 0.482981, adv: 0.006737, recon: 0.021554, id: 0.008139] time: 6:13:53.517822 \n",
      "[Epoch 1/1] [Batch 35801/56436] [D loss: 0.465838, acc:  49%] [G loss: 0.700266, adv: 0.011204, recon: 0.030996, id: 0.015936] time: 6:14:24.757933 \n",
      "[Epoch 1/1] [Batch 35851/56436] [D loss: 0.473243, acc:  47%] [G loss: 0.635269, adv: 0.024424, recon: 0.026583, id: 0.012243] time: 6:14:56.026702 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 35901/56436] [D loss: 0.454405, acc:  49%] [G loss: 0.242749, adv: 0.011017, recon: 0.009855, id: 0.006278] time: 6:15:26.721967 \n",
      "[Epoch 1/1] [Batch 35951/56436] [D loss: 0.451435, acc:  47%] [G loss: 0.389365, adv: 0.025648, recon: 0.015676, id: 0.004662] time: 6:15:57.898442 \n",
      "[Epoch 1/1] [Batch 36001/56436] [D loss: 0.537996, acc:  48%] [G loss: 0.205370, adv: 0.005462, recon: 0.008849, id: 0.002857] time: 6:16:29.078936 \n",
      "[Epoch 1/1] [Batch 36051/56436] [D loss: 0.539605, acc:  49%] [G loss: 0.535138, adv: 0.016803, recon: 0.022719, id: 0.015622] time: 6:17:00.251624 \n",
      "[Epoch 1/1] [Batch 36101/56436] [D loss: 0.509972, acc:  49%] [G loss: 0.327980, adv: 0.006640, recon: 0.013961, id: 0.017414] time: 6:17:31.430358 \n",
      "[Epoch 1/1] [Batch 36151/56436] [D loss: 0.530819, acc:  49%] [G loss: 0.496094, adv: 0.008455, recon: 0.021913, id: 0.011945] time: 6:18:02.532442 \n",
      "[Epoch 1/1] [Batch 36201/56436] [D loss: 0.544284, acc:  48%] [G loss: 0.228881, adv: 0.010435, recon: 0.008779, id: 0.004768] time: 6:18:33.806447 \n",
      "[Epoch 1/1] [Batch 36251/56436] [D loss: 0.519596, acc:  49%] [G loss: 0.523395, adv: 0.009161, recon: 0.023019, id: 0.015261] time: 6:19:05.403215 \n",
      "[Epoch 1/1] [Batch 36301/56436] [D loss: 0.557368, acc:  49%] [G loss: 0.256620, adv: 0.005959, recon: 0.010852, id: 0.005907] time: 6:19:36.718574 \n",
      "[Epoch 1/1] [Batch 36351/56436] [D loss: 0.488387, acc:  49%] [G loss: 0.140843, adv: 0.009212, recon: 0.005411, id: 0.002800] time: 6:20:08.014439 \n",
      "[Epoch 1/1] [Batch 36401/56436] [D loss: 0.502165, acc:  49%] [G loss: 0.424914, adv: 0.006133, recon: 0.018475, id: 0.008900] time: 6:20:39.326177 \n",
      "[Epoch 1/1] [Batch 36451/56436] [D loss: 0.525822, acc:  49%] [G loss: 0.350294, adv: 0.011633, recon: 0.014737, id: 0.008603] time: 6:21:10.523359 \n",
      "[Epoch 1/1] [Batch 36501/56436] [D loss: 0.472341, acc:  49%] [G loss: 0.322789, adv: 0.007559, recon: 0.013155, id: 0.005982] time: 6:21:41.915938 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 36551/56436] [D loss: 0.539314, acc:  50%] [G loss: 0.430915, adv: 0.008608, recon: 0.018778, id: 0.012493] time: 6:22:11.907894 \n",
      "[Epoch 1/1] [Batch 36601/56436] [D loss: 0.395696, acc:  49%] [G loss: 0.258440, adv: 0.029012, recon: 0.008878, id: 0.003554] time: 6:22:43.129985 \n",
      "[Epoch 1/1] [Batch 36651/56436] [D loss: 0.541837, acc:  50%] [G loss: 0.480322, adv: 0.009525, recon: 0.020313, id: 0.014991] time: 6:23:14.337931 \n",
      "[Epoch 1/1] [Batch 36701/56436] [D loss: 0.525891, acc:  48%] [G loss: 0.286459, adv: 0.009650, recon: 0.012161, id: 0.005348] time: 6:23:45.496460 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 36751/56436] [D loss: 0.585684, acc:  46%] [G loss: 0.120872, adv: 0.007609, recon: 0.004866, id: 0.002514] time: 6:24:16.036331 \n",
      "[Epoch 1/1] [Batch 36801/56436] [D loss: 0.392405, acc:  49%] [G loss: 0.445788, adv: 0.021654, recon: 0.017282, id: 0.019733] time: 6:24:47.205487 \n",
      "[Epoch 1/1] [Batch 36851/56436] [D loss: 0.472656, acc:  48%] [G loss: 0.269255, adv: 0.008165, recon: 0.011582, id: 0.002957] time: 6:25:18.470722 \n",
      "[Epoch 1/1] [Batch 36901/56436] [D loss: 0.512264, acc:  49%] [G loss: 0.581965, adv: 0.010237, recon: 0.025688, id: 0.030015] time: 6:25:49.588434 \n",
      "[Epoch 1/1] [Batch 36951/56436] [D loss: 0.510044, acc:  44%] [G loss: 0.294916, adv: 0.008203, recon: 0.012709, id: 0.013869] time: 6:26:21.162868 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37001/56436] [D loss: 0.621025, acc:  25%] [G loss: 0.476021, adv: 0.010415, recon: 0.020412, id: 0.015954] time: 6:26:52.011067 \n",
      "[Epoch 1/1] [Batch 37051/56436] [D loss: 0.524593, acc:  49%] [G loss: 0.195355, adv: 0.007026, recon: 0.007785, id: 0.006403] time: 6:27:23.265619 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37101/56436] [D loss: 0.535485, acc:  50%] [G loss: 0.384042, adv: 0.008290, recon: 0.016634, id: 0.009188] time: 6:27:53.882490 \n",
      "[Epoch 1/1] [Batch 37151/56436] [D loss: 0.479714, acc:  49%] [G loss: 0.333195, adv: 0.009444, recon: 0.013992, id: 0.009803] time: 6:28:25.070146 \n",
      "[Epoch 1/1] [Batch 37201/56436] [D loss: 0.485631, acc:  49%] [G loss: 0.410721, adv: 0.008218, recon: 0.017994, id: 0.017250] time: 6:28:56.402306 \n",
      "[Epoch 1/1] [Batch 37251/56436] [D loss: 0.668852, acc:  25%] [G loss: 0.508455, adv: 0.008473, recon: 0.022065, id: 0.017629] time: 6:29:27.586661 \n",
      "[Epoch 1/1] [Batch 37301/56436] [D loss: 0.517992, acc:  48%] [G loss: 0.394657, adv: 0.005642, recon: 0.016835, id: 0.002841] time: 6:29:58.950730 \n",
      "[Epoch 1/1] [Batch 37351/56436] [D loss: 0.523936, acc:  49%] [G loss: 0.436053, adv: 0.013900, recon: 0.018314, id: 0.008462] time: 6:30:30.020213 \n",
      "[Epoch 1/1] [Batch 37401/56436] [D loss: 0.560222, acc:  48%] [G loss: 0.173936, adv: 0.010683, recon: 0.006586, id: 0.002857] time: 6:31:01.283932 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37451/56436] [D loss: 0.658582, acc:  25%] [G loss: 0.269772, adv: 0.005509, recon: 0.011787, id: 0.004028] time: 6:31:31.906875 \n",
      "[Epoch 1/1] [Batch 37501/56436] [D loss: 0.555103, acc:  48%] [G loss: 0.372488, adv: 0.010382, recon: 0.016070, id: 0.004961] time: 6:32:03.269095 \n",
      "[Epoch 1/1] [Batch 37551/56436] [D loss: 0.544687, acc:  48%] [G loss: 0.183982, adv: 0.010811, recon: 0.007254, id: 0.004978] time: 6:32:34.883176 \n",
      "[Epoch 1/1] [Batch 37601/56436] [D loss: 0.529342, acc:  49%] [G loss: 0.524420, adv: 0.012552, recon: 0.022591, id: 0.013238] time: 6:33:06.459816 \n",
      "[Epoch 1/1] [Batch 37651/56436] [D loss: 0.541239, acc:  49%] [G loss: 0.409696, adv: 0.010649, recon: 0.017076, id: 0.008757] time: 6:33:38.119815 \n",
      "[Epoch 1/1] [Batch 37701/56436] [D loss: 0.473843, acc:  48%] [G loss: 0.269791, adv: 0.016223, recon: 0.010770, id: 0.004449] time: 6:34:09.499717 \n",
      "[Epoch 1/1] [Batch 37751/56436] [D loss: 0.562548, acc:  49%] [G loss: 0.172866, adv: 0.009359, recon: 0.006978, id: 0.006803] time: 6:34:41.150681 \n",
      "[Epoch 1/1] [Batch 37801/56436] [D loss: 0.438948, acc:  49%] [G loss: 0.298297, adv: 0.011622, recon: 0.012505, id: 0.006587] time: 6:35:12.437617 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37851/56436] [D loss: 0.547611, acc:  49%] [G loss: 0.420793, adv: 0.010701, recon: 0.017654, id: 0.014140] time: 6:35:43.034247 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37901/56436] [D loss: 0.513677, acc:  48%] [G loss: 0.219989, adv: 0.005024, recon: 0.009301, id: 0.004520] time: 6:36:13.697276 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 37951/56436] [D loss: 0.501147, acc:  49%] [G loss: 0.186666, adv: 0.008579, recon: 0.007495, id: 0.007170] time: 6:36:44.372625 \n",
      "[Epoch 1/1] [Batch 38001/56436] [D loss: 0.493342, acc:  49%] [G loss: 0.467341, adv: 0.006466, recon: 0.021041, id: 0.009502] time: 6:37:15.463806 \n",
      "[Epoch 1/1] [Batch 38051/56436] [D loss: 0.342081, acc:  49%] [G loss: 0.570794, adv: 0.045861, recon: 0.017282, id: 0.020417] time: 6:37:46.599710 \n",
      "[Epoch 1/1] [Batch 38101/56436] [D loss: 0.528186, acc:  48%] [G loss: 0.331815, adv: 0.006244, recon: 0.014379, id: 0.011443] time: 6:38:17.753620 \n",
      "[Epoch 1/1] [Batch 38151/56436] [D loss: 0.547511, acc:  49%] [G loss: 0.379100, adv: 0.010863, recon: 0.015228, id: 0.018810] time: 6:38:48.835590 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 38201/56436] [D loss: 0.512285, acc:  49%] [G loss: 0.273093, adv: 0.004689, recon: 0.012040, id: 0.005818] time: 6:39:19.891023 \n",
      "[Epoch 1/1] [Batch 38251/56436] [D loss: 0.442679, acc:  49%] [G loss: 0.595239, adv: 0.015621, recon: 0.025764, id: 0.022080] time: 6:39:51.274157 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 38301/56436] [D loss: 0.534042, acc:  49%] [G loss: 0.468528, adv: 0.006884, recon: 0.020836, id: 0.008555] time: 6:40:21.329273 \n",
      "[Epoch 1/1] [Batch 38351/56436] [D loss: 0.477841, acc:  47%] [G loss: 0.315232, adv: 0.008812, recon: 0.013420, id: 0.004217] time: 6:40:52.568540 \n",
      "[Epoch 1/1] [Batch 38401/56436] [D loss: 0.739352, acc:  25%] [G loss: 0.319990, adv: 0.015470, recon: 0.013368, id: 0.004960] time: 6:41:23.689866 \n",
      "[Epoch 1/1] [Batch 38451/56436] [D loss: 0.532885, acc:  41%] [G loss: 0.407118, adv: 0.008189, recon: 0.016830, id: 0.014653] time: 6:41:55.007801 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 38501/56436] [D loss: 0.489118, acc:  49%] [G loss: 0.466422, adv: 0.010203, recon: 0.020383, id: 0.007386] time: 6:42:25.560322 \n",
      "[Epoch 1/1] [Batch 38551/56436] [D loss: 0.550995, acc:  48%] [G loss: 0.706075, adv: 0.015845, recon: 0.030638, id: 0.026570] time: 6:42:56.760438 \n",
      "[Epoch 1/1] [Batch 38601/56436] [D loss: 0.530298, acc:  48%] [G loss: 0.622702, adv: 0.006635, recon: 0.027822, id: 0.024293] time: 6:43:27.929702 \n",
      "[Epoch 1/1] [Batch 38651/56436] [D loss: 0.530961, acc:  49%] [G loss: 0.157576, adv: 0.007414, recon: 0.006240, id: 0.006155] time: 6:43:59.103847 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 38701/56436] [D loss: 0.468385, acc:  48%] [G loss: 0.342294, adv: 0.014696, recon: 0.014268, id: 0.015765] time: 6:44:29.004303 \n",
      "[Epoch 1/1] [Batch 38751/56436] [D loss: 0.496422, acc:  48%] [G loss: 0.544932, adv: 0.008965, recon: 0.023895, id: 0.007919] time: 6:45:00.131236 \n",
      "[Epoch 1/1] [Batch 38801/56436] [D loss: 0.480699, acc:  49%] [G loss: 0.321367, adv: 0.007062, recon: 0.013890, id: 0.006676] time: 6:45:31.330800 \n",
      "[Epoch 1/1] [Batch 38851/56436] [D loss: 0.554749, acc:  48%] [G loss: 0.214028, adv: 0.007747, recon: 0.008280, id: 0.003623] time: 6:46:02.458982 \n",
      "[Epoch 1/1] [Batch 38901/56436] [D loss: 0.587801, acc:  49%] [G loss: 0.765431, adv: 0.015198, recon: 0.032632, id: 0.008630] time: 6:46:34.014083 \n",
      "[Epoch 1/1] [Batch 38951/56436] [D loss: 0.565418, acc:  48%] [G loss: 0.683033, adv: 0.010194, recon: 0.030065, id: 0.023887] time: 6:47:05.315845 \n",
      "[Epoch 1/1] [Batch 39001/56436] [D loss: 0.487439, acc:  49%] [G loss: 0.291310, adv: 0.007057, recon: 0.012471, id: 0.008172] time: 6:47:36.620466 \n",
      "[Epoch 1/1] [Batch 39051/56436] [D loss: 0.503496, acc:  49%] [G loss: 0.594679, adv: 0.007538, recon: 0.025999, id: 0.017900] time: 6:48:07.953056 \n",
      "[Epoch 1/1] [Batch 39101/56436] [D loss: 0.570002, acc:  48%] [G loss: 0.333936, adv: 0.008175, recon: 0.014435, id: 0.001437] time: 6:48:39.140127 \n",
      "[Epoch 1/1] [Batch 39151/56436] [D loss: 0.467237, acc:  48%] [G loss: 0.780475, adv: 0.018812, recon: 0.033813, id: 0.017374] time: 6:49:10.270592 \n",
      "[Epoch 1/1] [Batch 39201/56436] [D loss: 0.567688, acc:  49%] [G loss: 0.386231, adv: 0.010316, recon: 0.015973, id: 0.007676] time: 6:49:41.499054 \n",
      "[Epoch 1/1] [Batch 39251/56436] [D loss: 0.594678, acc:  42%] [G loss: 0.285425, adv: 0.006860, recon: 0.012213, id: 0.013707] time: 6:50:12.726053 \n",
      "[Epoch 1/1] [Batch 39301/56436] [D loss: 0.459761, acc:  48%] [G loss: 0.338335, adv: 0.012004, recon: 0.014343, id: 0.008848] time: 6:50:43.918131 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 39351/56436] [D loss: 0.481108, acc:  49%] [G loss: 0.199889, adv: 0.008161, recon: 0.008243, id: 0.006771] time: 6:51:14.475290 \n",
      "[Epoch 1/1] [Batch 39401/56436] [D loss: 0.574402, acc:  49%] [G loss: 0.365864, adv: 0.014241, recon: 0.015098, id: 0.010314] time: 6:51:45.655740 \n",
      "[Epoch 1/1] [Batch 39451/56436] [D loss: 0.461534, acc:  48%] [G loss: 0.529759, adv: 0.010716, recon: 0.023951, id: 0.015269] time: 6:52:16.651086 \n",
      "[Epoch 1/1] [Batch 39501/56436] [D loss: 0.473969, acc:  49%] [G loss: 0.195000, adv: 0.007298, recon: 0.008058, id: 0.007332] time: 6:52:47.827148 \n",
      "[Epoch 1/1] [Batch 39551/56436] [D loss: 0.505131, acc:  49%] [G loss: 0.310716, adv: 0.006146, recon: 0.013555, id: 0.006431] time: 6:53:19.424912 \n",
      "[Epoch 1/1] [Batch 39601/56436] [D loss: 0.571824, acc:  50%] [G loss: 0.448307, adv: 0.022884, recon: 0.018603, id: 0.008314] time: 6:53:50.671224 \n",
      "[Epoch 1/1] [Batch 39651/56436] [D loss: 0.527725, acc:  49%] [G loss: 0.296712, adv: 0.009510, recon: 0.012322, id: 0.011409] time: 6:54:21.699299 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 39701/56436] [D loss: 0.512300, acc:  48%] [G loss: 0.366606, adv: 0.009073, recon: 0.015794, id: 0.004109] time: 6:54:52.701967 \n",
      "[Epoch 1/1] [Batch 39751/56436] [D loss: 0.530234, acc:  45%] [G loss: 0.649212, adv: 0.008751, recon: 0.027367, id: 0.030732] time: 6:55:24.034455 \n",
      "[Epoch 1/1] [Batch 39801/56436] [D loss: 0.509905, acc:  48%] [G loss: 0.399721, adv: 0.006104, recon: 0.016954, id: 0.007740] time: 6:55:55.298103 \n",
      "[Epoch 1/1] [Batch 39851/56436] [D loss: 0.632280, acc:  26%] [G loss: 0.557018, adv: 0.010604, recon: 0.024068, id: 0.012302] time: 6:56:26.540981 \n",
      "[Epoch 1/1] [Batch 39901/56436] [D loss: 0.523457, acc:  46%] [G loss: 0.372960, adv: 0.005712, recon: 0.016294, id: 0.007773] time: 6:56:57.830076 \n",
      "[Epoch 1/1] [Batch 39951/56436] [D loss: 0.671614, acc:  25%] [G loss: 0.292140, adv: 0.006520, recon: 0.012687, id: 0.000049] time: 6:57:29.046327 \n",
      "[Epoch 1/1] [Batch 40001/56436] [D loss: 0.545004, acc:  49%] [G loss: 0.575980, adv: 0.008132, recon: 0.025706, id: 0.014144] time: 6:58:00.273583 \n",
      "[Epoch 1/1] [Batch 40051/56436] [D loss: 0.499688, acc:  49%] [G loss: 0.195116, adv: 0.008126, recon: 0.008105, id: 0.008689] time: 6:58:37.155200 \n",
      "[Epoch 1/1] [Batch 40101/56436] [D loss: 0.523924, acc:  48%] [G loss: 0.447954, adv: 0.007059, recon: 0.020659, id: 0.004346] time: 6:59:08.171295 \n",
      "[Epoch 1/1] [Batch 40151/56436] [D loss: 0.542448, acc:  47%] [G loss: 0.157995, adv: 0.008532, recon: 0.006478, id: 0.003034] time: 6:59:39.204760 \n",
      "[Epoch 1/1] [Batch 40201/56436] [D loss: 0.562987, acc:  49%] [G loss: 0.661196, adv: 0.020618, recon: 0.028336, id: 0.028332] time: 7:00:10.282047 \n",
      "[Epoch 1/1] [Batch 40251/56436] [D loss: 0.523952, acc:  48%] [G loss: 0.303497, adv: 0.005437, recon: 0.012519, id: 0.002963] time: 7:00:41.358239 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 40301/56436] [D loss: 0.497342, acc:  48%] [G loss: 0.237325, adv: 0.007787, recon: 0.010117, id: 0.002880] time: 7:01:11.781679 \n",
      "[Epoch 1/1] [Batch 40351/56436] [D loss: 0.491446, acc:  49%] [G loss: 0.349365, adv: 0.010030, recon: 0.014909, id: 0.009212] time: 7:01:42.738488 \n",
      "[Epoch 1/1] [Batch 40401/56436] [D loss: 0.461299, acc:  49%] [G loss: 0.507296, adv: 0.013761, recon: 0.021810, id: 0.022139] time: 7:02:14.488090 \n",
      "[Epoch 1/1] [Batch 40451/56436] [D loss: 0.577762, acc:  48%] [G loss: 0.422389, adv: 0.010347, recon: 0.017901, id: 0.002693] time: 7:02:45.866611 \n",
      "[Epoch 1/1] [Batch 40501/56436] [D loss: 0.542454, acc:  48%] [G loss: 0.277039, adv: 0.009538, recon: 0.011747, id: 0.005239] time: 7:03:17.200483 \n",
      "[Epoch 1/1] [Batch 40551/56436] [D loss: 0.561169, acc:  50%] [G loss: 0.358290, adv: 0.018988, recon: 0.014591, id: 0.011894] time: 7:03:48.359650 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 40601/56436] [D loss: 0.671014, acc:  50%] [G loss: 0.432692, adv: 0.027814, recon: 0.016203, id: 0.014912] time: 7:04:18.901409 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 40651/56436] [D loss: 0.571158, acc:  49%] [G loss: 0.445639, adv: 0.017700, recon: 0.018465, id: 0.010101] time: 7:04:49.564289 \n",
      "[Epoch 1/1] [Batch 40701/56436] [D loss: 0.450511, acc:  49%] [G loss: 0.164756, adv: 0.009924, recon: 0.006545, id: 0.006298] time: 7:05:20.625269 \n",
      "[Epoch 1/1] [Batch 40751/56436] [D loss: 0.510924, acc:  49%] [G loss: 0.269181, adv: 0.010299, recon: 0.011466, id: 0.008331] time: 7:05:51.744863 \n",
      "[Epoch 1/1] [Batch 40801/56436] [D loss: 0.513453, acc:  49%] [G loss: 0.310262, adv: 0.007127, recon: 0.013313, id: 0.004100] time: 7:06:23.015298 \n",
      "[Epoch 1/1] [Batch 40851/56436] [D loss: 0.521169, acc:  44%] [G loss: 0.534459, adv: 0.013178, recon: 0.022310, id: 0.022309] time: 7:06:54.123488 \n",
      "[Epoch 1/1] [Batch 40901/56436] [D loss: 0.496734, acc:  49%] [G loss: 0.341661, adv: 0.009562, recon: 0.014602, id: 0.017566] time: 7:07:25.206621 \n",
      "[Epoch 1/1] [Batch 40951/56436] [D loss: 0.560463, acc:  26%] [G loss: 0.385992, adv: 0.007194, recon: 0.016461, id: 0.008393] time: 7:07:56.259343 \n",
      "[Epoch 1/1] [Batch 41001/56436] [D loss: 0.554628, acc:  48%] [G loss: 0.281440, adv: 0.007360, recon: 0.011734, id: 0.002755] time: 7:08:27.389829 \n",
      "[Epoch 1/1] [Batch 41051/56436] [D loss: 0.543840, acc:  45%] [G loss: 0.381075, adv: 0.007458, recon: 0.015942, id: 0.018534] time: 7:08:58.444750 \n",
      "[Epoch 1/1] [Batch 41101/56436] [D loss: 0.514958, acc:  48%] [G loss: 0.553398, adv: 0.006658, recon: 0.024253, id: 0.024476] time: 7:09:29.435874 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 41151/56436] [D loss: 0.551144, acc:  49%] [G loss: 0.451794, adv: 0.011212, recon: 0.019582, id: 0.007528] time: 7:10:00.034329 \n",
      "[Epoch 1/1] [Batch 41201/56436] [D loss: 0.482010, acc:  48%] [G loss: 0.439287, adv: 0.014841, recon: 0.018240, id: 0.011275] time: 7:10:31.111165 \n",
      "[Epoch 1/1] [Batch 41251/56436] [D loss: 0.489372, acc:  49%] [G loss: 0.416783, adv: 0.008439, recon: 0.018045, id: 0.005422] time: 7:11:02.657009 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 41301/56436] [D loss: 0.491398, acc:  48%] [G loss: 0.247634, adv: 0.007107, recon: 0.010511, id: 0.002421] time: 7:11:32.943773 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 41351/56436] [D loss: 0.403026, acc:  48%] [G loss: 0.214202, adv: 0.022725, recon: 0.007753, id: 0.007395] time: 7:12:03.701733 \n",
      "[Epoch 1/1] [Batch 41401/56436] [D loss: 0.520381, acc:  48%] [G loss: 0.337810, adv: 0.008446, recon: 0.013823, id: 0.003981] time: 7:12:34.938203 \n",
      "[Epoch 1/1] [Batch 41451/56436] [D loss: 0.534198, acc:  50%] [G loss: 0.363217, adv: 0.009162, recon: 0.015707, id: 0.009284] time: 7:13:06.244375 \n",
      "[Epoch 1/1] [Batch 41501/56436] [D loss: 0.542332, acc:  49%] [G loss: 0.211108, adv: 0.007433, recon: 0.009011, id: 0.007537] time: 7:13:37.423180 \n",
      "[Epoch 1/1] [Batch 41551/56436] [D loss: 0.626416, acc:  25%] [G loss: 0.374432, adv: 0.014425, recon: 0.015710, id: 0.008352] time: 7:14:08.501746 \n",
      "[Epoch 1/1] [Batch 41601/56436] [D loss: 0.467374, acc:  48%] [G loss: 0.312105, adv: 0.013802, recon: 0.012890, id: 0.006724] time: 7:14:39.697587 \n",
      "[Epoch 1/1] [Batch 41651/56436] [D loss: 0.651447, acc:  27%] [G loss: 0.502633, adv: 0.011508, recon: 0.021804, id: 0.020242] time: 7:15:10.928791 \n",
      "[Epoch 1/1] [Batch 41701/56436] [D loss: 0.468272, acc:  49%] [G loss: 0.466302, adv: 0.009879, recon: 0.020335, id: 0.022390] time: 7:15:41.961529 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 41751/56436] [D loss: 0.493689, acc:  49%] [G loss: 0.505530, adv: 0.015004, recon: 0.021577, id: 0.020537] time: 7:16:11.938594 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 41801/56436] [D loss: 0.522283, acc:  49%] [G loss: 0.552078, adv: 0.010052, recon: 0.024104, id: 0.026101] time: 7:16:42.531781 \n",
      "[Epoch 1/1] [Batch 41851/56436] [D loss: 0.556007, acc:  48%] [G loss: 0.203983, adv: 0.009916, recon: 0.008412, id: 0.004406] time: 7:17:13.790406 \n",
      "[Epoch 1/1] [Batch 41901/56436] [D loss: 0.595899, acc:  47%] [G loss: 0.187627, adv: 0.013112, recon: 0.007147, id: 0.005800] time: 7:17:44.888294 \n",
      "[Epoch 1/1] [Batch 41951/56436] [D loss: 0.539175, acc:  48%] [G loss: 0.298984, adv: 0.011425, recon: 0.012180, id: 0.004958] time: 7:18:16.123394 \n",
      "[Epoch 1/1] [Batch 42001/56436] [D loss: 0.453270, acc:  48%] [G loss: 0.382709, adv: 0.012926, recon: 0.016249, id: 0.009410] time: 7:18:47.170882 \n",
      "[Epoch 1/1] [Batch 42051/56436] [D loss: 0.515723, acc:  49%] [G loss: 0.505035, adv: 0.007994, recon: 0.022302, id: 0.015816] time: 7:19:18.232167 \n",
      "[Epoch 1/1] [Batch 42101/56436] [D loss: 0.480887, acc:  49%] [G loss: 0.348568, adv: 0.006714, recon: 0.015107, id: 0.007428] time: 7:19:49.374621 \n",
      "[Epoch 1/1] [Batch 42151/56436] [D loss: 0.515692, acc:  49%] [G loss: 0.304160, adv: 0.004871, recon: 0.013431, id: 0.004786] time: 7:20:20.427664 \n",
      "[Epoch 1/1] [Batch 42201/56436] [D loss: 0.465711, acc:  50%] [G loss: 0.278087, adv: 0.011424, recon: 0.011478, id: 0.014491] time: 7:20:51.724470 \n",
      "[Epoch 1/1] [Batch 42251/56436] [D loss: 0.495286, acc:  49%] [G loss: 0.342906, adv: 0.008883, recon: 0.014926, id: 0.014003] time: 7:21:23.358869 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 42301/56436] [D loss: 0.536583, acc:  48%] [G loss: 0.211605, adv: 0.007728, recon: 0.008900, id: 0.004416] time: 7:21:54.066990 \n",
      "[Epoch 1/1] [Batch 42351/56436] [D loss: 0.440793, acc:  49%] [G loss: 0.635188, adv: 0.010700, recon: 0.028004, id: 0.021384] time: 7:22:25.189567 \n",
      "[Epoch 1/1] [Batch 42401/56436] [D loss: 0.529464, acc:  48%] [G loss: 0.526951, adv: 0.008994, recon: 0.023265, id: 0.017664] time: 7:22:56.546185 \n",
      "[Epoch 1/1] [Batch 42451/56436] [D loss: 0.505215, acc:  49%] [G loss: 0.560774, adv: 0.007652, recon: 0.024475, id: 0.012988] time: 7:23:27.822618 \n",
      "[Epoch 1/1] [Batch 42501/56436] [D loss: 0.498682, acc:  48%] [G loss: 0.128660, adv: 0.006327, recon: 0.005280, id: 0.002702] time: 7:23:59.041956 \n",
      "[Epoch 1/1] [Batch 42551/56436] [D loss: 0.466543, acc:  49%] [G loss: 0.397501, adv: 0.009621, recon: 0.016919, id: 0.008453] time: 7:24:30.234256 \n",
      "[Epoch 1/1] [Batch 42601/56436] [D loss: 0.476029, acc:  49%] [G loss: 0.157470, adv: 0.008550, recon: 0.006298, id: 0.002640] time: 7:25:01.440669 \n",
      "[Epoch 1/1] [Batch 42651/56436] [D loss: 0.500338, acc:  49%] [G loss: 0.372537, adv: 0.007939, recon: 0.015631, id: 0.016533] time: 7:25:32.618165 \n",
      "[Epoch 1/1] [Batch 42701/56436] [D loss: 0.413038, acc:  49%] [G loss: 0.691912, adv: 0.016852, recon: 0.029396, id: 0.016082] time: 7:26:03.685103 \n",
      "[Epoch 1/1] [Batch 42751/56436] [D loss: 0.516594, acc:  50%] [G loss: 0.237618, adv: 0.008038, recon: 0.010187, id: 0.008050] time: 7:26:34.780168 \n",
      "[Epoch 1/1] [Batch 42801/56436] [D loss: 0.487929, acc:  46%] [G loss: 0.535189, adv: 0.019767, recon: 0.022441, id: 0.036390] time: 7:27:05.949596 \n",
      "[Epoch 1/1] [Batch 42851/56436] [D loss: 0.339112, acc:  55%] [G loss: 0.923394, adv: 0.108654, recon: 0.030788, id: 0.002779] time: 7:27:37.120163 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 42901/56436] [D loss: 0.505013, acc:  49%] [G loss: 0.524380, adv: 0.011706, recon: 0.022838, id: 0.012187] time: 7:28:07.506454 \n",
      "[Epoch 1/1] [Batch 42951/56436] [D loss: 0.618457, acc:  25%] [G loss: 0.907493, adv: 0.011059, recon: 0.040571, id: 0.047656] time: 7:28:38.859301 \n",
      "[Epoch 1/1] [Batch 43001/56436] [D loss: 0.530726, acc:  49%] [G loss: 0.665951, adv: 0.016007, recon: 0.023166, id: 0.127370] time: 7:29:10.239904 \n",
      "[Epoch 1/1] [Batch 43051/56436] [D loss: 0.581396, acc:  48%] [G loss: 0.444252, adv: 0.014904, recon: 0.018912, id: 0.005782] time: 7:29:41.549652 \n",
      "[Epoch 1/1] [Batch 43101/56436] [D loss: 0.530802, acc:  48%] [G loss: 0.254744, adv: 0.009093, recon: 0.010394, id: 0.008858] time: 7:30:12.964099 \n",
      "[Epoch 1/1] [Batch 43151/56436] [D loss: 0.634713, acc:  25%] [G loss: 0.449932, adv: 0.005492, recon: 0.016707, id: 0.000019] time: 7:30:44.200366 \n",
      "[Epoch 1/1] [Batch 43201/56436] [D loss: 0.529963, acc:  48%] [G loss: 0.334578, adv: 0.006323, recon: 0.014412, id: 0.015513] time: 7:31:15.316440 \n",
      "[Epoch 1/1] [Batch 43251/56436] [D loss: 0.522920, acc:  50%] [G loss: 0.480644, adv: 0.010958, recon: 0.020591, id: 0.010655] time: 7:31:46.567273 \n",
      "[Epoch 1/1] [Batch 43301/56436] [D loss: 0.485323, acc:  49%] [G loss: 0.330556, adv: 0.009464, recon: 0.013903, id: 0.016515] time: 7:32:17.652929 \n",
      "[Epoch 1/1] [Batch 43351/56436] [D loss: 0.584446, acc:  25%] [G loss: 0.645575, adv: 0.017953, recon: 0.026950, id: 0.024671] time: 7:32:48.809918 \n",
      "[Epoch 1/1] [Batch 43401/56436] [D loss: 0.505580, acc:  29%] [G loss: 0.474963, adv: 0.024293, recon: 0.017381, id: 0.055067] time: 7:33:19.929324 \n",
      "[Epoch 1/1] [Batch 43451/56436] [D loss: 0.571315, acc:  50%] [G loss: 0.597225, adv: 0.017012, recon: 0.025371, id: 0.025047] time: 7:33:51.091572 \n",
      "[Epoch 1/1] [Batch 43501/56436] [D loss: 0.400075, acc:  48%] [G loss: 1.225718, adv: 0.060408, recon: 0.049353, id: 0.016117] time: 7:34:22.096251 \n",
      "[Epoch 1/1] [Batch 43551/56436] [D loss: 0.513489, acc:  49%] [G loss: 0.192379, adv: 0.007073, recon: 0.007971, id: 0.011526] time: 7:34:53.111175 \n",
      "[Epoch 1/1] [Batch 43601/56436] [D loss: 0.544641, acc:  49%] [G loss: 0.483626, adv: 0.013350, recon: 0.020447, id: 0.013847] time: 7:35:24.019461 \n",
      "[Epoch 1/1] [Batch 43651/56436] [D loss: 0.476720, acc:  49%] [G loss: 0.327115, adv: 0.007668, recon: 0.014210, id: 0.010445] time: 7:35:55.130714 \n",
      "[Epoch 1/1] [Batch 43701/56436] [D loss: 0.493926, acc:  48%] [G loss: 0.629103, adv: 0.010684, recon: 0.027645, id: 0.024216] time: 7:36:26.141946 \n",
      "[Epoch 1/1] [Batch 43751/56436] [D loss: 0.561456, acc:  49%] [G loss: 0.861561, adv: 0.013948, recon: 0.038127, id: 0.052594] time: 7:36:57.207560 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 43801/56436] [D loss: 0.720289, acc:  25%] [G loss: 0.839935, adv: 0.019795, recon: 0.036737, id: 0.029112] time: 7:37:27.040498 \n",
      "[Epoch 1/1] [Batch 43851/56436] [D loss: 0.481666, acc:  49%] [G loss: 0.208999, adv: 0.007899, recon: 0.006858, id: 0.005259] time: 7:37:58.144938 \n",
      "[Epoch 1/1] [Batch 43901/56436] [D loss: 0.431311, acc:  49%] [G loss: 0.333910, adv: 0.023383, recon: 0.012428, id: 0.010969] time: 7:38:29.304610 \n",
      "[Epoch 1/1] [Batch 43951/56436] [D loss: 0.547994, acc:  47%] [G loss: 0.278126, adv: 0.006345, recon: 0.012026, id: 0.004445] time: 7:39:01.010002 \n",
      "[Epoch 1/1] [Batch 44001/56436] [D loss: 0.513003, acc:  49%] [G loss: 0.159482, adv: 0.005980, recon: 0.006616, id: 0.005623] time: 7:39:32.415807 \n",
      "[Epoch 1/1] [Batch 44051/56436] [D loss: 0.526273, acc:  49%] [G loss: 0.360419, adv: 0.005747, recon: 0.015761, id: 0.011587] time: 7:40:03.731319 \n",
      "[Epoch 1/1] [Batch 44101/56436] [D loss: 0.514945, acc:  48%] [G loss: 0.210565, adv: 0.006201, recon: 0.008860, id: 0.008576] time: 7:40:35.047462 \n",
      "[Epoch 1/1] [Batch 44151/56436] [D loss: 0.499557, acc:  49%] [G loss: 0.340162, adv: 0.006972, recon: 0.013511, id: 0.035189] time: 7:41:06.408536 \n",
      "[Epoch 1/1] [Batch 44201/56436] [D loss: 0.559041, acc:  48%] [G loss: 0.233966, adv: 0.011847, recon: 0.009362, id: 0.004864] time: 7:41:37.514734 \n",
      "[Epoch 1/1] [Batch 44251/56436] [D loss: 0.550572, acc:  48%] [G loss: 0.200953, adv: 0.007704, recon: 0.008390, id: 0.007114] time: 7:42:08.736858 \n",
      "[Epoch 1/1] [Batch 44301/56436] [D loss: 0.499314, acc:  48%] [G loss: 0.350995, adv: 0.006337, recon: 0.015214, id: 0.006229] time: 7:42:40.024543 \n",
      "[Epoch 1/1] [Batch 44351/56436] [D loss: 0.500969, acc:  48%] [G loss: 0.416851, adv: 0.011665, recon: 0.017172, id: 0.011463] time: 7:43:11.153180 \n",
      "[Epoch 1/1] [Batch 44401/56436] [D loss: 0.454941, acc:  47%] [G loss: 0.498057, adv: 0.016729, recon: 0.021229, id: 0.010237] time: 7:43:42.230327 \n",
      "[Epoch 1/1] [Batch 44451/56436] [D loss: 0.505324, acc:  48%] [G loss: 0.464303, adv: 0.006996, recon: 0.020532, id: 0.024051] time: 7:44:13.425267 \n",
      "[Epoch 1/1] [Batch 44501/56436] [D loss: 0.503415, acc:  49%] [G loss: 0.186600, adv: 0.005602, recon: 0.008047, id: 0.008494] time: 7:44:44.601398 \n",
      "[Epoch 1/1] [Batch 44551/56436] [D loss: 0.522248, acc:  49%] [G loss: 0.508147, adv: 0.014881, recon: 0.022215, id: 0.017508] time: 7:45:15.680166 \n",
      "[Epoch 1/1] [Batch 44601/56436] [D loss: 0.526127, acc:  49%] [G loss: 0.301947, adv: 0.012035, recon: 0.012784, id: 0.010766] time: 7:45:46.775988 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 44651/56436] [D loss: 0.675021, acc:  25%] [G loss: 0.227264, adv: 0.004740, recon: 0.008832, id: 0.000312] time: 7:46:17.377562 \n",
      "[Epoch 1/1] [Batch 44701/56436] [D loss: 0.648641, acc:  25%] [G loss: 0.476741, adv: 0.007875, recon: 0.017617, id: 0.084920] time: 7:46:48.509447 \n",
      "[Epoch 1/1] [Batch 44751/56436] [D loss: 0.570586, acc:  49%] [G loss: 0.423495, adv: 0.010230, recon: 0.018299, id: 0.018210] time: 7:47:19.553930 \n",
      "[Epoch 1/1] [Batch 44801/56436] [D loss: 0.497860, acc:  49%] [G loss: 0.400892, adv: 0.011016, recon: 0.017169, id: 0.024230] time: 7:47:50.687051 \n",
      "[Epoch 1/1] [Batch 44851/56436] [D loss: 0.462697, acc:  50%] [G loss: 0.522589, adv: 0.013251, recon: 0.022558, id: 0.014982] time: 7:48:21.772170 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 44901/56436] [D loss: 0.482268, acc:  49%] [G loss: 0.558915, adv: 0.007223, recon: 0.024890, id: 0.015354] time: 7:48:52.565700 \n",
      "[Epoch 1/1] [Batch 44951/56436] [D loss: 0.530821, acc:  47%] [G loss: 0.327671, adv: 0.008676, recon: 0.013965, id: 0.005479] time: 7:49:23.947290 \n",
      "[Epoch 1/1] [Batch 45001/56436] [D loss: 0.518145, acc:  48%] [G loss: 0.375921, adv: 0.006077, recon: 0.016399, id: 0.003814] time: 7:49:55.415695 \n",
      "[Epoch 1/1] [Batch 45051/56436] [D loss: 0.462857, acc:  49%] [G loss: 0.212165, adv: 0.010916, recon: 0.008575, id: 0.006978] time: 7:50:32.202127 \n",
      "[Epoch 1/1] [Batch 45101/56436] [D loss: 0.536245, acc:  48%] [G loss: 0.200626, adv: 0.006245, recon: 0.008478, id: 0.004560] time: 7:51:03.487678 \n",
      "[Epoch 1/1] [Batch 45151/56436] [D loss: 0.539455, acc:  49%] [G loss: 0.314478, adv: 0.007773, recon: 0.012145, id: 0.005292] time: 7:51:34.977802 \n",
      "[Epoch 1/1] [Batch 45201/56436] [D loss: 0.542365, acc:  49%] [G loss: 0.696533, adv: 0.005919, recon: 0.031645, id: 0.014060] time: 7:52:06.244608 \n",
      "[Epoch 1/1] [Batch 45251/56436] [D loss: 0.537601, acc:  49%] [G loss: 0.373726, adv: 0.007562, recon: 0.016308, id: 0.010551] time: 7:52:37.495987 \n",
      "[Epoch 1/1] [Batch 45301/56436] [D loss: 0.468405, acc:  49%] [G loss: 0.276608, adv: 0.009710, recon: 0.011732, id: 0.009038] time: 7:53:08.628018 \n",
      "[Epoch 1/1] [Batch 45351/56436] [D loss: 0.522347, acc:  49%] [G loss: 0.320018, adv: 0.005109, recon: 0.013536, id: 0.007106] time: 7:53:39.667768 \n",
      "[Epoch 1/1] [Batch 45401/56436] [D loss: 0.441263, acc:  50%] [G loss: 0.366436, adv: 0.017354, recon: 0.012395, id: 0.009019] time: 7:54:10.794018 \n",
      "[Epoch 1/1] [Batch 45451/56436] [D loss: 0.499377, acc:  49%] [G loss: 0.538275, adv: 0.005533, recon: 0.023801, id: 0.013765] time: 7:54:41.949596 \n",
      "[Epoch 1/1] [Batch 45501/56436] [D loss: 0.588935, acc:  40%] [G loss: 0.435705, adv: 0.009558, recon: 0.018912, id: 0.004520] time: 7:55:12.921505 \n",
      "[Epoch 1/1] [Batch 45551/56436] [D loss: 0.468942, acc:  49%] [G loss: 0.393531, adv: 0.006996, recon: 0.017222, id: 0.008052] time: 7:55:44.122471 \n",
      "[Epoch 1/1] [Batch 45601/56436] [D loss: 0.507897, acc:  49%] [G loss: 0.297690, adv: 0.006708, recon: 0.012933, id: 0.007054] time: 7:56:15.643419 \n",
      "[Epoch 1/1] [Batch 45651/56436] [D loss: 0.513304, acc:  48%] [G loss: 0.537274, adv: 0.006294, recon: 0.024552, id: 0.010764] time: 7:56:47.053971 \n",
      "[Epoch 1/1] [Batch 45701/56436] [D loss: 0.446095, acc:  44%] [G loss: 0.665661, adv: 0.029271, recon: 0.027836, id: 0.017030] time: 7:57:18.238741 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 45751/56436] [D loss: 0.435126, acc:  49%] [G loss: 0.387901, adv: 0.015491, recon: 0.016395, id: 0.012317] time: 7:57:48.888814 \n",
      "[Epoch 1/1] [Batch 45801/56436] [D loss: 0.466233, acc:  49%] [G loss: 0.388392, adv: 0.012443, recon: 0.016595, id: 0.014429] time: 7:58:20.096200 \n",
      "[Epoch 1/1] [Batch 45851/56436] [D loss: 0.557094, acc:  48%] [G loss: 0.430602, adv: 0.011026, recon: 0.016162, id: 0.020569] time: 7:58:51.288906 \n",
      "[Epoch 1/1] [Batch 45901/56436] [D loss: 0.559986, acc:  49%] [G loss: 0.330505, adv: 0.008276, recon: 0.014102, id: 0.010920] time: 7:59:22.474762 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 45951/56436] [D loss: 0.576497, acc:  48%] [G loss: 0.200833, adv: 0.010296, recon: 0.008287, id: 0.002948] time: 7:59:53.005798 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 46001/56436] [D loss: 0.431735, acc:  50%] [G loss: 0.218122, adv: 0.026186, recon: 0.007430, id: 0.010130] time: 8:00:23.583138 \n",
      "[Epoch 1/1] [Batch 46051/56436] [D loss: 0.575719, acc:  48%] [G loss: 0.469328, adv: 0.008893, recon: 0.020035, id: 0.024886] time: 8:00:54.625937 \n",
      "[Epoch 1/1] [Batch 46101/56436] [D loss: 0.468771, acc:  49%] [G loss: 0.241387, adv: 0.012225, recon: 0.009563, id: 0.005789] time: 8:01:25.806591 \n",
      "[Epoch 1/1] [Batch 46151/56436] [D loss: 0.461476, acc:  48%] [G loss: 0.275458, adv: 0.009079, recon: 0.011799, id: 0.002840] time: 8:01:57.083887 \n",
      "[Epoch 1/1] [Batch 46201/56436] [D loss: 0.537787, acc:  49%] [G loss: 0.435109, adv: 0.012036, recon: 0.018825, id: 0.009232] time: 8:02:28.136652 \n",
      "[Epoch 1/1] [Batch 46251/56436] [D loss: 0.514034, acc:  48%] [G loss: 0.244039, adv: 0.005055, recon: 0.010619, id: 0.002824] time: 8:02:59.779332 \n",
      "[Epoch 1/1] [Batch 46301/56436] [D loss: 0.491391, acc:  48%] [G loss: 0.950882, adv: 0.010261, recon: 0.041757, id: 0.016493] time: 8:03:31.201068 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 46351/56436] [D loss: 0.496061, acc:  49%] [G loss: 0.631990, adv: 0.011609, recon: 0.027628, id: 0.012307] time: 8:04:01.875967 \n",
      "[Epoch 1/1] [Batch 46401/56436] [D loss: 0.415873, acc:  49%] [G loss: 0.303634, adv: 0.015308, recon: 0.012436, id: 0.008320] time: 8:04:33.087200 \n",
      "[Epoch 1/1] [Batch 46451/56436] [D loss: 0.515888, acc:  45%] [G loss: 0.240505, adv: 0.010486, recon: 0.009878, id: 0.008444] time: 8:05:04.283411 \n",
      "[Epoch 1/1] [Batch 46501/56436] [D loss: 0.398711, acc:  49%] [G loss: 0.528570, adv: 0.025596, recon: 0.021835, id: 0.006042] time: 8:05:35.450693 \n",
      "[Epoch 1/1] [Batch 46551/56436] [D loss: 0.479911, acc:  49%] [G loss: 0.406058, adv: 0.012616, recon: 0.016987, id: 0.015154] time: 8:06:06.619270 \n",
      "[Epoch 1/1] [Batch 46601/56436] [D loss: 0.511700, acc:  50%] [G loss: 0.283720, adv: 0.007674, recon: 0.010576, id: 0.014749] time: 8:06:37.779953 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 46651/56436] [D loss: 0.485296, acc:  49%] [G loss: 0.456507, adv: 0.009687, recon: 0.019658, id: 0.004593] time: 8:07:08.354520 \n",
      "[Epoch 1/1] [Batch 46701/56436] [D loss: 0.457793, acc:  49%] [G loss: 0.409944, adv: 0.009416, recon: 0.017705, id: 0.015060] time: 8:07:39.337129 \n",
      "[Epoch 1/1] [Batch 46751/56436] [D loss: 0.596490, acc:  26%] [G loss: 0.461022, adv: 0.021480, recon: 0.015469, id: 0.076182] time: 8:08:10.387346 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 46801/56436] [D loss: 0.472917, acc:  48%] [G loss: 0.384372, adv: 0.010470, recon: 0.016953, id: 0.005089] time: 8:08:40.926156 \n",
      "[Epoch 1/1] [Batch 46851/56436] [D loss: 0.617641, acc:  25%] [G loss: 0.437094, adv: 0.012421, recon: 0.017909, id: 0.015811] time: 8:09:11.869350 \n",
      "[Epoch 1/1] [Batch 46901/56436] [D loss: 0.521879, acc:  49%] [G loss: 0.295253, adv: 0.007072, recon: 0.012120, id: 0.006185] time: 8:09:42.895621 \n",
      "[Epoch 1/1] [Batch 46951/56436] [D loss: 0.515458, acc:  48%] [G loss: 0.238876, adv: 0.006634, recon: 0.010286, id: 0.004203] time: 8:10:14.429196 \n",
      "[Epoch 1/1] [Batch 47001/56436] [D loss: 0.498270, acc:  49%] [G loss: 0.560083, adv: 0.010807, recon: 0.023669, id: 0.007056] time: 8:10:45.748031 \n",
      "[Epoch 1/1] [Batch 47051/56436] [D loss: 0.506184, acc:  49%] [G loss: 0.343116, adv: 0.007517, recon: 0.014927, id: 0.006181] time: 8:11:16.986202 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 47101/56436] [D loss: 0.566981, acc:  48%] [G loss: 0.298686, adv: 0.006814, recon: 0.012926, id: 0.000735] time: 8:11:47.560725 \n",
      "[Epoch 1/1] [Batch 47151/56436] [D loss: 0.496126, acc:  32%] [G loss: 0.555661, adv: 0.012821, recon: 0.024243, id: 0.018182] time: 8:12:18.572618 \n",
      "[Epoch 1/1] [Batch 47201/56436] [D loss: 0.492500, acc:  48%] [G loss: 0.557508, adv: 0.006428, recon: 0.024649, id: 0.008792] time: 8:12:49.649382 \n",
      "[Epoch 1/1] [Batch 47251/56436] [D loss: 0.490058, acc:  49%] [G loss: 0.466925, adv: 0.008743, recon: 0.020465, id: 0.006678] time: 8:13:20.705051 \n",
      "[Epoch 1/1] [Batch 47301/56436] [D loss: 0.548705, acc:  49%] [G loss: 0.428959, adv: 0.014849, recon: 0.017993, id: 0.013664] time: 8:13:51.695029 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 47351/56436] [D loss: 0.440705, acc:  49%] [G loss: 0.365917, adv: 0.011046, recon: 0.015770, id: 0.007166] time: 8:14:22.128975 \n",
      "[Epoch 1/1] [Batch 47401/56436] [D loss: 0.490890, acc:  49%] [G loss: 0.335012, adv: 0.005821, recon: 0.014734, id: 0.011249] time: 8:14:53.261402 \n",
      "[Epoch 1/1] [Batch 47451/56436] [D loss: 0.530558, acc:  49%] [G loss: 0.194345, adv: 0.004997, recon: 0.008236, id: 0.005755] time: 8:15:24.380747 \n",
      "[Epoch 1/1] [Batch 47501/56436] [D loss: 0.709975, acc:  25%] [G loss: 0.565331, adv: 0.012988, recon: 0.024460, id: 0.013285] time: 8:15:55.356018 \n",
      "[Epoch 1/1] [Batch 47551/56436] [D loss: 0.483759, acc:  48%] [G loss: 0.340745, adv: 0.007167, recon: 0.014846, id: 0.004017] time: 8:16:26.377403 \n",
      "[Epoch 1/1] [Batch 47601/56436] [D loss: 0.513576, acc:  49%] [G loss: 0.561956, adv: 0.011781, recon: 0.024407, id: 0.013995] time: 8:16:57.340071 \n",
      "[Epoch 1/1] [Batch 47651/56436] [D loss: 0.557483, acc:  49%] [G loss: 0.219830, adv: 0.015370, recon: 0.008394, id: 0.003942] time: 8:17:28.198643 \n",
      "[Epoch 1/1] [Batch 47701/56436] [D loss: 0.756759, acc:  25%] [G loss: 0.649332, adv: 0.012360, recon: 0.029160, id: 0.019984] time: 8:17:59.216031 \n",
      "[Epoch 1/1] [Batch 47751/56436] [D loss: 0.482658, acc:  49%] [G loss: 0.383563, adv: 0.008459, recon: 0.016667, id: 0.013467] time: 8:18:30.749799 \n",
      "[Epoch 1/1] [Batch 47801/56436] [D loss: 0.532601, acc:  40%] [G loss: 0.284502, adv: 0.009575, recon: 0.012162, id: 0.008320] time: 8:19:02.048679 \n",
      "[Epoch 1/1] [Batch 47851/56436] [D loss: 0.468250, acc:  49%] [G loss: 0.426737, adv: 0.010312, recon: 0.017897, id: 0.025557] time: 8:19:33.286041 \n",
      "[Epoch 1/1] [Batch 47901/56436] [D loss: 0.524254, acc:  49%] [G loss: 0.313915, adv: 0.006728, recon: 0.013075, id: 0.008350] time: 8:20:04.378725 \n",
      "[Epoch 1/1] [Batch 47951/56436] [D loss: 0.480403, acc:  48%] [G loss: 0.178889, adv: 0.006963, recon: 0.007635, id: 0.002825] time: 8:20:35.480162 \n",
      "[Epoch 1/1] [Batch 48001/56436] [D loss: 0.519273, acc:  48%] [G loss: 0.568820, adv: 0.006426, recon: 0.025383, id: 0.010451] time: 8:21:06.545249 \n",
      "[Epoch 1/1] [Batch 48051/56436] [D loss: 0.571283, acc:  50%] [G loss: 0.457379, adv: 0.009748, recon: 0.019878, id: 0.027925] time: 8:21:37.679549 \n",
      "[Epoch 1/1] [Batch 48101/56436] [D loss: 0.527329, acc:  49%] [G loss: 0.648746, adv: 0.011021, recon: 0.028643, id: 0.025759] time: 8:22:08.688792 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 48151/56436] [D loss: 0.499096, acc:  47%] [G loss: 0.207317, adv: 0.010018, recon: 0.008461, id: 0.001930] time: 8:22:39.053593 \n",
      "[Epoch 1/1] [Batch 48201/56436] [D loss: 0.524312, acc:  49%] [G loss: 0.351582, adv: 0.006754, recon: 0.015341, id: 0.007862] time: 8:23:10.309027 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 48251/56436] [D loss: 0.455275, acc:  49%] [G loss: 0.364948, adv: 0.011054, recon: 0.015587, id: 0.006991] time: 8:23:40.824866 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 48301/56436] [D loss: 0.538539, acc:  49%] [G loss: 0.229007, adv: 0.007882, recon: 0.009716, id: 0.005587] time: 8:24:11.216855 \n",
      "[Epoch 1/1] [Batch 48351/56436] [D loss: 0.517535, acc:  49%] [G loss: 0.471720, adv: 0.005776, recon: 0.020650, id: 0.006991] time: 8:24:42.262830 \n",
      "[Epoch 1/1] [Batch 48401/56436] [D loss: 0.537463, acc:  49%] [G loss: 0.197602, adv: 0.007510, recon: 0.008337, id: 0.008197] time: 8:25:13.268097 \n",
      "[Epoch 1/1] [Batch 48451/56436] [D loss: 0.515192, acc:  48%] [G loss: 0.522250, adv: 0.008101, recon: 0.022766, id: 0.023164] time: 8:25:44.246284 \n",
      "[Epoch 1/1] [Batch 48501/56436] [D loss: 0.597450, acc:  49%] [G loss: 0.355266, adv: 0.011873, recon: 0.013603, id: 0.014827] time: 8:26:15.466891 \n",
      "[Epoch 1/1] [Batch 48551/56436] [D loss: 0.568789, acc:  49%] [G loss: 0.538748, adv: 0.013565, recon: 0.023126, id: 0.012350] time: 8:26:46.961609 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 48601/56436] [D loss: 0.522645, acc:  49%] [G loss: 0.411963, adv: 0.005758, recon: 0.018218, id: 0.017071] time: 8:27:17.103875 \n",
      "[Epoch 1/1] [Batch 48651/56436] [D loss: 0.377939, acc:  49%] [G loss: 0.694291, adv: 0.055544, recon: 0.026461, id: 0.004578] time: 8:27:48.189226 \n",
      "[Epoch 1/1] [Batch 48701/56436] [D loss: 0.491726, acc:  49%] [G loss: 0.301875, adv: 0.005167, recon: 0.012569, id: 0.008239] time: 8:28:19.267840 \n",
      "[Epoch 1/1] [Batch 48751/56436] [D loss: 0.639176, acc:  26%] [G loss: 0.410251, adv: 0.007052, recon: 0.017959, id: 0.015986] time: 8:28:50.374050 \n",
      "[Epoch 1/1] [Batch 48801/56436] [D loss: 0.408716, acc:  48%] [G loss: 0.566535, adv: 0.030134, recon: 0.022846, id: 0.015865] time: 8:29:21.339943 \n",
      "[Epoch 1/1] [Batch 48851/56436] [D loss: 0.573655, acc:  39%] [G loss: 0.259820, adv: 0.004667, recon: 0.011033, id: 0.009185] time: 8:29:52.493531 \n",
      "[Epoch 1/1] [Batch 48901/56436] [D loss: 0.633205, acc:  45%] [G loss: 0.370901, adv: 0.012474, recon: 0.015550, id: 0.019658] time: 8:30:23.482442 \n",
      "[Epoch 1/1] [Batch 48951/56436] [D loss: 0.590029, acc:  25%] [G loss: 0.576328, adv: 0.025503, recon: 0.023913, id: 0.018054] time: 8:30:54.556430 \n",
      "[Epoch 1/1] [Batch 49001/56436] [D loss: 0.532999, acc:  49%] [G loss: 0.311612, adv: 0.006409, recon: 0.013748, id: 0.006447] time: 8:31:25.470669 \n",
      "[Epoch 1/1] [Batch 49051/56436] [D loss: 0.475402, acc:  49%] [G loss: 0.577883, adv: 0.009146, recon: 0.025712, id: 0.012337] time: 8:31:56.356488 \n",
      "[Epoch 1/1] [Batch 49101/56436] [D loss: 0.461243, acc:  49%] [G loss: 0.541883, adv: 0.010097, recon: 0.023909, id: 0.008734] time: 8:32:27.220266 \n",
      "[Epoch 1/1] [Batch 49151/56436] [D loss: 0.512067, acc:  49%] [G loss: 0.263266, adv: 0.006381, recon: 0.010662, id: 0.002465] time: 8:32:59.039837 \n",
      "[Epoch 1/1] [Batch 49201/56436] [D loss: 0.483697, acc:  50%] [G loss: 0.383599, adv: 0.007319, recon: 0.016820, id: 0.013057] time: 8:33:30.382968 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 49251/56436] [D loss: 0.560380, acc:  49%] [G loss: 0.701489, adv: 0.023177, recon: 0.030311, id: 0.012138] time: 8:34:01.203042 \n",
      "[Epoch 1/1] [Batch 49301/56436] [D loss: 0.482545, acc:  49%] [G loss: 0.389062, adv: 0.006766, recon: 0.017274, id: 0.007541] time: 8:34:32.442542 \n",
      "[Epoch 1/1] [Batch 49351/56436] [D loss: 0.505401, acc:  49%] [G loss: 0.308052, adv: 0.007149, recon: 0.013443, id: 0.019538] time: 8:35:03.690375 \n",
      "[Epoch 1/1] [Batch 49401/56436] [D loss: 0.517258, acc:  48%] [G loss: 0.638906, adv: 0.005511, recon: 0.028644, id: 0.022053] time: 8:35:34.853268 \n",
      "[Epoch 1/1] [Batch 49451/56436] [D loss: 0.441418, acc:  49%] [G loss: 0.378474, adv: 0.011511, recon: 0.016086, id: 0.005231] time: 8:36:06.023854 \n",
      "[Epoch 1/1] [Batch 49501/56436] [D loss: 0.482747, acc:  50%] [G loss: 0.275207, adv: 0.006657, recon: 0.011991, id: 0.008723] time: 8:36:37.094425 \n",
      "[Epoch 1/1] [Batch 49551/56436] [D loss: 0.476991, acc:  49%] [G loss: 0.527232, adv: 0.008731, recon: 0.023350, id: 0.013093] time: 8:37:08.177747 \n",
      "[Epoch 1/1] [Batch 49601/56436] [D loss: 0.492490, acc:  48%] [G loss: 0.375457, adv: 0.009850, recon: 0.016039, id: 0.009027] time: 8:37:39.343269 \n",
      "[Epoch 1/1] [Batch 49651/56436] [D loss: 0.428229, acc:  49%] [G loss: 0.338583, adv: 0.014844, recon: 0.013832, id: 0.014063] time: 8:38:10.262968 \n",
      "[Epoch 1/1] [Batch 49701/56436] [D loss: 0.563742, acc:  50%] [G loss: 0.557868, adv: 0.011429, recon: 0.024493, id: 0.016155] time: 8:38:41.149654 \n",
      "[Epoch 1/1] [Batch 49751/56436] [D loss: 0.583840, acc:  48%] [G loss: 0.367069, adv: 0.009961, recon: 0.015664, id: 0.004660] time: 8:39:12.371296 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 49801/56436] [D loss: 0.514188, acc:  49%] [G loss: 0.301601, adv: 0.006241, recon: 0.013097, id: 0.006299] time: 8:39:42.790209 \n",
      "[Epoch 1/1] [Batch 49851/56436] [D loss: 0.521737, acc:  49%] [G loss: 0.464424, adv: 0.009910, recon: 0.019798, id: 0.014097] time: 8:40:14.420068 \n",
      "[Epoch 1/1] [Batch 49901/56436] [D loss: 0.493916, acc:  50%] [G loss: 0.213821, adv: 0.007260, recon: 0.009064, id: 0.006166] time: 8:40:45.760583 \n",
      "[Epoch 1/1] [Batch 49951/56436] [D loss: 0.561341, acc:  49%] [G loss: 0.316467, adv: 0.010284, recon: 0.012216, id: 0.009565] time: 8:41:17.013704 \n",
      "[Epoch 1/1] [Batch 50001/56436] [D loss: 0.478441, acc:  50%] [G loss: 0.286180, adv: 0.006610, recon: 0.012044, id: 0.017602] time: 8:41:48.247604 \n",
      "[Epoch 1/1] [Batch 50051/56436] [D loss: 0.550723, acc:  33%] [G loss: 0.357331, adv: 0.006063, recon: 0.015716, id: 0.008433] time: 8:42:24.526236 \n",
      "[Epoch 1/1] [Batch 50101/56436] [D loss: 0.544893, acc:  49%] [G loss: 0.418522, adv: 0.010513, recon: 0.018604, id: 0.016998] time: 8:42:55.826721 \n",
      "[Epoch 1/1] [Batch 50151/56436] [D loss: 0.474059, acc:  49%] [G loss: 0.300784, adv: 0.011333, recon: 0.011553, id: 0.008242] time: 8:43:26.939420 \n",
      "[Epoch 1/1] [Batch 50201/56436] [D loss: 0.486891, acc:  49%] [G loss: 0.315957, adv: 0.005512, recon: 0.013585, id: 0.006699] time: 8:43:58.020247 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 50251/56436] [D loss: 0.526956, acc:  48%] [G loss: 0.415212, adv: 0.003907, recon: 0.018683, id: 0.003751] time: 8:44:27.887743 \n",
      "[Epoch 1/1] [Batch 50301/56436] [D loss: 0.524247, acc:  47%] [G loss: 0.153655, adv: 0.006419, recon: 0.006321, id: 0.007074] time: 8:44:58.874051 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 50351/56436] [D loss: 0.490360, acc:  49%] [G loss: 0.481615, adv: 0.007022, recon: 0.021161, id: 0.007012] time: 8:45:29.261947 \n",
      "[Epoch 1/1] [Batch 50401/56436] [D loss: 0.585129, acc:  30%] [G loss: 0.576045, adv: 0.011101, recon: 0.025703, id: 0.018229] time: 8:46:00.140765 \n",
      "[Epoch 1/1] [Batch 50451/56436] [D loss: 0.553683, acc:  48%] [G loss: 0.231689, adv: 0.007093, recon: 0.009825, id: 0.004004] time: 8:46:31.149176 \n",
      "[Epoch 1/1] [Batch 50501/56436] [D loss: 0.593056, acc:  48%] [G loss: 0.318671, adv: 0.013692, recon: 0.013198, id: 0.009653] time: 8:47:02.158560 \n",
      "[Epoch 1/1] [Batch 50551/56436] [D loss: 0.479539, acc:  45%] [G loss: 0.257253, adv: 0.011896, recon: 0.010698, id: 0.009628] time: 8:47:33.470225 \n",
      "[Epoch 1/1] [Batch 50601/56436] [D loss: 0.440977, acc:  47%] [G loss: 0.203605, adv: 0.015436, recon: 0.007696, id: 0.012427] time: 8:48:04.830486 \n",
      "[Epoch 1/1] [Batch 50651/56436] [D loss: 0.515451, acc:  48%] [G loss: 0.137198, adv: 0.004171, recon: 0.005835, id: 0.003417] time: 8:48:36.184762 \n",
      "[Epoch 1/1] [Batch 50701/56436] [D loss: 0.478327, acc:  49%] [G loss: 0.594674, adv: 0.007582, recon: 0.026164, id: 0.015796] time: 8:49:07.464393 \n",
      "[Epoch 1/1] [Batch 50751/56436] [D loss: 0.489753, acc:  49%] [G loss: 0.640924, adv: 0.010309, recon: 0.028902, id: 0.010440] time: 8:49:38.575573 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 50801/56436] [D loss: 0.446103, acc:  49%] [G loss: 0.478303, adv: 0.014690, recon: 0.019877, id: 0.018052] time: 8:50:08.689043 \n",
      "[Epoch 1/1] [Batch 50851/56436] [D loss: 0.492738, acc:  30%] [G loss: 0.657182, adv: 0.018511, recon: 0.027974, id: 0.038328] time: 8:50:39.856216 \n",
      "[Epoch 1/1] [Batch 50901/56436] [D loss: 0.511189, acc:  49%] [G loss: 0.297729, adv: 0.008509, recon: 0.012871, id: 0.005739] time: 8:51:10.842464 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 50951/56436] [D loss: 0.430793, acc:  46%] [G loss: 0.486874, adv: 0.027837, recon: 0.018983, id: 0.034529] time: 8:51:41.218371 \n",
      "[Epoch 1/1] [Batch 51001/56436] [D loss: 0.558178, acc:  47%] [G loss: 0.541913, adv: 0.008528, recon: 0.024123, id: 0.015087] time: 8:52:12.286597 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51051/56436] [D loss: 0.509749, acc:  49%] [G loss: 0.434793, adv: 0.004699, recon: 0.019420, id: 0.013985] time: 8:52:42.796085 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51101/56436] [D loss: 0.503006, acc:  50%] [G loss: 0.388120, adv: 0.007445, recon: 0.016996, id: 0.006375] time: 8:53:12.714872 \n",
      "[Epoch 1/1] [Batch 51151/56436] [D loss: 0.472866, acc:  48%] [G loss: 0.475577, adv: 0.011067, recon: 0.020852, id: 0.025061] time: 8:53:43.729453 \n",
      "[Epoch 1/1] [Batch 51201/56436] [D loss: 0.453079, acc:  49%] [G loss: 0.332993, adv: 0.012068, recon: 0.014002, id: 0.007640] time: 8:54:14.780324 \n",
      "[Epoch 1/1] [Batch 51251/56436] [D loss: 0.564948, acc:  49%] [G loss: 0.522889, adv: 0.011514, recon: 0.022259, id: 0.007717] time: 8:54:45.759359 \n",
      "[Epoch 1/1] [Batch 51301/56436] [D loss: 0.557671, acc:  25%] [G loss: 0.223175, adv: 0.011779, recon: 0.008975, id: 0.011817] time: 8:55:16.973300 \n",
      "[Epoch 1/1] [Batch 51351/56436] [D loss: 0.522286, acc:  48%] [G loss: 0.156601, adv: 0.007613, recon: 0.006334, id: 0.003544] time: 8:55:48.194564 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51401/56436] [D loss: 0.642252, acc:  29%] [G loss: 0.438650, adv: 0.007074, recon: 0.019352, id: 0.013587] time: 8:56:18.768710 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51451/56436] [D loss: 0.502537, acc:  49%] [G loss: 0.192897, adv: 0.004745, recon: 0.008280, id: 0.008253] time: 8:56:49.707601 \n",
      "[Epoch 1/1] [Batch 51501/56436] [D loss: 0.455298, acc:  49%] [G loss: 0.450848, adv: 0.009751, recon: 0.019632, id: 0.009860] time: 8:57:20.807451 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51551/56436] [D loss: 0.558162, acc:  48%] [G loss: 0.271012, adv: 0.009464, recon: 0.011212, id: 0.003398] time: 8:57:51.291843 \n",
      "[Epoch 1/1] [Batch 51601/56436] [D loss: 0.457585, acc:  48%] [G loss: 0.373940, adv: 0.012423, recon: 0.015946, id: 0.010728] time: 8:58:22.493996 \n",
      "[Epoch 1/1] [Batch 51651/56436] [D loss: 0.465561, acc:  49%] [G loss: 0.311751, adv: 0.010507, recon: 0.013462, id: 0.009938] time: 8:58:53.610599 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 51701/56436] [D loss: 0.537202, acc:  50%] [G loss: 0.491149, adv: 0.007921, recon: 0.020439, id: 0.024134] time: 8:59:23.967880 \n",
      "[Epoch 1/1] [Batch 51751/56436] [D loss: 0.515715, acc:  50%] [G loss: 0.504935, adv: 0.008244, recon: 0.022047, id: 0.015072] time: 8:59:55.260243 \n",
      "[Epoch 1/1] [Batch 51801/56436] [D loss: 0.603121, acc:  25%] [G loss: 0.458880, adv: 0.008181, recon: 0.020413, id: 0.016504] time: 9:00:26.321367 \n",
      "[Epoch 1/1] [Batch 51851/56436] [D loss: 0.549400, acc:  48%] [G loss: 0.513331, adv: 0.014138, recon: 0.022067, id: 0.009189] time: 9:00:57.337222 \n",
      "[Epoch 1/1] [Batch 51901/56436] [D loss: 0.540330, acc:  48%] [G loss: 0.460716, adv: 0.005718, recon: 0.020274, id: 0.024796] time: 9:01:28.377691 \n",
      "[Epoch 1/1] [Batch 51951/56436] [D loss: 0.442691, acc:  49%] [G loss: 0.319607, adv: 0.018116, recon: 0.012949, id: 0.005868] time: 9:01:59.601101 \n",
      "[Epoch 1/1] [Batch 52001/56436] [D loss: 0.443790, acc:  49%] [G loss: 0.196030, adv: 0.010048, recon: 0.007798, id: 0.008381] time: 9:02:30.592468 \n",
      "[Epoch 1/1] [Batch 52051/56436] [D loss: 0.527487, acc:  49%] [G loss: 0.439074, adv: 0.007256, recon: 0.019365, id: 0.013003] time: 9:03:01.632957 \n",
      "[Epoch 1/1] [Batch 52101/56436] [D loss: 0.527448, acc:  49%] [G loss: 0.271028, adv: 0.008205, recon: 0.011298, id: 0.013627] time: 9:03:32.600234 \n",
      "[Epoch 1/1] [Batch 52151/56436] [D loss: 0.460622, acc:  49%] [G loss: 0.298183, adv: 0.011690, recon: 0.012979, id: 0.009769] time: 9:04:03.713820 \n",
      "[Epoch 1/1] [Batch 52201/56436] [D loss: 0.514919, acc:  48%] [G loss: 0.282743, adv: 0.009008, recon: 0.012069, id: 0.004298] time: 9:04:34.766635 \n",
      "[Epoch 1/1] [Batch 52251/56436] [D loss: 0.477420, acc:  48%] [G loss: 0.598720, adv: 0.008092, recon: 0.026395, id: 0.024423] time: 9:05:05.920306 \n",
      "[Epoch 1/1] [Batch 52301/56436] [D loss: 0.520576, acc:  48%] [G loss: 0.371684, adv: 0.005689, recon: 0.016368, id: 0.003354] time: 9:05:37.373732 \n",
      "[Epoch 1/1] [Batch 52351/56436] [D loss: 0.496961, acc:  48%] [G loss: 0.271271, adv: 0.007611, recon: 0.011726, id: 0.006238] time: 9:06:08.714887 \n",
      "[Epoch 1/1] [Batch 52401/56436] [D loss: 0.519627, acc:  49%] [G loss: 0.291855, adv: 0.006328, recon: 0.012552, id: 0.006394] time: 9:06:39.962575 \n",
      "[Epoch 1/1] [Batch 52451/56436] [D loss: 0.569262, acc:  49%] [G loss: 0.330083, adv: 0.011770, recon: 0.013685, id: 0.007858] time: 9:07:11.302399 \n",
      "[Epoch 1/1] [Batch 52501/56436] [D loss: 0.489334, acc:  47%] [G loss: 0.546568, adv: 0.010298, recon: 0.023857, id: 0.011791] time: 9:07:42.563367 \n",
      "[Epoch 1/1] [Batch 52551/56436] [D loss: 0.533139, acc:  49%] [G loss: 0.200034, adv: 0.009443, recon: 0.008218, id: 0.010264] time: 9:08:13.702442 \n",
      "[Epoch 1/1] [Batch 52601/56436] [D loss: 0.471369, acc:  49%] [G loss: 0.171336, adv: 0.006454, recon: 0.007143, id: 0.006307] time: 9:08:44.850759 \n",
      "[Epoch 1/1] [Batch 52651/56436] [D loss: 0.512080, acc:  48%] [G loss: 0.379130, adv: 0.012738, recon: 0.016053, id: 0.016694] time: 9:09:15.979380 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 52701/56436] [D loss: 0.508936, acc:  48%] [G loss: 0.371430, adv: 0.006512, recon: 0.016378, id: 0.004402] time: 9:09:46.459167 \n",
      "[Epoch 1/1] [Batch 52751/56436] [D loss: 0.654614, acc:  31%] [G loss: 0.450698, adv: 0.011003, recon: 0.019302, id: 0.018274] time: 9:10:17.549526 \n",
      "[Epoch 1/1] [Batch 52801/56436] [D loss: 0.515047, acc:  48%] [G loss: 0.232949, adv: 0.006783, recon: 0.009958, id: 0.010873] time: 9:10:48.550663 \n",
      "[Epoch 1/1] [Batch 52851/56436] [D loss: 0.536682, acc:  49%] [G loss: 0.386142, adv: 0.008098, recon: 0.016757, id: 0.010148] time: 9:11:19.659287 \n",
      "[Epoch 1/1] [Batch 52901/56436] [D loss: 0.432961, acc:  49%] [G loss: 0.217387, adv: 0.014150, recon: 0.008505, id: 0.010375] time: 9:11:50.709685 \n",
      "[Epoch 1/1] [Batch 52951/56436] [D loss: 0.652787, acc:  25%] [G loss: 0.282739, adv: 0.010994, recon: 0.011129, id: 0.004611] time: 9:12:21.707153 \n",
      "[Epoch 1/1] [Batch 53001/56436] [D loss: 0.468993, acc:  49%] [G loss: 0.198659, adv: 0.009543, recon: 0.008212, id: 0.010207] time: 9:12:53.066436 \n",
      "[Epoch 1/1] [Batch 53051/56436] [D loss: 0.401052, acc:  49%] [G loss: 0.244170, adv: 0.035161, recon: 0.008333, id: 0.002907] time: 9:13:24.189759 \n",
      "[Epoch 1/1] [Batch 53101/56436] [D loss: 0.551767, acc:  49%] [G loss: 0.440999, adv: 0.008604, recon: 0.019217, id: 0.017642] time: 9:13:55.463607 \n",
      "[Epoch 1/1] [Batch 53151/56436] [D loss: 0.472409, acc:  48%] [G loss: 0.310367, adv: 0.006412, recon: 0.013456, id: 0.006341] time: 9:14:26.588970 \n",
      "[Epoch 1/1] [Batch 53201/56436] [D loss: 0.516218, acc:  49%] [G loss: 0.458237, adv: 0.006790, recon: 0.020278, id: 0.010288] time: 9:14:57.642523 \n",
      "[Epoch 1/1] [Batch 53251/56436] [D loss: 0.510518, acc:  49%] [G loss: 0.435829, adv: 0.008105, recon: 0.018966, id: 0.019404] time: 9:15:28.631835 \n",
      "[Epoch 1/1] [Batch 53301/56436] [D loss: 0.512872, acc:  49%] [G loss: 0.327183, adv: 0.009722, recon: 0.014058, id: 0.010687] time: 9:15:59.647222 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 53351/56436] [D loss: 0.514856, acc:  47%] [G loss: 0.231353, adv: 0.008237, recon: 0.009407, id: 0.008625] time: 9:16:30.010739 \n",
      "[Epoch 1/1] [Batch 53401/56436] [D loss: 0.482200, acc:  49%] [G loss: 0.555997, adv: 0.006445, recon: 0.024820, id: 0.006889] time: 9:17:00.879796 \n",
      "[Epoch 1/1] [Batch 53451/56436] [D loss: 0.520817, acc:  49%] [G loss: 0.124643, adv: 0.004780, recon: 0.005180, id: 0.005478] time: 9:17:31.950201 \n",
      "[Epoch 1/1] [Batch 53501/56436] [D loss: 0.499493, acc:  49%] [G loss: 0.376547, adv: 0.007756, recon: 0.016416, id: 0.008156] time: 9:18:03.021784 \n",
      "[Epoch 1/1] [Batch 53551/56436] [D loss: 0.536058, acc:  49%] [G loss: 0.152776, adv: 0.007377, recon: 0.006195, id: 0.005918] time: 9:18:33.962367 \n",
      "[Epoch 1/1] [Batch 53601/56436] [D loss: 0.473554, acc:  49%] [G loss: 0.269697, adv: 0.009876, recon: 0.010734, id: 0.011757] time: 9:19:04.737521 \n",
      "[Epoch 1/1] [Batch 53651/56436] [D loss: 0.536190, acc:  48%] [G loss: 0.536264, adv: 0.005306, recon: 0.024384, id: 0.004748] time: 9:19:36.362246 \n",
      "[Epoch 1/1] [Batch 53701/56436] [D loss: 0.571534, acc:  46%] [G loss: 0.512409, adv: 0.014387, recon: 0.022527, id: 0.014885] time: 9:20:07.766937 \n",
      "[Epoch 1/1] [Batch 53751/56436] [D loss: 0.505916, acc:  34%] [G loss: 0.372785, adv: 0.014060, recon: 0.015329, id: 0.019756] time: 9:20:38.859218 \n",
      "[Epoch 1/1] [Batch 53801/56436] [D loss: 0.473427, acc:  49%] [G loss: 0.436017, adv: 0.009780, recon: 0.019047, id: 0.007285] time: 9:21:09.868437 \n",
      "[Epoch 1/1] [Batch 53851/56436] [D loss: 0.507911, acc:  48%] [G loss: 0.212945, adv: 0.004858, recon: 0.009203, id: 0.008696] time: 9:21:40.955963 \n",
      "[Epoch 1/1] [Batch 53901/56436] [D loss: 0.542643, acc:  49%] [G loss: 0.527751, adv: 0.015214, recon: 0.022762, id: 0.008280] time: 9:22:12.135076 \n",
      "[Epoch 1/1] [Batch 53951/56436] [D loss: 0.418990, acc:  44%] [G loss: 0.667737, adv: 0.037103, recon: 0.027228, id: 0.010008] time: 9:22:43.101176 \n",
      "[Epoch 1/1] [Batch 54001/56436] [D loss: 0.505437, acc:  49%] [G loss: 0.161426, adv: 0.009726, recon: 0.006417, id: 0.007368] time: 9:23:14.089042 \n",
      "[Epoch 1/1] [Batch 54051/56436] [D loss: 0.488273, acc:  49%] [G loss: 0.210084, adv: 0.005919, recon: 0.009064, id: 0.010252] time: 9:23:45.078644 \n",
      "[Epoch 1/1] [Batch 54101/56436] [D loss: 0.506763, acc:  49%] [G loss: 0.456137, adv: 0.007336, recon: 0.020256, id: 0.009376] time: 9:24:16.100747 \n",
      "[Epoch 1/1] [Batch 54151/56436] [D loss: 0.512464, acc:  49%] [G loss: 0.580509, adv: 0.006512, recon: 0.026059, id: 0.018101] time: 9:24:47.087007 \n",
      "[Epoch 1/1] [Batch 54201/56436] [D loss: 0.468518, acc:  49%] [G loss: 0.281575, adv: 0.010619, recon: 0.011950, id: 0.014227] time: 9:25:18.049092 \n",
      "[Epoch 1/1] [Batch 54251/56436] [D loss: 0.521021, acc:  47%] [G loss: 0.227754, adv: 0.012438, recon: 0.009367, id: 0.009134] time: 9:25:49.317268 \n",
      "[Epoch 1/1] [Batch 54301/56436] [D loss: 0.508843, acc:  49%] [G loss: 0.146606, adv: 0.005589, recon: 0.006136, id: 0.005624] time: 9:26:20.711830 \n",
      "[Epoch 1/1] [Batch 54351/56436] [D loss: 0.505624, acc:  49%] [G loss: 0.278868, adv: 0.004736, recon: 0.012146, id: 0.005734] time: 9:26:51.962422 \n",
      "[Epoch 1/1] [Batch 54401/56436] [D loss: 0.458421, acc:  49%] [G loss: 0.515379, adv: 0.026772, recon: 0.020588, id: 0.029497] time: 9:27:23.129352 \n",
      "[Epoch 1/1] [Batch 54451/56436] [D loss: 0.540976, acc:  49%] [G loss: 0.230782, adv: 0.008405, recon: 0.009442, id: 0.011529] time: 9:27:54.190515 \n",
      "[Epoch 1/1] [Batch 54501/56436] [D loss: 0.492566, acc:  49%] [G loss: 0.275493, adv: 0.006042, recon: 0.011350, id: 0.003723] time: 9:28:25.301185 \n",
      "[Epoch 1/1] [Batch 54551/56436] [D loss: 0.552105, acc:  48%] [G loss: 0.251199, adv: 0.006682, recon: 0.011116, id: 0.002471] time: 9:28:56.364573 \n",
      "[Epoch 1/1] [Batch 54601/56436] [D loss: 0.502617, acc:  48%] [G loss: 0.228543, adv: 0.005142, recon: 0.009935, id: 0.002841] time: 9:29:27.366059 \n",
      "[Epoch 1/1] [Batch 54651/56436] [D loss: 0.472861, acc:  49%] [G loss: 0.364155, adv: 0.009968, recon: 0.015858, id: 0.005733] time: 9:29:58.406426 \n",
      "[Epoch 1/1] [Batch 54701/56436] [D loss: 0.423742, acc:  49%] [G loss: 0.535239, adv: 0.021674, recon: 0.022380, id: 0.019864] time: 9:30:29.492334 \n",
      "[Epoch 1/1] [Batch 54751/56436] [D loss: 0.531343, acc:  48%] [G loss: 0.235266, adv: 0.006864, recon: 0.009790, id: 0.004758] time: 9:31:00.548625 \n",
      "[Epoch 1/1] [Batch 54801/56436] [D loss: 0.518546, acc:  49%] [G loss: 0.274495, adv: 0.003942, recon: 0.011835, id: 0.004743] time: 9:31:31.546499 \n",
      "[Epoch 1/1] [Batch 54851/56436] [D loss: 0.481175, acc:  49%] [G loss: 0.341719, adv: 0.014110, recon: 0.014305, id: 0.010584] time: 9:32:02.539863 \n",
      "[Epoch 1/1] [Batch 54901/56436] [D loss: 0.470757, acc:  49%] [G loss: 0.363604, adv: 0.009966, recon: 0.016016, id: 0.013542] time: 9:32:33.537056 \n",
      "[Epoch 1/1] [Batch 54951/56436] [D loss: 0.516308, acc:  49%] [G loss: 0.443191, adv: 0.007068, recon: 0.019640, id: 0.003358] time: 9:33:04.916057 \n",
      "[Epoch 1/1] [Batch 55001/56436] [D loss: 0.465912, acc:  49%] [G loss: 0.396116, adv: 0.007931, recon: 0.016821, id: 0.002967] time: 9:33:36.250823 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 55051/56436] [D loss: 0.493068, acc:  49%] [G loss: 0.296537, adv: 0.012032, recon: 0.008650, id: 0.002686] time: 9:34:12.294107 \n",
      "[Epoch 1/1] [Batch 55101/56436] [D loss: 0.507307, acc:  49%] [G loss: 0.276725, adv: 0.006095, recon: 0.011849, id: 0.007610] time: 9:34:43.452274 \n",
      "[Epoch 1/1] [Batch 55151/56436] [D loss: 0.569754, acc:  49%] [G loss: 0.487588, adv: 0.014567, recon: 0.020631, id: 0.019414] time: 9:35:14.751315 \n",
      "[Epoch 1/1] [Batch 55201/56436] [D loss: 0.506142, acc:  49%] [G loss: 0.451118, adv: 0.010410, recon: 0.019107, id: 0.008705] time: 9:35:45.909590 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 55251/56436] [D loss: 0.409827, acc:  50%] [G loss: 0.719752, adv: 0.028210, recon: 0.030502, id: 0.017006] time: 9:36:16.365858 \n",
      "[Epoch 1/1] [Batch 55301/56436] [D loss: 0.700266, acc:  25%] [G loss: 0.173591, adv: 0.005701, recon: 0.007308, id: 0.004110] time: 9:36:47.391873 \n",
      "[Epoch 1/1] [Batch 55351/56436] [D loss: 0.473351, acc:  48%] [G loss: 0.188415, adv: 0.009740, recon: 0.007827, id: 0.002867] time: 9:37:18.437157 \n",
      "[Epoch 1/1] [Batch 55401/56436] [D loss: 0.507752, acc:  49%] [G loss: 0.194355, adv: 0.005340, recon: 0.007716, id: 0.004546] time: 9:37:49.387818 \n",
      "[Epoch 1/1] [Batch 55451/56436] [D loss: 0.468361, acc:  49%] [G loss: 0.545004, adv: 0.009610, recon: 0.021636, id: 0.020110] time: 9:38:20.420371 \n",
      "[Epoch 1/1] [Batch 55501/56436] [D loss: 0.546903, acc:  49%] [G loss: 0.261476, adv: 0.005221, recon: 0.011707, id: 0.004743] time: 9:38:51.307261 \n",
      "[Epoch 1/1] [Batch 55551/56436] [D loss: 0.526688, acc:  40%] [G loss: 0.402040, adv: 0.013212, recon: 0.016746, id: 0.011165] time: 9:39:22.178222 \n",
      "[Epoch 1/1] [Batch 55601/56436] [D loss: 0.522345, acc:  49%] [G loss: 0.212551, adv: 0.004588, recon: 0.009238, id: 0.005994] time: 9:39:53.067522 \n",
      "[Epoch 1/1] [Batch 55651/56436] [D loss: 0.551856, acc:  49%] [G loss: 0.453805, adv: 0.009092, recon: 0.018870, id: 0.011166] time: 9:40:23.900668 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 55701/56436] [D loss: 0.427969, acc:  48%] [G loss: 0.456547, adv: 0.015910, recon: 0.019310, id: 0.003634] time: 9:40:54.305471 \n",
      "[Epoch 1/1] [Batch 55751/56436] [D loss: 0.499499, acc:  49%] [G loss: 0.410279, adv: 0.007636, recon: 0.018320, id: 0.011533] time: 9:41:25.292214 \n",
      "[Epoch 1/1] [Batch 55801/56436] [D loss: 0.678479, acc:  28%] [G loss: 0.754590, adv: 0.008409, recon: 0.031163, id: 0.017076] time: 9:41:56.270654 \n",
      "[Epoch 1/1] [Batch 55851/56436] [D loss: 0.531309, acc:  48%] [G loss: 0.120012, adv: 0.006574, recon: 0.004819, id: 0.003837] time: 9:42:27.130287 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 55901/56436] [D loss: 0.458079, acc:  50%] [G loss: 0.295411, adv: 0.013789, recon: 0.011954, id: 0.009181] time: 9:42:57.506818 \n",
      "[Epoch 1/1] [Batch 55951/56436] [D loss: 0.527899, acc:  48%] [G loss: 0.421889, adv: 0.006443, recon: 0.018669, id: 0.009271] time: 9:43:29.092376 \n",
      "[Epoch 1/1] [Batch 56001/56436] [D loss: 0.458531, acc:  49%] [G loss: 0.357813, adv: 0.009130, recon: 0.015500, id: 0.006383] time: 9:44:00.295455 \n",
      "[Epoch 1/1] [Batch 56051/56436] [D loss: 0.546138, acc:  50%] [G loss: 0.634897, adv: 0.006852, recon: 0.027923, id: 0.034039] time: 9:44:31.506910 \n",
      "[Epoch 1/1] [Batch 56101/56436] [D loss: 0.709009, acc:  25%] [G loss: 0.268933, adv: 0.010369, recon: 0.011535, id: 0.000033] time: 9:45:02.661650 \n",
      "\n",
      "\n",
      "[Epoch 1/1] [Batch 56151/56436] [D loss: 0.485231, acc:  49%] [G loss: 0.233824, adv: 0.006712, recon: 0.010046, id: 0.003888] time: 9:45:33.175628 \n",
      "[Epoch 1/1] [Batch 56201/56436] [D loss: 0.479378, acc:  49%] [G loss: 0.362548, adv: 0.012819, recon: 0.015370, id: 0.003969] time: 9:46:04.223266 \n",
      "[Epoch 1/1] [Batch 56251/56436] [D loss: 0.493616, acc:  48%] [G loss: 0.493837, adv: 0.006928, recon: 0.021578, id: 0.004266] time: 9:46:35.280407 \n",
      "[Epoch 1/1] [Batch 56301/56436] [D loss: 0.634532, acc:  25%] [G loss: 0.266679, adv: 0.009594, recon: 0.011102, id: 0.003909] time: 9:47:06.301887 \n",
      "[Epoch 1/1] [Batch 56351/56436] [D loss: 0.540056, acc:  47%] [G loss: 0.179436, adv: 0.006313, recon: 0.007604, id: 0.006313] time: 9:47:37.296972 \n",
      "[Epoch 1/1] [Batch 56401/56436] [D loss: 0.473702, acc:  48%] [G loss: 0.637981, adv: 0.011308, recon: 0.028037, id: 0.017614] time: 9:48:08.304814 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gan = CycleGAN(d_A=d_A, d_B=d_B, g_AB=g_AB, g_BA=g_BA)\n",
    "history = gan.train(epochs=1,batch_size=1,sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES = glob('./trainA/*.jpg')\n",
    "TEST_IMAGES = glob('./testB/*.jpg')\n",
    "\n",
    "print(len(TRAIN_IMAGES))\n",
    "print(len(TEST_IMAGES))\n",
    "\n",
    "def load_images(path):\n",
    "    img_height = 768\n",
    "    img_width = 512\n",
    "    image_list = np.zeros((len(path), img_height, img_width, 1))\n",
    "    for i, fig in enumerate(path):\n",
    "        img = image.load_img(fig, color_mode='grayscale', target_size=(img_height, img_width))\n",
    "        x = image.img_to_array(img).astype('float32')\n",
    "        image_list[i] = x\n",
    "    \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_images(TRAIN_IMAGES)\n",
    "x_test = load_images(TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gan = gan.g_AB.predict(x_train[:5])\n",
    "x_test_gan = gan.g_AB.predict(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('original_x_train_gan', x_train_gan)\n",
    "np.save('original_x_test_gan', x_test_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.d_A.save('retrained_original_d_A.h5')\n",
    "gan.d_B.save('retrained_original_d_B.h5')\n",
    "gan.g_AB.save('retrained_original_g_AB.h5')\n",
    "gan.g_BA.save('retrained_original_g_BA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelin = load_model('generative_g_BA.h5',custom_objects={'InstanceNormalization':InstanceNormalization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10,64,769,513] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_27/conv2d_77/Conv2D (defined at <ipython-input-89-d51f299a7b60>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_11472446]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-d00ef3834d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import cv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/unb/tcc1/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[10,64,769,513] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_27/conv2d_77/Conv2D (defined at <ipython-input-89-d51f299a7b60>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_11472446]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "new_img = modelin.predict(x_test[:10])\n",
    "new_img.shape\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# new_img = cv2.threshold(new_img,1,255,cv2.THRESH_BINARY)[1]\n",
    "# ??cv2.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-4cd4ede7d6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORIGINAL IMAGE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 5"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(new_img[6], cmap='gray')\n",
    "plt.title('ORIGINAL IMAGE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5138329084729776,\n",
       " 0.5067934327526018,\n",
       " 0.5074587690178305,\n",
       " 0.5460161067312583,\n",
       " 0.5696328164776787,\n",
       " 0.5361802405677736,\n",
       " 0.5005852495087311,\n",
       " 0.5193826503818855,\n",
       " 0.4705587571952492,\n",
       " 0.7016737181693316,\n",
       " 0.5000073799164966,\n",
       " 0.5541554861702025,\n",
       " 0.5225545330904424,\n",
       " 0.5334532801061869,\n",
       " 0.4855466049630195,\n",
       " 0.523476185859181,\n",
       " 0.4790895455516875,\n",
       " 0.5135448676301166,\n",
       " 0.49287041323259473,\n",
       " 0.5163010258693248,\n",
       " 0.2798791085369885,\n",
       " 0.2917193208122626,\n",
       " 0.49002415279392153,\n",
       " 0.5250104154692963,\n",
       " 0.43845284124836326,\n",
       " 0.41908930393401533,\n",
       " 0.5973115763626993,\n",
       " 0.5573925534263253,\n",
       " 0.4855340027716011,\n",
       " 0.4883947875350714,\n",
       " 0.5471741994842887,\n",
       " 0.5175894380081445,\n",
       " 0.5285138860344887,\n",
       " 0.5301176975481212,\n",
       " 0.5957594264764339,\n",
       " 0.6123407538980246,\n",
       " 0.5074687656015158,\n",
       " 0.709308153251186,\n",
       " 0.4521897278027609,\n",
       " 0.5243933124002069,\n",
       " 0.5382968904450536,\n",
       " 0.5504087798763067,\n",
       " 0.5111310478532687,\n",
       " 0.5564606934785843,\n",
       " 0.456507800729014,\n",
       " 0.4603442082880065,\n",
       " 0.44456498301588,\n",
       " 0.5554624479264021,\n",
       " 0.49306804314255714,\n",
       " 0.6289378224173561,\n",
       " 0.5486463920678943,\n",
       " 0.45497998711653054,\n",
       " 0.515266194473952,\n",
       " 0.6645017834380269,\n",
       " 0.5706781768240035,\n",
       " 0.49763975106179714,\n",
       " 0.537704887567088,\n",
       " 0.48854592675343156,\n",
       " 0.5266032447107136,\n",
       " 0.5300059996079654,\n",
       " 0.5297186382813379,\n",
       " 0.499659338966012,\n",
       " 0.45288610737770796,\n",
       " 0.5345001781824976,\n",
       " 0.5158500623656437,\n",
       " 0.4912709570489824,\n",
       " 0.576836483553052,\n",
       " 0.5283319717273116,\n",
       " 0.4963805659208447,\n",
       " 0.5245014056563377,\n",
       " 0.5551676831673831,\n",
       " 0.36282600089907646,\n",
       " 0.4714164489414543,\n",
       " 0.5112608396448195,\n",
       " 0.5528442743234336,\n",
       " 0.5301853828132153,\n",
       " 0.516812649788335,\n",
       " 0.45860517874825746,\n",
       " 0.6012668425100856,\n",
       " 0.5156187345273793,\n",
       " 0.5324999934528023,\n",
       " 0.5433965304400772,\n",
       " 0.4832422863692045,\n",
       " 0.6015596566721797,\n",
       " 0.47690025717020035,\n",
       " 0.5123192359460518,\n",
       " 0.4909763488685712,\n",
       " 0.5319231383036822,\n",
       " 0.5097040913533419,\n",
       " 0.5390426241792738,\n",
       " 0.5210442610550672,\n",
       " 0.5353908153483644,\n",
       " 0.6338616905268282,\n",
       " 0.49723832053132355,\n",
       " 0.5741663423832506,\n",
       " 0.5274698669090867,\n",
       " 0.45666226861067116,\n",
       " 0.5016348898643628,\n",
       " 0.5073070111684501,\n",
       " 0.5315931078512222,\n",
       " 0.519986436702311,\n",
       " 0.6434764547739178,\n",
       " 0.46476669516414404,\n",
       " 0.48226184281520545,\n",
       " 0.44912660005502403,\n",
       " 0.4609759111190215,\n",
       " 0.7791277603246272,\n",
       " 0.4684013198129833,\n",
       " 0.4921773080714047,\n",
       " 0.5195775447646156,\n",
       " 0.594672326289583,\n",
       " 0.5568638700060546,\n",
       " 0.5575878513045609,\n",
       " 0.5021410222398117,\n",
       " 0.49059903965098783,\n",
       " 0.47976272099185735,\n",
       " 0.4663385021267459,\n",
       " 0.56619250937365,\n",
       " 0.46074096648953855,\n",
       " 0.5312476052786224,\n",
       " 0.557175257592462,\n",
       " 0.46890960924793035,\n",
       " 0.5285261757671833,\n",
       " 0.4444940695539117,\n",
       " 0.4929746410343796,\n",
       " 0.4497526098275557,\n",
       " 0.5092586178798229,\n",
       " 0.5036438612733036,\n",
       " 0.5261954609304667,\n",
       " 0.4866851419210434,\n",
       " 0.5886696418747306,\n",
       " 0.5002342698862776,\n",
       " 0.5520845034625381,\n",
       " 0.5247080619446933,\n",
       " 0.4749945825897157,\n",
       " 0.507678444031626,\n",
       " 0.5821520067984238,\n",
       " 0.5985546780284494,\n",
       " 0.4254170046187937,\n",
       " 0.3989579740446061,\n",
       " 0.46167213656008244,\n",
       " 0.5595364002510905,\n",
       " 0.5455488512525335,\n",
       " 0.43627136549912393,\n",
       " 0.48207088035997003,\n",
       " 0.4764058324508369,\n",
       " 0.4729582532308996,\n",
       " 0.5437897928059101,\n",
       " 0.5697536775842309,\n",
       " 0.5319083789363503,\n",
       " 0.4924260104307905,\n",
       " 0.5069621312431991,\n",
       " 0.4897208781912923,\n",
       " 0.5011610426008701,\n",
       " 0.5196807608008385,\n",
       " 0.5136888672132045,\n",
       " 0.4593661685939878,\n",
       " 0.5848035049857572,\n",
       " 0.4661971088498831,\n",
       " 0.47639690805226564,\n",
       " 0.5121657495619729,\n",
       " 0.5185241948347539,\n",
       " 0.4872714397497475,\n",
       " 0.5580066009424627,\n",
       " 0.4474965054541826,\n",
       " 0.6751561677083373,\n",
       " 0.5522598908282816,\n",
       " 0.5270049388054758,\n",
       " 0.5686465813778341,\n",
       " 0.703111176379025,\n",
       " 0.5284255289006978,\n",
       " 0.49581091734580696,\n",
       " 0.4741097919177264,\n",
       " 0.4798981761559844,\n",
       " 0.5069165315944701,\n",
       " 0.4987219893373549,\n",
       " 0.5391533756628633,\n",
       " 0.44892579573206604,\n",
       " 0.5084650105563924,\n",
       " 0.48990906623657793,\n",
       " 0.5136593976058066,\n",
       " 0.5125267608091235,\n",
       " 0.6466015754267573,\n",
       " 0.5575995382387191,\n",
       " 0.4733256660401821,\n",
       " 0.47368655540049076,\n",
       " 0.46790986275300384,\n",
       " 0.5644992766901851,\n",
       " 0.4988094768486917,\n",
       " 0.46269255969673395,\n",
       " 0.5155679644085467,\n",
       " 0.4988879597513005,\n",
       " 0.45707769016735256,\n",
       " 0.5960584643762559,\n",
       " 0.5156160804908723,\n",
       " 0.5326692024245858,\n",
       " 0.5494476623134688,\n",
       " 0.4705921004060656,\n",
       " 0.5061422071885318,\n",
       " 0.5141272232867777,\n",
       " 0.4551659281132743,\n",
       " 0.5429108152166009,\n",
       " 0.5454236546065658,\n",
       " 0.4948537970194593,\n",
       " 0.5347833416890353,\n",
       " 0.4802291627274826,\n",
       " 0.5639106193557382,\n",
       " 0.45092259417288005,\n",
       " 0.5255488540860824,\n",
       " 0.5166792266536504,\n",
       " 0.4495601109229028,\n",
       " 0.5233232788741589,\n",
       " 0.4959351912839338,\n",
       " 0.5491085315588862,\n",
       " 0.4821105336304754,\n",
       " 0.6183628250146285,\n",
       " 0.552233555004932,\n",
       " 0.5043309405446053,\n",
       " 0.321589763276279,\n",
       " 0.42103509907610714,\n",
       " 0.4331952566280961,\n",
       " 0.28998046834021807,\n",
       " 0.4277368220500648,\n",
       " 0.5095396263059229,\n",
       " 0.4146055062301457,\n",
       " 0.3485206621699035,\n",
       " 0.41536036552861333,\n",
       " 0.44747408479452133,\n",
       " 0.3682232038117945,\n",
       " 0.5745120486244559,\n",
       " 0.49421313870698214,\n",
       " 0.5486359866335988,\n",
       " 0.5884704594500363,\n",
       " 0.48374183021951467,\n",
       " 0.5152997639961541,\n",
       " 0.6470918107079342,\n",
       " 0.6134278310928494,\n",
       " 0.43066511722281575,\n",
       " 0.4612108739092946,\n",
       " 0.6171032306738198,\n",
       " 0.5247070705518126,\n",
       " 0.6197108738124371,\n",
       " 0.4778754636645317,\n",
       " 0.4915731934597716,\n",
       " 0.7096580807119608,\n",
       " 0.4966675522737205,\n",
       " 0.48941447120159864,\n",
       " 0.409826529212296,\n",
       " 0.5116786082508042,\n",
       " 0.5095981635386124,\n",
       " 0.5928157136077061,\n",
       " 0.4637962784618139,\n",
       " 0.43856717250309885,\n",
       " 0.46854089270345867,\n",
       " 0.46433088334742934,\n",
       " 0.5351215312257409,\n",
       " 0.48632033821195364,\n",
       " 0.4540099448058754,\n",
       " 0.5370446104789153,\n",
       " 0.4348067429382354,\n",
       " 0.5412265345221385,\n",
       " 0.5230424302862957,\n",
       " 0.44843837479129434,\n",
       " 0.506280411966145,\n",
       " 0.4933014219859615,\n",
       " 0.44652366603258997,\n",
       " 0.5231498854700476,\n",
       " 0.5112176584079862,\n",
       " 0.5565717220306396,\n",
       " 0.4853828679770231,\n",
       " 0.4914139194879681,\n",
       " 0.6588097601779737,\n",
       " 0.5129811007063836,\n",
       " 0.47312903101556003,\n",
       " 0.5488613967318088,\n",
       " 0.4636489547556266,\n",
       " 0.539024043828249,\n",
       " 0.5014668327057734,\n",
       " 0.498486147262156,\n",
       " 0.5278112989617512,\n",
       " 0.5941052904818207,\n",
       " 0.5236668197903782,\n",
       " 0.4294249608647078,\n",
       " 0.5034475419670343,\n",
       " 0.5302466955035925,\n",
       " 0.4698441254440695,\n",
       " 0.3322210971964523,\n",
       " 0.5552956755273044,\n",
       " 0.47054804384242743,\n",
       " 0.41947778314352036,\n",
       " 0.5292450747219846,\n",
       " 0.5352143967757002,\n",
       " 0.49827951518818736,\n",
       " 0.51650034065824,\n",
       " 0.5195725022349507,\n",
       " 0.5080076619051397,\n",
       " 0.5510477456264198,\n",
       " 0.7002660515718162,\n",
       " 0.5566769149154425,\n",
       " 0.5131216018926352,\n",
       " 0.531358037958853,\n",
       " 0.49325236887671053,\n",
       " 0.5041736629791558,\n",
       " 0.5472503788769245,\n",
       " 0.47262182645499706,\n",
       " 0.5020616083638743,\n",
       " 0.4933289245236665,\n",
       " 0.5472397183766589,\n",
       " 0.5449498407542706,\n",
       " 0.44270863226847723,\n",
       " 0.4598876116797328,\n",
       " 0.4505155375227332,\n",
       " 0.5267375407274812,\n",
       " 0.5033801140962169,\n",
       " 0.6665686576161534,\n",
       " 0.524921573116444,\n",
       " 0.5321829784661531,\n",
       " 0.5309932879172266,\n",
       " 0.5083828247152269,\n",
       " 0.5509709524922073,\n",
       " 0.5855955512961373,\n",
       " 0.5503467650851235,\n",
       " 0.40444512420799583,\n",
       " 0.5649458635598421,\n",
       " 0.568513278500177,\n",
       " 0.5098268573638052,\n",
       " 0.5087000816129148,\n",
       " 0.4370589214959182,\n",
       " 0.4571194328600541,\n",
       " 0.51680514565669,\n",
       " 0.4981502895243466,\n",
       " 0.4992046379484236,\n",
       " 0.5320191823411733,\n",
       " 0.6288714809343219,\n",
       " 0.49357941711787134,\n",
       " 0.575290959328413,\n",
       " 0.5136951633030549,\n",
       " 0.5021012558136135,\n",
       " 0.5240084724500775,\n",
       " 0.5101156461751089,\n",
       " 0.5233497585868463,\n",
       " 0.513385909376666,\n",
       " 0.5105263255536556,\n",
       " 0.5566170520614833,\n",
       " 0.5006746305152774,\n",
       " 0.5388761012000032,\n",
       " 0.496941830846481,\n",
       " 0.4733511807862669,\n",
       " 0.5289264949969947,\n",
       " 0.5752015092875808,\n",
       " 0.5924824187532067,\n",
       " 0.47783157718367875,\n",
       " 0.4787171953357756,\n",
       " 0.5277463600505143,\n",
       " 0.5615264279767871,\n",
       " 0.5599641520529985,\n",
       " 0.4178275503218174,\n",
       " 0.5020128663163632,\n",
       " 0.5592941814102232,\n",
       " 0.5085546143818647,\n",
       " 0.5067412019707263,\n",
       " 0.535721649765037,\n",
       " 0.5110801924020052,\n",
       " 0.4748935238458216,\n",
       " 0.5032266076887026,\n",
       " 0.5595927713438869,\n",
       " 0.48755491932388395,\n",
       " 0.658544862177223,\n",
       " 0.5000828157644719,\n",
       " 0.5249000160256401,\n",
       " 0.5571050202706829,\n",
       " 0.4876324040815234,\n",
       " 0.4700511023402214,\n",
       " 0.47424138290807605,\n",
       " 0.5635562911629677,\n",
       " 0.5433694357052445,\n",
       " 0.529693647287786,\n",
       " 0.48353205109015107,\n",
       " 0.5289579874370247,\n",
       " 0.45729073882102966,\n",
       " 0.527687075547874,\n",
       " 0.49159874266479164,\n",
       " 0.4879718169104308,\n",
       " 0.5958921927958727,\n",
       " 0.40572885633446276,\n",
       " 0.5250384692335501,\n",
       " 0.5739613373298198,\n",
       " 0.5501623819582164,\n",
       " 0.601876424450893,\n",
       " 0.5195441073738039,\n",
       " 0.4678019096609205,\n",
       " 0.5223132404498756,\n",
       " 0.4682158655487001,\n",
       " 0.4930727002210915,\n",
       " 0.49259096663445234,\n",
       " 0.5134744122624397,\n",
       " 0.5180421541444957,\n",
       " 0.5077520338818431,\n",
       " 0.481758103473112,\n",
       " 0.4125288075301796,\n",
       " 0.4894709575455636,\n",
       " 0.5365009572124109,\n",
       " 0.5046099981991574,\n",
       " 0.5450311757158488,\n",
       " 0.621164106822107,\n",
       " 0.5490664304234087,\n",
       " 0.577169272582978,\n",
       " 0.564032263122499,\n",
       " 0.4644986013881862,\n",
       " 0.4641339420340955,\n",
       " 0.4494425019947812,\n",
       " 0.48995050182566047,\n",
       " 0.5827134409919381,\n",
       " 0.6193205716554075,\n",
       " 0.42202913318760693,\n",
       " 0.5029834466986358,\n",
       " 0.48751103575341403,\n",
       " 0.5158110801130533,\n",
       " 0.5191802936606109,\n",
       " 0.5668269679299556,\n",
       " 0.49596250092145056,\n",
       " 0.46200787648558617,\n",
       " 0.5258474731817842,\n",
       " 0.4865658099297434,\n",
       " 0.540953990072012,\n",
       " 0.7049701486248523,\n",
       " 0.4999064225703478,\n",
       " 0.5282614084426314,\n",
       " 0.5211707600392401,\n",
       " 0.5478743626736104,\n",
       " 0.47976025799289346,\n",
       " 0.40409829607233405,\n",
       " 0.44477956532500684,\n",
       " 0.4947388032451272,\n",
       " 0.5273662225226872,\n",
       " 0.24693156487774104,\n",
       " 0.48017173260450363,\n",
       " 0.48445755639113486,\n",
       " 0.49349581357091665,\n",
       " 0.47350682504475117,\n",
       " 0.5086746652377769,\n",
       " 0.5705962119391188,\n",
       " 0.5084503444377333,\n",
       " 0.49231011990923434,\n",
       " 0.5019379070145078,\n",
       " 0.5523985978215933,\n",
       " 0.6415775076020509,\n",
       " 0.4683605330064893,\n",
       " 0.5580127753783017,\n",
       " 0.514518080628477,\n",
       " 0.4968973055947572,\n",
       " 0.5550256362184882,\n",
       " 0.5203776510898024,\n",
       " 0.3705034157028422,\n",
       " 0.49050396867096424,\n",
       " 0.5092473938129842,\n",
       " 0.7216806299984455,\n",
       " 0.5766024228651077,\n",
       " 0.5064525110647082,\n",
       " 0.5346076337154955,\n",
       " 0.5261008704546839,\n",
       " 0.49036606040317565,\n",
       " 0.5315907020121813,\n",
       " 0.5285107372328639,\n",
       " 0.5706164939329028,\n",
       " 0.5221332740038633,\n",
       " 0.5737286815419793,\n",
       " 0.4831174585269764,\n",
       " 0.5095018309075385,\n",
       " 0.5015579222235829,\n",
       " 0.5257006846368313,\n",
       " 0.5162397981621325,\n",
       " 0.5522600268013775,\n",
       " 0.5719925835728645,\n",
       " 0.47396431933157146,\n",
       " 0.42518915026448667,\n",
       " 0.5724897212348878,\n",
       " 0.5415803594514728,\n",
       " 0.42049270449206233,\n",
       " 0.6310158320702612,\n",
       " 0.5876999015454203,\n",
       " 0.60852080443874,\n",
       " 0.4952558781951666,\n",
       " 0.5523853302001953,\n",
       " 0.5306017507100478,\n",
       " 0.4864168936619535,\n",
       " 0.6224207350751385,\n",
       " 0.5011947808670811,\n",
       " 0.5384581509279087,\n",
       " 0.4497643314534798,\n",
       " 0.5126350664068013,\n",
       " 0.4681948487414047,\n",
       " 0.5823796498589218,\n",
       " 0.47065048990771174,\n",
       " 0.49438313487917185,\n",
       " 0.5306455786339939,\n",
       " 0.5406752454582602,\n",
       " 0.5469029718078673,\n",
       " 0.48817533743567765,\n",
       " 0.4790295623242855,\n",
       " 0.6718419607495889,\n",
       " 0.5716962781734765,\n",
       " 0.565200375393033,\n",
       " 0.6628861322533339,\n",
       " 0.5266256583854556,\n",
       " 0.5952562934253365,\n",
       " 0.538404589984566,\n",
       " 0.49511543242260814,\n",
       " 0.5053453596774489,\n",
       " 0.4898518908303231,\n",
       " 0.49985236080829054,\n",
       " 0.4954779850086197,\n",
       " 0.5224890348035842,\n",
       " 0.4929084423929453,\n",
       " 0.5352957295253873,\n",
       " 0.499468753230758,\n",
       " 0.482428411138244,\n",
       " 0.5281040375120938,\n",
       " 0.49927648273296654,\n",
       " 0.5072526952717453,\n",
       " 0.6434295575600117,\n",
       " 0.46548747876659036,\n",
       " 0.5753766847774386,\n",
       " 0.5651245787739754,\n",
       " 0.4959243753692135,\n",
       " 0.5228228755295277,\n",
       " 0.5215359930880368,\n",
       " 0.48508900008164346,\n",
       " 0.4593835275154561,\n",
       " 0.5360462304670364,\n",
       " 0.5246189991012216,\n",
       " 0.5388955962844193,\n",
       " 0.5371841927990317,\n",
       " 0.4888287706999108,\n",
       " 0.4856312251649797,\n",
       " 0.4484427383868024,\n",
       " 0.5355649244738743,\n",
       " 0.678056508069858,\n",
       " 0.5847303427290171,\n",
       " 0.5655321378726512,\n",
       " 0.2849518300499767,\n",
       " 0.5145865980302915,\n",
       " 0.5100764674134552,\n",
       " 0.5671732204500586,\n",
       " 0.4804765972075984,\n",
       " 0.6211263511795551,\n",
       " 0.5257015395909548,\n",
       " 0.5266876304522157,\n",
       " 0.5636125004384667,\n",
       " 0.5075667763594538,\n",
       " 0.5792494211345911,\n",
       " 0.5752570787444711,\n",
       " 0.5895115651655942,\n",
       " 0.4761238587088883,\n",
       " 0.4607378717046231,\n",
       " 0.48426016501616687,\n",
       " 0.6119852638803422,\n",
       " 0.4226364600472152,\n",
       " 0.4502406696556136,\n",
       " 0.5196492173708975,\n",
       " 0.5111918728798628,\n",
       " 0.5274095688946545,\n",
       " 0.465896520181559,\n",
       " 0.5410501675214618,\n",
       " 0.5093837454915047,\n",
       " 0.5332599896937609,\n",
       " 0.56540197879076,\n",
       " 0.49810722656548023,\n",
       " 0.4365249682450667,\n",
       " 0.5518434201367199,\n",
       " 0.5101943162735552,\n",
       " 0.49820151226595044,\n",
       " 0.5196180418133736,\n",
       " 0.4646265539340675,\n",
       " 0.5278571919479873,\n",
       " 0.6395325679332018,\n",
       " 0.6710770009085536,\n",
       " 0.4776726410491392,\n",
       " 0.521926034707576,\n",
       " 0.5131023216526955,\n",
       " 0.5283264904282987,\n",
       " 0.4722506678663194,\n",
       " 0.5407618794124573,\n",
       " 0.5449309557443485,\n",
       " 0.3955878260312602,\n",
       " 0.47814357792958617,\n",
       " 0.41688856622204185,\n",
       " 0.4136402830481529,\n",
       " 0.4554988570744172,\n",
       " 0.5350078342016786,\n",
       " 0.42172608291730285,\n",
       " 0.5306415616068989,\n",
       " 0.5268816349562258,\n",
       " 0.47594150085933506,\n",
       " 0.5887691542739049,\n",
       " 0.3538114984985441,\n",
       " 0.505132129881531,\n",
       " 0.5223449927289039,\n",
       " 0.5112174061359838,\n",
       " 0.5269116406561807,\n",
       " 0.5323601216077805,\n",
       " 0.5803736140951514,\n",
       " 0.49159245379269123,\n",
       " 0.43463942175731063,\n",
       " 0.4893873333930969,\n",
       " 0.5529697572346777,\n",
       " 0.5341762759489939,\n",
       " 0.5273075676523149,\n",
       " 0.4749657101929188,\n",
       " 0.6829366507008672,\n",
       " 0.5421498795039952,\n",
       " 0.37933273799717426,\n",
       " 0.5367731814039871,\n",
       " 0.5235035521909595,\n",
       " 0.5100371828302741,\n",
       " 0.5556651016231626,\n",
       " 0.5058205587556586,\n",
       " 0.46192745747976005,\n",
       " 0.44794292969163507,\n",
       " 0.4736790324677713,\n",
       " 0.5097936454694718,\n",
       " 0.5229132808744907,\n",
       " 0.5325755872763693,\n",
       " 0.4719423046335578,\n",
       " 0.46337440982460976,\n",
       " 0.6186315095983446,\n",
       " 0.5032872157171369,\n",
       " 0.5492676512803882,\n",
       " 0.6510129789821804,\n",
       " 0.45525768818333745,\n",
       " 0.549809102434665,\n",
       " 0.583738359156996,\n",
       " 0.48556676716543734,\n",
       " 0.3567769769579172,\n",
       " 0.46228001976851374,\n",
       " 0.5752552733756602,\n",
       " 0.5101302131079137,\n",
       " 0.5029692626558244,\n",
       " 0.5176363317295909,\n",
       " 0.5573485711356625,\n",
       " 0.4755332423374057,\n",
       " 0.4850588240660727,\n",
       " 0.5502167134545743,\n",
       " 0.5229638312011957,\n",
       " 0.47965904837474227,\n",
       " 0.5052922286558896,\n",
       " 0.6033957188483328,\n",
       " 0.551856069592759,\n",
       " 0.6763175878440961,\n",
       " 0.5684459055773914,\n",
       " 0.4667769721709192,\n",
       " 0.5328082637861371,\n",
       " 0.5501802550861612,\n",
       " 0.519068002467975,\n",
       " 0.5216474626213312,\n",
       " 0.4213046352379024,\n",
       " 0.48972245084587485,\n",
       " 0.4560340102761984,\n",
       " 0.4700801856815815,\n",
       " 0.524583115009591,\n",
       " 0.5883302215952426,\n",
       " 0.5757349031046033,\n",
       " 0.49527634447440505,\n",
       " 0.5143659632885829,\n",
       " 0.4742795501369983,\n",
       " 0.5443190517835319,\n",
       " 0.4584044914226979,\n",
       " 0.4637508634477854,\n",
       " 0.5744869523914531,\n",
       " 0.5895651467144489,\n",
       " 0.5709105953574181,\n",
       " 0.6962885865941644,\n",
       " 0.4932578285224736,\n",
       " 0.41280956903938204,\n",
       " 0.5714127700775862,\n",
       " 0.4845557087101042,\n",
       " 0.5040983048966154,\n",
       " 0.5261270370101556,\n",
       " 0.673010153346695,\n",
       " 0.5300275017507374,\n",
       " 0.48934162239311263,\n",
       " 0.5092177154729143,\n",
       " 0.2717331633903086,\n",
       " 0.4699372439645231,\n",
       " 0.42327259574085474,\n",
       " 0.42744600772857666,\n",
       " 0.521357136167353,\n",
       " 0.6900418568402529,\n",
       " 0.5155799982603639,\n",
       " 0.5226948480121791,\n",
       " 0.4966757372021675,\n",
       " 0.4103421337204054,\n",
       " 0.5136017011245713,\n",
       " 0.5523736826144159,\n",
       " 0.5673079211264849,\n",
       " 0.5207220921292901,\n",
       " 0.4279689935501665,\n",
       " 0.44427658012136817,\n",
       " 0.6373704764991999,\n",
       " 0.5399099881760776,\n",
       " 0.50912622991018,\n",
       " 0.48520497558638453,\n",
       " 0.5147783765569329,\n",
       " 0.4926004067528993,\n",
       " 0.5470606407616287,\n",
       " 0.5523992236703634,\n",
       " 0.5524910327512771,\n",
       " 0.5590609724167734,\n",
       " 0.4815557834226638,\n",
       " 0.41785408766008914,\n",
       " 0.5056390347890556,\n",
       " 0.5829137021210045,\n",
       " 0.46308012027293444,\n",
       " 0.4988103909417987,\n",
       " 0.5155802932567894,\n",
       " 0.5172125920653343,\n",
       " 0.47774452762678266,\n",
       " 0.5790991726098582,\n",
       " 0.5079152313992381,\n",
       " 0.5754464752972126,\n",
       " 0.46069421945139766,\n",
       " 0.4544180815573782,\n",
       " 0.4305396639974788,\n",
       " 0.47043645149096847,\n",
       " 0.5287552597001195,\n",
       " 0.5131752913584933,\n",
       " 0.5235633989796042,\n",
       " 0.5179684711620212,\n",
       " 0.524747685296461,\n",
       " 0.5770127957221121,\n",
       " 0.5186869486933574,\n",
       " 0.4867860395461321,\n",
       " 0.4631359353661537,\n",
       " 0.4783233223715797,\n",
       " 0.441990613238886,\n",
       " 0.48021894600242376,\n",
       " 0.40100152930244803,\n",
       " 0.4658796666190028,\n",
       " 0.5296577415429056,\n",
       " 0.47860935842618346,\n",
       " 0.4293763772584498,\n",
       " 0.4639164716936648,\n",
       " 0.44469003402628005,\n",
       " 0.5219588195905089,\n",
       " 0.6501409313641489,\n",
       " 0.5531779360026121,\n",
       " 0.4994990759296343,\n",
       " 0.5398968968074769,\n",
       " 0.3911560142878443,\n",
       " 0.46100396383553743,\n",
       " 0.48165933683048934,\n",
       " 0.5033323280513287,\n",
       " 0.50624916004017,\n",
       " 0.4438817324116826,\n",
       " 0.5586101766675711,\n",
       " 0.573075613938272,\n",
       " 0.4841756784589961,\n",
       " 0.42337273177690804,\n",
       " 0.4016966694034636,\n",
       " 0.40689401619601995,\n",
       " 0.5839037348050624,\n",
       " 0.5864863157039508,\n",
       " 0.5455755656585097,\n",
       " 0.4629240996437147,\n",
       " 0.5637235241010785,\n",
       " 0.42410264583304524,\n",
       " 0.4567105546593666,\n",
       " 0.5806995688471943,\n",
       " 0.5695932776434347,\n",
       " 0.42242628522217274,\n",
       " 0.4945617225021124,\n",
       " 0.43667964823544025,\n",
       " 0.5273343157023191,\n",
       " 0.6241063247434795,\n",
       " 0.49417993356473744,\n",
       " 0.5318693316075951,\n",
       " 0.5475125145167112,\n",
       " 0.5308632351807319,\n",
       " 0.5337162363575771,\n",
       " 0.6251115811755881,\n",
       " 0.5052820212440565,\n",
       " 0.5325204497203231,\n",
       " 0.5538365988759324,\n",
       " 0.5342715913429856,\n",
       " 0.49933587457053363,\n",
       " 0.5010954784229398,\n",
       " 0.4707161810947582,\n",
       " 0.46518215897958726,\n",
       " 0.4801321148406714,\n",
       " 0.6437039040029049,\n",
       " 0.5850892453454435,\n",
       " 0.5378527520224452,\n",
       " 0.6056154696270823,\n",
       " 0.49382095923647285,\n",
       " 0.49312582379207015,\n",
       " 0.49258154863491654,\n",
       " 0.678479028865695,\n",
       " 0.5320895690238103,\n",
       " 0.42641551746055484,\n",
       " 0.43007995560765266,\n",
       " 0.4763018937082961,\n",
       " 0.530352626927197,\n",
       " 0.5309502696618438,\n",
       " 0.5387065468821675,\n",
       " 0.5803915741853416,\n",
       " 0.4876763578504324,\n",
       " 0.47946935752406716,\n",
       " 0.5585714289918542,\n",
       " 0.5469014531699941,\n",
       " 0.5141309351893142,\n",
       " 0.3782511822646484,\n",
       " 0.507172021549195,\n",
       " 0.6176093718968332,\n",
       " 0.48808750114403665,\n",
       " 0.6891673938371241,\n",
       " 0.5125631270930171,\n",
       " 0.6876548279542476,\n",
       " 0.5864866189658642,\n",
       " 0.4959265710785985,\n",
       " 0.47747125942260027,\n",
       " 0.4739077591220848,\n",
       " 0.47998452302999794,\n",
       " 0.508108458481729,\n",
       " 0.5124355613952503,\n",
       " 0.4665322140790522,\n",
       " 0.6322816540487111,\n",
       " 0.49651720584370196,\n",
       " 0.5469189425930381,\n",
       " 0.45827010995708406,\n",
       " 0.45459458022378385,\n",
       " 0.49736413382925093,\n",
       " 0.517461143201217,\n",
       " 0.5190614331513643,\n",
       " 0.5401413693325594,\n",
       " 0.47484020236879587,\n",
       " 0.4752977597527206,\n",
       " 0.4582741241902113,\n",
       " 0.4440753299277276,\n",
       " 0.5365988257690333,\n",
       " 0.489540594862774,\n",
       " 0.6150866105454043,\n",
       " 0.6058716475963593,\n",
       " 0.5555447326041758,\n",
       " 0.5162338002119213,\n",
       " 0.49280977062880993,\n",
       " 0.5303949329536408,\n",
       " 0.5313091313000768,\n",
       " 0.3366635820129886,\n",
       " 0.6477373912930489,\n",
       " 0.48746542166918516,\n",
       " 0.5170669564977288,\n",
       " 0.5727042024955153,\n",
       " 0.4724648268893361,\n",
       " 0.5003149578114972,\n",
       " 0.516036645276472,\n",
       " 0.4732786030508578,\n",
       " 0.5174813885241747,\n",
       " 0.5146529847988859,\n",
       " 0.49714099057018757,\n",
       " 0.4844937026500702,\n",
       " 0.5270666354335845,\n",
       " 0.5140174273401499,\n",
       " 0.477851357543841,\n",
       " 0.5400928327580914,\n",
       " 0.559342609718442,\n",
       " 0.6000420078635216,\n",
       " 0.47464036801829934,\n",
       " 0.5251549133099616,\n",
       " 0.5424123598495498,\n",
       " 0.6328275996493176,\n",
       " 0.546415468910709,\n",
       " 0.5096168095478788,\n",
       " 0.5533139158505946,\n",
       " 0.7293920222437009,\n",
       " 0.4800721462816,\n",
       " 0.5333221179898828,\n",
       " 0.5180621636100113,\n",
       " 0.4473449542419985,\n",
       " 0.5113499037688598,\n",
       " 0.5208978734444827,\n",
       " 0.521419869037345,\n",
       " 0.5209928825497627,\n",
       " 0.6597691362258047,\n",
       " 0.5294784388970584,\n",
       " 0.48697728384286165,\n",
       " 0.5129062659107149,\n",
       " 0.46774348767939955,\n",
       " 0.4828053789678961,\n",
       " 0.4986702650785446,\n",
       " 0.4340893916087225,\n",
       " 0.4991546075325459,\n",
       " 0.48608152417000383,\n",
       " 0.5023497230140492,\n",
       " 0.6877280303742737,\n",
       " 0.7258045189082623,\n",
       " 0.4580786399310455,\n",
       " 0.5207366929389536,\n",
       " 0.49231022014282644,\n",
       " 0.4685654720524326,\n",
       " 0.4844193102326244,\n",
       " 0.5704945961479098,\n",
       " 0.5627036041114479,\n",
       " 0.6204858351266012,\n",
       " 0.5345650741364807,\n",
       " 0.4595013821963221,\n",
       " 0.4875161414965987,\n",
       " 0.4789488296955824,\n",
       " 0.4956090444466099,\n",
       " 0.43810162181034684,\n",
       " 0.4581684118602425,\n",
       " 0.5598514713346958,\n",
       " 0.4475109826307744,\n",
       " 0.5540738203562796,\n",
       " 0.5260368147864938,\n",
       " 0.4172031384659931,\n",
       " 0.5062646195292473,\n",
       " 0.5766624890966341,\n",
       " 0.5692336349748075,\n",
       " 0.5103308310499415,\n",
       " 0.5213993974030018,\n",
       " 0.5240559750236571,\n",
       " 0.5062019523466006,\n",
       " 0.4484835710609332,\n",
       " 0.48605446820147336,\n",
       " 0.46542967844288796,\n",
       " 0.4773140841862187,\n",
       " 0.487784807337448,\n",
       " 0.4525914606638253,\n",
       " 0.5859052650630474,\n",
       " 0.6730809297878295,\n",
       " 0.6949487812817097,\n",
       " 0.5820119176059961,\n",
       " 0.4002227638848126,\n",
       " 0.5390118944924325,\n",
       " 0.5731966949533671,\n",
       " 0.6169124664738774,\n",
       " 0.5302683773916215,\n",
       " 0.4897615450900048,\n",
       " 0.5960797648876905,\n",
       " 0.4923779424279928,\n",
       " 0.5370630680117756,\n",
       " 0.6273857060587034,\n",
       " 0.5115017765201628,\n",
       " 0.3861727037001401,\n",
       " 0.663679214194417,\n",
       " 0.5278994415421039,\n",
       " 0.4986195128876716,\n",
       " 0.4660792216891423,\n",
       " 0.4699247730895877,\n",
       " 0.5170675860717893,\n",
       " 0.585011642659083,\n",
       " 0.5392204284435138,\n",
       " 0.6430315673351288,\n",
       " 0.7110114893876016,\n",
       " 0.49676862568594515,\n",
       " 0.5026976198423654,\n",
       " 0.43848666711710393,\n",
       " 0.46342542045749724,\n",
       " 0.48996216629166156,\n",
       " 0.5067776245996356,\n",
       " 0.5016365913907066,\n",
       " 0.5485760048031807,\n",
       " 0.4881413450348191,\n",
       " 0.5213917256332934,\n",
       " 0.5308818863704801,\n",
       " 0.5064045418985188,\n",
       " 0.5220589367672801,\n",
       " 0.5050206026062369,\n",
       " 0.5457745251478627,\n",
       " 0.49821051594335586,\n",
       " 0.5671057645231485,\n",
       " 0.49873426789417863,\n",
       " 0.5043763071298599,\n",
       " 0.5405412972904742,\n",
       " 0.5196004426106811,\n",
       " 0.6040946636348963,\n",
       " 0.45598110544960946,\n",
       " 0.44618583749979734,\n",
       " 0.625032075564377,\n",
       " 0.5461280500749126,\n",
       " 0.5203860058682039,\n",
       " 0.5154407166410238,\n",
       " 0.5267731740605086,\n",
       " 0.5439014434814453,\n",
       " 0.4939220460364595,\n",
       " 0.4960252814926207,\n",
       " 0.6562666376121342,\n",
       " 0.4958383863558993,\n",
       " 0.5089664504048415,\n",
       " 0.5095831558573991,\n",
       " 0.555330079048872,\n",
       " 0.48421533149667084,\n",
       " 0.45923288236372173,\n",
       " 0.5512005491182208,\n",
       " 0.4950089246267453,\n",
       " 0.4585311855189502,\n",
       " 0.5496542055625468,\n",
       " 0.5569154811091721,\n",
       " 0.520356826018542,\n",
       " 0.5242887147469446,\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('./original_g_losses.csv', 'w', newline='') as csvfile\n",
    "    spamwriter = csv.writer(csvfile, delimiter=';',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in history[1]:\n",
    "        spamwriter.writerow([i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_A = load_model('original_d_A.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "d_B = load_model('original_d_B.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "g_AB = load_model('original_g_AB.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "g_BA = load_model('original_g_BA.h5',custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "\n",
    "g_AB.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "g_BA.compile(optimizer=\"Adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_trained_gan = CycleGAN(d_A, d_B, g_AB, g_BA)\n",
    "retrained_history = pre_trained_gan.train(epochs=1,batch_size=1,sample_interval=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pre = pre_trained_gan.g_BA.predict(x_test[:7])\n",
    "img_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(img_pre[7], cmap='gray')\n",
    "plt.title('ORIGINAL IMAGE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
